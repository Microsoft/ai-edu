# Step 1

## 2.1 线性反向传播

利用偏导数进行计算，$delta w$与$delta z$的对应关系，代码为BP-Linear.py，个人理解其实是一个反向修正的作用，修正权重w和偏移量b

## 2.2 非线性反向传播

引入了激活函数和网络的概念

## 3.0 损失函数

引入了一些常见的损失函数

均方差函数，主要用于回归问题；

交叉熵函数，主要用于分类问题；

二者都是非负函数，极值在底部，用梯度下降法可以求解

## 3.1 均方差损失函数

详细解释

## 3.2 交叉熵损失函数

是Shannon信息论的一个重要概念，主要用于度量两个概率分布间的差异性信息

# Step 2

## 4.0-4.4

介绍了线性回归问题的具体求解方法

## 4.5 梯度下降的三种基本形式

单样本随机梯度下降：假设一共100个样本，每次使用1个样本

小批量样本梯度下降：假设一共100个样本，每个小批量5个样本

全批量样本梯度下降：假设一共100个样本，每次使用全部样本

## 5.1 正规方程法

运用解方程的方法求解

## 5.3 样本特征数据的归一化

神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，所以它对样本数据的要求比较苛刻

# Step 3

## 6.0

分类问题在很多资料中也被称为逻辑回归，其原因是使用了线性回归中的线性模型，加上一个Logistic二分类函数，共同构造了一个分类器。

## 6.1 二分类函数

对率函数函数Logistici Function，既可以作为激活函数使用，又可以当作二分类函数使用，在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。在分类问题中，阈值越大，准确率越高，召回率越低；阈值越小，准确度越低，召回率越高。

# Step 4 NonLinearRegression

利用多层神经网络，从而解决非线性问题

## 8.0 激活函数

激活函数的作用：

1. 给神经网络增加非线性因素
2. 归一化等

基本性质：非线性 可导性 单调性

### 何时用到激活函数

在多层神经网络中，在最后一层只会用到分类函数来完成二分类或多分类任务，如果是拟合任务，则不需要分类函数。

> 注意不要将分类函数与激活函数混淆在一起
>
> 在二分类任务中使用的Logistic分类函数与在神经网络之间连接的Sigmoid激活函数，是同样的形式，所以他既是激活函数，又是分类函数，是个特例

总结：

1. 神经网络最后一层不需要激活函数
2. 激活函数只用于连接前后两层神经网络

## 9.0 单入单出的双层神经网路

### 回归模型的评估标准

回归问题主要是求值，评价标准主要是看求得值与实际结果的偏差有多大

- 平均绝对误差MAE

  ![1571486933670](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571486933670.png)

  对异常值不如MSE敏感，类似中位数

- MAPE，绝对平均值率误差

- 和方差SSE

- 均方差MSE：对异常值非常敏感

- 均方根误差RMSE

- R平方R-Squared

网络结构

 ![img](https://github.com/Easonrust/ai-edu/raw/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Images/9/nn.png) 

## 9.7 验证与测试

- 训练集（train set）：用于模型训练的数据样本

- 验证集（development set or dev set or validation set）：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估

  作用：

  - 寻找最优的网络深度
  - 决定反向传播算法的停止点
  - 在神经网络中选择隐藏层神经元的数量

- 测试集：用来评估最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。

三者关系：

 ![img](https://github.com/Easonrust/ai-edu/raw/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Images/9/dataset.png) 

### 交叉验证

#### 传统的机器学习

在传统的机器学习中，我们经常用交叉验证(Cross Validation)的方法，比如把数据分成10份，V1-V10，其中V1-V9用来训练，V10用来验证。然后用V2-V10做训练，V1做验证......如此我们可以做10次训练和验证，大大增加了模型的可靠性。

这样的话，验证集也可以做训练，训练集数据也可以做验证，当样本很少时，这个很有用。

#### 神经网络/深度学习

那么深度学习中的用法是什么呢？

比如在神经网络中，训练时到底迭代多少次停止呢？或者我们设置学习率为多少何时呢？或者用几个中间层，以及每个中间层用几个神经元呢？如何正则化？这些都是超参数设置，都可以用验证集来解决。

在咱们前面的学习中，一般使用loss小于门限值做为迭代终止条件，因为我们预先知道了这个门限值可以满足训练精度。但对于实际应用中的问题，没有先验的门限值可以参考，如何设定终止条件？此时，我们可以用验证集来验证一下准确率，假设只有90%的的准确率，那可能确实是局部最优解。这样我们可以继续迭代，寻找全局最优解

## skip connection（残差连接)

### 为什么要skip connect？

首先大家已经形成了一个通识，在一定程度上，网络越深表达能力越强，性能越好。

不过，好是好了，随着网络深度的增加，带来了许多问题，梯度消散，梯度爆炸；在resnet出来之前大家没想办法去解决吗？当然不是。更好的优化方法，更好的初始化策略，BN层，Relu等各种激活函数，都被用过了，但是仍然不够，改善问题的能力有限，直到残差连接被广泛使用。
有效防止深层网络的 关于对称性引发的特征退化问题 ， 打破了网络的对称性，提升了网络的表征能力 





## 为了配合项目，跳过这些曾经已经接触过一些的基础，直接步入卷积神经网络阶段

## 17.1 卷积的前向计算Convolutions

这里明确一点这里的卷积和数字图像处理的卷积并非相同，这里的卷积无需翻转卷积核

结论：

1. 我们实现的卷积操作不是原始数学含义的卷积，而是工程上的卷积，可以简称为卷积
2. 在实现卷积操作时，并不会反转卷积核

### 单入单出的二维卷积

![1571068253608](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068253608.png)

### 单入多出的升维卷积

![1571068289054](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068289054.png)

原始输入为1维的图片，与多个卷积核分别对其计算，从而得到多个特征输出

### 多入单出的降维卷积

![1571068596610](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068596610.png)

### 多入多出的同维卷积

![1571068644588](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068644588.png)

![1571068678474](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068678474.png)

### 步长stride

滑窗操作中移动的格数

![1571068720661](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068720661.png)

### 填充padding

 如果原始图为4x4，用3x3的卷积核进行卷积后，目标图片变成了2x2。如果我们想保持目标图片和原始图片为同样大小，该怎么办呢？一般我们会向原始图片周围填充一圈0，然后再做卷积。如下图： 

![1571068762872](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571068762872.png)

两点注意：

1. 一般情况下，我们用正方形的卷积核，且为奇数
2. 如果计算出的输出图片尺寸为小数，则取整，不做四舍五入

因为卷积运算后与原图像大小不同，因此需要进行图像填充（Padding）运算，计算卷积后得到图像结果的公式

![1571123535250](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571123535250.png)

## 17.3 卷积的反向传播原理

## 17.5 池化层

池化 pooling，又称为下采样downstream sampling or sub-sampling

池化方法分为两种，一种是最大值池化，一种是平均值池化。

最大值池化：取当前池化视野中所有元素的最大值，输出到下一层特征图中。

平均值池化：取当前池化视野中所有元素的平均值，输出到下一层特征图中。

池化目的：

- 扩大视野
- 降维：在保留图片局部特征的前提下，使得图片更小，更易于计算。
- 平移不变性：轻微扰动不会影响输出
- 位处同尺寸图片，便于后端处理：假设输入的图片不是一样大小的，就需要用池化来转换成同尺寸图片。

一般我们采用最大值池化。

![1571124242126](C:\Users\yangl\AppData\Roaming\Typora\typora-user-images\1571124242126.png)

池化的常用模式：步长与池化尺寸相同

**池化层跟卷积层是一样的，在padding的时候都是在外围补零，然后再池化**

## 18.0 经典卷积神经网络模型

**之前的卷积层和池化层是提取特征的，剩下的全连接层可以认为是实现分类的**

卷积神经网络是现在深度学习领域中最有用的网络模型，尤其在计算机视觉领域更是一枝独秀。

### LeNet

1998年的LeNet是CNN的鼻祖，用于解决手写数字识别的视觉任务，自此CNN的最基本的架构就定下来了：**卷积层、池化层、全连接层**。

其网络结构：

1. 输入为单通道32x32灰度图
2. 使用6组5x5的过滤器，每个过滤器里有一个卷积核，stride=1，得到6张28x28的特征图
3. 使用2x2的池化，stride=2，得到6张14x14的特征图
4. 使用16组5x5的过滤器，每个过滤器里有6个卷积核，对应上一层的6个特征图，得到16张10x10的特征图
5. 池化，得到16张5x5的特征图
6. 接全连接层，120个神经元
7. 接全连接层，84个神经元
8. 接全连接层，10个神经元，softmax输出

> softmax：一种激活函数
>
> - 用于多重分类逻辑回归模型
> - 在构建神经网络中，在不同的层使用softmax函数。

如今各大深度学习框架中使用的LeNet都是简化改进过的LeNet-5（5表示具有5个层），将激活函数改为了现在很常用的Relu。 LeNet-5跟现有的conv->pool->ReLU的套路不同，它使用的方式是conv1->pool->conv2->pool2再接全连接层，但是不变的是，卷积层后紧接池化层的模式依旧不变。 

### 

