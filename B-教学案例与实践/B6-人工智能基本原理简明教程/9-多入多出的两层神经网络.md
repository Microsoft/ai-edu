

在本博客中，我们将用对MNIST手写体数据集中的0，1两个子数据集做例子，不用任何框架，而是仅仅使用NumPy库，来搭建一个神经网络，来识别0，1手写数据。所以，咱们先引入Numpy库，另外还需要引入struct库来读取MNIST二进制文件：

```Python
import numpy as np
import struct as st
```

关于如何从MNIST中提取出只有0和1的数据，由于篇幅有限，请见另一篇博客。

# 定义神经网络结构

我们准备用一个两层的神经网络来做这个实验：输入层->隐藏层->输出层。其中输入层就是数据输入而已，不能算作一层。

<img src=".\images\9\SetupNN.PNG" width="640">

## 输入层

如上图所示，我们暂时忘记数据集是图片（二维数组）这个性质，而是把它们当作是一个784x1的向量，作为神经网络的数据输入。如果是想学习针对图片的处理模式，可以手动搭建一个卷积神经网络，请参考我们的后续博客。

## 权重矩阵W1/B1

它是连接两层之间的纽带，有的人理解它应该属于输入层，有的人理解应该属于隐藏层，各有各的道理，我个人倾向于把它归到隐藏层，理由是$A1=W1*X+B1$，在X固定的前提下，W1决定了A1的值。另外一个理由是B1的存在位置，在本例中B1是一个10x1的矩阵，它是隐藏层10个神经元的偏移，所以它应该属于隐藏层。

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-10的权重线B1-1...B1-10应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-10的矩阵/向量。

W1的尺寸是10x784，B1的尺寸是10x1。

## 隐藏层

我们用一个10个神经元的网络来处理0/1这两个数字，相信已经足够了。如果是处理0~9这10个数字，那么10个神经元就有些吃力了，这个大家可以自己试验一下，把代码中的神经元数量修改一下，然后在保持迭代次数和其它（超）参数不变的情况，看看最终的精确度有何区别，训练时间的差异，以及内存占用有何差异。

每个神经元的输入$A1 = W1 * X + B1$，我们在这里使用双曲正切函数，所以输出是$Z1 = tanh(A1)$。当然也可以使用其它激活函数如果sigmoid, Relu等等。

## 权重矩阵W2/B2

与W1/B1类似，我个人认为它属于输出层。W2的尺寸是2x10，B2的尺寸是2x1。

## 输出层

由于我们只想完成0/1的识别，所以输出层只有两个神经元。它们的左侧是$A2=W2*Z1+B2$，右侧是$Z2=sigmoid(A2)$。

为什么我们在两个层要用两个不同的激活函数？主要是为了丰富这个例子，帮助大家理解得更多，并没有什么理论上的依据。如果楞说有的话，就是在这种简单的神经网络中，我们不怕梯度消散问题，所以用了这两个函数来做实验。

至此，我们得到了以下一串公式：

$$Z1=W1*X+B_1$$
$$A1=tanh(Z1)$$
$$Z2=W2*A1+B2$$
$$A2=sigmoid(Z2)$$

## 定义前向计算函数

确定好了网络结构，我们可以写出如下代码：

```Python
# 正向传播
def forward_calculation(X, dict_Params):
    # fetch dict_params from dictionary
    W1=dict_Params["W1"]
    B1=dict_Params["B1"]
    W2=dict_Params["W2"]
    B2=dict_Params["B2"]

    # hidden layer calculation
    Z1=np.dot(W1,X)+B1
    # you can use sigmoid function to change tanh function, 
    # but if you do so, don't forget to change the backward function too
    A1=np.tanh(Z1)
    # output layter calculation
    Z2=np.dot(W2,A1)+B2
    # activation function
    A2=sigmoid(Z2)
    dict_cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    return A2, dict_cache
```

注意，这里并没有定义Softmax计算。对于多分类问题，我们一般还要在最后一层加上一个Softmax函数，以便把所有数值变成概率形式，但是在本例中，为了简化问题，我们把Softmax从前向计算中单独拿出来，而不作为反向传播的一个环节，否则还要再把对Softmax的求导问题混进来，增加了理解难度。

绝大多数程序员的思维还停留在单值计算图上，刚刚接触矩阵运算的时候，笔者满脑子都想着矩阵的样子，脑袋的形状也会慢慢从圆形变成方形的。为了避免这种不幸再次发生，我们把前向传播的矩阵运算过程画个简单的示意图：

<img src=".\images\9\CalculationGraph.PNG" width="640">

# 定义损失函数和反向传播机制

从我们的另外一篇博客中，相信大家已经知道了神经网络中反向传播的工作过程，简而言之：

1. 建立正向计算过程
2. 定义损失函数
3. 用样本X作为输入进行正向计算得出A
4. 用A与样本X对应的Y值作为输入计算损失
5. 从损失值求导得出反向传播公式
6. 根据传播公式迭代更新参数
7. goto step 3 until we get a good enough result

## 损失函数

关于损失函数，我们有另外一篇博客专门讲到。我们在这个例子中使用交叉熵损失函数，其函数形式是：

$$Loss=-{\frac{1}{m}}\sum_{i=1}^{m}[Y_i*ln(A2_i)+(1-Y_i)ln(1-A2_i)]$$

其中，A是我们前向计算的结果，Y是label为0/1的数值。由于计算出来的是一个矩阵（或一维数组），所以我们要把每一项的$A_i与Y_i$做计算，然后用和函数相加，算整体的损失值，m是数组的元素个数。

```Python
# 损失函数
def get_cost(A2,Y):
    # A2 = sigmoid(Z2), so A2 >= 0
    # define a very small value to avoid A2 is 0 then the log(0) is meaningless
    t = 0.000001
    part1 = np.multiply(Y, np.log(A2+t))
    part2 = np.multiply(np.log(1-A2+t), (1-Y))
    # 矩阵内部数据求和
    part3 = np.sum(part1+part2, axis=0, keepdims=True)
    # m = A2.shape[0]
    cost = part3 / A2.shape[0]
    # cost is a scalor
    return cost
```

## 推导反向传播函数

有兴趣的朋友可以看一下这一章节，没有兴趣的直接跳过，记住结论公式就可以了。但是，如果可以看完这一部分，就会对神经网络的工作机制有更深的了解。

<img src=".\images\9\CalculationFlow.PNG" width="800">

先回顾一下上图的计算过程（蓝色箭头所示），我们的最终目的是要得到Loss函数值对于W2/B2/W1/B1的偏导，以便更新这四组权重值，这就是训练的过程。基于大学高数中的链式求导法则，我们知道如果想得到${\partial{Loss}}/{\partial{W2}}$，应该如黄色箭头所示这样做：

$$\frac{\partial{Loss}}{\partial{W2}}=\frac{\partial{Loss}}{\partial{A2}}*\frac{\partial{A2}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{W2}}$$

刚一看这个公式可能头大，克服一下恐惧情绪，用绳子把头勒住，慢慢来，咱们先进行一个不太严格的求导，以便说明问题。

先看第一个${\partial{Loss}}/{\partial{A2}}$，由于

$$Loss=-{\frac{1}{m}}\sum_{i=1}^{m}[Y_i*ln(A2_i)+(1-Y_i)ln(1-A2_i)]$$

对Loss求A2的偏导：

$$\frac{\partial{Loss}}{\partial{A2}}=-{\frac{1}{m}}\sum_{i=1}^{m}(\frac{Y_i}{A2_i}-\frac{1-Y_i}{1-A2_i})$$

$$=-{\frac{1}{m}}\sum_{i=1}^{m}\frac{Y_i(1-A2_i)-A2_i(1-Y_i)}{A2_i(1-A2_i)}$$

$$=-{\frac{1}{m}}\sum_{i=1}^{m}\frac{Y_i-A2_i}{A2_i(1-A2_i)}$$

对A2求Z2的偏导

$$\frac{\partial{A2}}{\partial{Z2}}=A2(1-A2)$$

因为Loss求Z2的偏导我们后面要用到很多次，所以我们可以先单独计算一下，作为中间变量保存：

$$dZ2=\frac{\partial{Loss}}{\partial{Z2}}=\frac{\partial{Loss}}{\partial{A2}}*\frac{\partial{A2}}{\partial{Z2}}$$

$$=-{\frac{1}{m}}\sum_{i=1}^{m}[\frac{Y_i-A2_i}{A2_i(1-A2_i)}A2_i(1-A2_i)]$$
$$={\frac{1}{m}}\sum_{i=1}^{m}(Y_i-A2_i)=Y-A2$$

哇呜！原来推导出来的结果就是一个简单的矩阵减法，多么神奇！其实这都是计算机科学家们的功劳，他们找到了即有理论依据，又便于计算的一堆公式，最后的结论是如此的简明而优美！

我们再接着对Z2求W2的偏导：

因为$Z2=W2*A1+B2$，所以：

$$\frac{\partial{Z2}}{\partial{W2}}=\frac{\partial{(W2*A1+B2)}}{\partial{W2}}=A1^T$$

$$dW2=\frac{\partial{Loss}}{\partial{W2}}=\frac{\partial{Loss}}{\partial{A2}}*\frac{\partial{A2}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{W2}}=dZ2*A1^T$$

如果实在不能理解A1的矩阵转置的话，你就这么想：Y-A2是一个2x1的矩阵，A1是个10x1的矩阵，这两个矩阵不可能相乘啊。另外，我们要求W2的偏导，所以最后出来的结果一定是和W2一样的2x10的shape，所以如果A1转置一下，变成1x10，然后$(Y-A2)A1^T$就是[2x1]x[1x10]=2x10了，也就是W2需要的shape了。

我们再看看B2如何计算，与W2类似：

$$\frac{\partial{Z2}}{\partial{B2}}=\frac{\partial{(W2*A1+B2)}}{\partial{B2}}=1^T$$

所以：

$$dB2 = \frac{\partial{Loss}}{\partial{B2}}=\frac{\partial{Loss}}{\partial{A2}}*\frac{\partial{A2}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{B2}}=dZ2*1^T$$

推公式好累啊，喝口水歇会儿。我们再接着向前推，看看W1/B1如何处理，和W2/B2类似，要从A1/Z1入手：

$$dZ1=\frac{\partial{Loss}}{\partial{Z1}}=\frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{A1}}*\frac{\partial{A1}}{\partial{Z1}}$$

其中：

$$\frac{\partial{Z2}}{\partial{A1}}=\frac{\partial{(W2*A1+B2)}}{\partial{A1}}=W2^T$$

由于：

$$A1=tanh(Z1)=\frac{e^{Z1}-e^{-Z1}}{e^{Z1}+e^{-Z1}}$$

所以（这一部分的求导过程请参考我们的另外一篇博客）:

$$\frac{\partial{A1}}{\partial{Z1}}=1-A1^2$$

我们先保存一下Z1的求导结果作为中间变量：

$$dZ1=\frac{\partial{Loss}}{\partial{Z1}}=\frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{A1}}*\frac{\partial{A1}}{\partial{Z1}}=dZ2*W2^T*(1-A1^2)$$

由于

$$\frac{\partial{Z1}}{\partial{W1}}=\frac{\partial{(W1*X+B1)}}{\partial{W1}}=X^T$$
$$\frac{\partial{Z1}}{\partial{B1}}=\frac{\partial{(W1*X+B1)}}{\partial{B1}}=1^T$$

所以

$$dW1=\frac{\partial{Loss}}{\partial{W1}}=\frac{\partial{Loss}}{\partial{Z1}}*\frac{\partial{Z1}}{\partial{W1}}=dZ1*X^T$$

$$dB1=\frac{\partial{Loss}}{\partial{B1}}=\frac{\partial{Loss}}{\partial{Z1}}*\frac{\partial{Z1}}{\partial{B1}}=dZ1*1^T$$

刚才水喝多了，上趟厕所，洗干净手，以免在写出如下代码时出现bug：

```Python
# 反向传播
def back_propagation(dict_params,dict_cache,X,Y):
    W1 = dict_params["W1"]
    W2 = dict_params["W2"]

    A1 = dict_cache["A1"]
    A2 = dict_cache["A2"]
    Z1 = dict_cache["Z1"]

    # output layer backward
    dZ2 = A2-Y
    dW2 = np.dot(dZ2,A1.T)
    dB2 = np.sum(dZ2,axis=1,keepdims=True)

    # hidden layer backward
    dZ1 = np.dot(W2.T,dZ2)*(1-np.square(A1))
    dW1 = np.dot(dZ1,X.T)
    dB1 = np.sum(dZ1,axis=1,keepdims=True)

    dict_grads = {
        "dW1": dW1,
        "dB1": dB1,
        "dW2": dW2,
        "dB2": dB2
    }
    return dict_grads
```

## 定义梯度下降计算公式

梯度下降的实际操作，实际上就是把以前的权重值W/B取出来，减去梯度值与步进值的乘积，当然这都是矩阵操作。步进值就是learning_rate，又叫做学习率。

```Python
def update_params(dict_params, dict_grads, learning_rate):
    W1 = dict_params["W1"]
    B1 = dict_params["B1"]
    W2 = dict_params["W2"]
    B2 = dict_params["B2"]

    dW1 = dict_grads["dW1"]
    dB1 = dict_grads["dB1"]
    dW2 = dict_grads["dW2"]
    dB2 = dict_grads["dB2"]

    # 梯度下降的实际实现
    W1=W1-learning_rate*dW1
    B1=B1-learning_rate*dB1
    W2=W2-learning_rate*dW2
    B2=B2-learning_rate*dB2

    dict_params = {"W1": W1,
                  "B1": B1,
                  "W2": W2,
                  "B2": B2}
    return dict_params
```

推导了半天，写代码就这么几行！核心工作我们都做完了，下面是写与数据相关的一些代码。

# 数据加载与初始化

我们的目的只是对0~1做训练，提取出来的数据和原MNIST数据集的形式/格式一样：训练图片集，训练标记集，测试图片集，测试标记集。唯一不同的是这4个数据集中，只包含0和1两种图片，并且0和1的顺序也是打乱的，并不是说前面的都是0，后面的都是1，这一点对于训练数据准备来说很重要。各个数据集的数量是：

* 训练图片集：12665，读入内存后是一个12665x28x28的三维数组
* 训练标记集：12665，读入内存后是一个12665x1的一维数组
* 测试图片集：2115，读入内存后是一个2115x28x28的三维数组
* 测试标记集：2115，读入内存后是一个2115x1的一维数组

## 数据加载

```Python
# 定义文件名
train_images_file = './MNIST/train-images-01'
train_labels_file = './MNIST/train-labels-01'
test_images_file = './MNIST/test-images-01'
test_labels_file = './MNIST/test-labels-01'

# read training data from image file
def parse_images_file(image_file):
    fileImage = open(image_file, 'rb')
    bin_data = fileImage.read()
    offset = 0
    fmt_header = '>IIII'
    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)
    print ("magic:%d, count: %d, size: %d*%d" % (magic_number, num_images, num_rows, num_cols))

    image_size = num_rows * num_cols
    offset += struct.calcsize(fmt_header)
    fmt_image = '>' + str(image_size) + 'B'
    images = np.empty((num_images, num_rows, num_cols))
    for i in range(num_images):
        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols))
        offset += struct.calcsize(fmt_image)

    print("read image file done:", image_file, num_images)
    return images


def parse_labels_file(label_file):
    fileLabel = open(label_file, 'rb').read()
    bin_data = fileLabel.read()
    offset = 0
    fmt_header = '>ii'
    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)
    print("magic:%d, num_images: %d" % (magic_number, num_images))

    offset += struct.calcsize(fmt_header)
    fmt_image = '>B'
    labels = np.empty(num_images)

    for i in range(num_images):
        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]
        offset += struct.calcsize(fmt_image)

    print("read label file done:", label_file, num_images)
    return labels

def load_train_images():
    return parse_images_file(train_images_file)

def load_train_labels():
    return parse_labels_file(train_lables_file)

def load_test_images():
    return parse_images_file(test_images_file)

def load_test_labels():
    return parse_labels_file(test_labels_file)
```

## 数据初始化

```Python
def initialize_array(num_input,nn_hidden,nn_output):
    np.random.seed(2)
    # W1=np.random.randn(nn_hidden,num_input)*0.00000001    # W1=np.random.randn(nn_hidden,num_input)
    W1=np.random.uniform(-np.sqrt(6)/np.sqrt(num_input+nn_hidden),np.sqrt(6)/np.sqrt(nn_hidden+num_input),size=(nn_hidden,num_input))
    # W1=np.reshape(32,784)
    B1=np.zeros((nn_hidden,1))
    # W2=np.random.randn(nn_output,nn_hidden)*0.00000001  # W2=np.random.randn(nn_output,nn_hidden)
    W2=np.random.uniform(-np.sqrt(6)/np.sqrt(nn_output+nn_hidden),np.sqrt(6)/np.sqrt(nn_output+nn_hidden),size=(nn_output,nn_hidden))
    B2=np.zeros((nn_output,1))

    dict_params = {
        "W1": W1,
        "B1": B1,
        "W2": W2,
        "B2": B2}

    return dict_params

# change 28x28 to 784x1
def image2vector(image):
    one_image_vector=np.reshape(image,[784,1])
    return one_image_vector

# change 0-255 to 0.000~1.000
def normalize_data(one_image_vector):
    a_max=np.max(one_image_vector)
    a_min=np.min(one_image_vector)
    for j in range(one_image_vector.shape[0]):
        norm_one_image_vector[j]=(one_image_vector[j]-a_min)/(a_max-a_min)

    return norm_one_image_vector

```
## 定义帮助函数

```Python
def sigmoid(x):
    y=1/(1+np.exp(-x))
    return y

def softmax(x):
    y=np.argmax(x)
    return y

def SaveModel(dict_params):
    W1=dict_params["W1"]
    B1=dict_params["B1"]
    W2=dict_params["W2"]
    B2=dict_params["B2"]
    np.save("w1.npy", W1)
    np.save("w2.npy", W2)
    np.save("B1.npy", B1)
    np.save("B2.npy", B2)

def LoadModel():
    W1 = np.load("w1.npy")
    W2 = np.load("w2.npy")
    B1 = np.load("B1.npy")
    B2 = np.load("B2.npy")
    loadParam = {"W1": W1,
                 "B1": B1,
                 "W2": W2,
                 "B2": B2}
    return loadParam

def Test(test_images, test_labels, dict_params):
    ii=0
    #test_loop = 10000 this if for 0~9
    test_loop = test_images.shape[0]
    for i in range(test_loop):
        img_train=test_images[i]
        vector_image=normalize_data(image2vector(img_train))
        label_trainx=test_labels[i]
        aa2,xxx=forward_propagation(vector_image,dict_params)
        predict_value=softmax(aa2)
        if predict_value==int(label_trainx):
            ii=ii+1

        # print("the real value is: ",label_trainx)
        # print("the value of our prediction is: ",predict_value)

    print(ii)
```

## 写程序主流程

```Python
if __name__ == '__main__':
    train_images = load_train_images()
    train_labels = load_train_labels()
    test_images = load_test_images()
    test_labels = load_test_labels()
 
    # input data dimension
    num_input=28*28
    # neuron numbers in hidden layer
    nn_hidden=10
    # output layer neuron numbers
    nn_output=2

    dict_params=initialize_with_zeros(num_input,nn_hidden,nn_output)

    # train
    # loop = 50000 this if for 0~9
    loop = train_images.shape[0]
    for i in range(loop):
        img_train=train_images[i]
        label_train1=train_labels[i]
        label_train=np.zeros((nn_output,1))
        # change learning rate to smaller upon with the training loop
        ttt=0.001
        if i>1000:
            ttt=ttt*0.999
        label_train[int(train_labels[i])]=1

        vector_image=image2vector(img_train)

        norm_image=normalize_data(vector_image)

        A2,dict_cache=forward_calculation(norm_image,dict_params)

        pre_label=softmax(A2)

        cost = getCost(A2,label_train,dict_params)
        dict_grads = back_propagation(dict_params, dict_cache, imgvector, label_train)

        dict_params = update_para(dict_params, dict_grads, learning_rate = ttt)
        dict_grads["dW1"]=0
        dict_grads["dW2"]=0
        dict_grads["dB1"]=0
        dict_grads["dB2"]=0

        print("cost after iteration %i:"%(i))
        print(cost)

    Test(test_images, test_labels, dict_params)
    # save the model
#   SaveModel(dict_params)
#   loadParam = LoadModel()
#   Test(test_images, test_labels, loadParam)
```
