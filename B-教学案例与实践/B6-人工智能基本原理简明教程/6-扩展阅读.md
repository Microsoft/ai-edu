Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 可视化训练结果

可视化的训练结果会对我们的调试、学习有巨大的帮助。我们的训练结果是个关于W,B的矩阵，如何画出分界线（下图中的橙色和蓝绿色的直线）？
<img src=".\Images\6\200-247-050.png">
```
W:
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
B:
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```
以这个训练结果为例，回忆一下前向计算的过程：

$$
Z = W*X+B
$$
实际上是以下计算的组合：
$$
Z1 = W1*X+B1=\begin{pmatrix}w_{11} & w_{12}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b1=w_{11}*x_1 + w_{12}*x_2+b1\\
Z2 = W2*X+B2=\begin{pmatrix}w_{21} & w_{22}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b2=w_{21}*x_1 + w_{22}*x_2+b2 \\
Z3 = W3*X+B3=\begin{pmatrix}w_{31} & w_{32}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b3=w_{31}*x_1 + w_{32}*x_2+b3
$$

如果想画出Z1/Z2之间的分界线，那么令Z1 = Z2就可以了：
$$
Z1 = Z2 \\
w_{11}*x_1 + w_{12}*x_2+b1 = w_{21}*x_1 + w_{22}*x_2+b2 \\
x_2 = \frac{(w_{21}-w_{11})*x_1+b2-b1}{w_{12}-w_{22}}
$$
同理，Z2/Z3的分界线是：
$$
Z2 = Z3 \\
x_2 = \frac{(w_{31}-w_{21})*x_1+b3-b2}{w_{22}-w_{32}}
$$

然后把求得的W,B的矩阵值分别代入就可以了。这是上图中那条绿色的分割线：

$$
x_2 = \frac{(2.865-0.224)*x_1+0.950-8.513}{-18.076-0.276} \\
x_2 = -0.144x_1 + 0.412
$$
这是上图中红色的分割线：
$$
x_2 = \frac{(-3.089-2.865)*x_1-9.463-0.950}{0.276-17.799} \\
x_2 = 0.339x_1 + 0.594
$$


# Softmax函数的Python实现

```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
笔者在前文的测试中，使用随机梯度下降方法，试图计算整体损失函数值时，这样写的代码：
```Python
def check_diff_cross(X, Y, W, B, count, prev_loss):
    Z = Forward(W,X,B)
    A = Softmax2(Z)
    p1 = (1-Y) * np.log(1-A)
    p2 = Y * np.log(A)
    # binary classification
    #LOSS = np.sum(-(p1 + p2))
    # multiple classification
    LOSS = np.sum(-p2)
    loss = LOSS / count
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss

for iteration in range(max_iteration):
    for i in range(num_samples):
        Xm = X[:,i].reshape(num_features,1)
        Ym = Y[:,i].reshape(num_category,1)
        Z = Forward(W,Xm,B)
        A = Softmax2(Z)
        dw,db = BackPropagation(Xm, Ym, A)
        W = W - eta * dw
        B = B - eta * db

        loss, diff_loss = check_diff_cross(X, Y, W, B, num_samples, prev_loss)
        prev_loss = loss
        print(loss, diff_loss)
        if diff_loss < eps:
            break

    print(iteration)
    if diff_loss < eps:
        break

```
但是得到的loss值和diff_loss值总居高不下，从来不会到达diff_loss < 1e-10而结束训练的效果。这是为什么呢？

开启Debug大法，看看每一步都是怎么回事儿！

由于是随机梯度下降，每次只使用一个样本，所以在主程序里计算Softmax2(Z)时是这样的值：
```
Z
array([[0.        ],
       [0.        ],
       [0.37513412]])

A
array([[0.28941997],
       [0.28941997],
       [0.42116006]])
```
得到的A值无比正确！说明我们的Softmax2函数正常工作了。

然后进入下面的check_diff_cross函数，在里面又调用了一次Softmax2函数，值是这样的：
```
Z
array([[0.39984422, 0.36663887, 0.39279115, 0.40192722, 0.40127821,
        0.38525265, 0.38244349, 0.36781724, 0.35705948, 0.39823856,
        0.37349707, 0.39216078, 0.38434552, 0.36263786, 0.36417886,
        0.35889674, 0.36924895, 0.40828777, 0.3947329 , 0.37949865,
        0.36893409, 0.39938841, 0.41674424, 0.36955942, 0.39969872,
        0.40445357, 0.41811252, 0.40688918, 0.41305447, 0.394483  ,
        0.37855679, 0.4057829 , 0.38210023, 0.41150485, 0.39825762,
        0.39004339, 0.37111255, 0.36772088, 0.37642829, 0.36200606,
        0.36712204, 0.37618966, 0.38577516, 0.35577969, 0.41544213,
        0.36360166, 0.40676256, 0.36199845, 0.41015781, 0.41013568,
        0.3557386 , 0.38592222, 0.38316238, 0.40863218, 0.3988269 ,
        0.38307085, 0.38244498, 0.38550292, 0.40052614, 0.41579697,
        0.35726004, 0.40051645, 0.36162111, 0.38425648, 0.40334634,
        0.36095495, 0.41723903, 0.41317813, 0.3786523 , 0.41907961,
        0.38805303, 0.4045512 , 0.38468882, 0.37...
A
array([[0.0018422 , 0.00178204, 0.00182926, 0.00184605, 0.00184485,
        0.00181552, 0.00181043, 0.00178414, 0.00176505, 0.00183925,
        0.0017943 , 0.0018281 , 0.00181387, 0.00177492, 0.00177766,
        0.00176829, 0.0017867 , 0.00185783, 0.00183281, 0.0018051 ,
        0.00178613, 0.00184137, 0.0018736 , 0.00178725, 0.00184194,
        0.00185072, 0.00187617, 0.00185523, 0.0018667 , 0.00183235,
        0.0018034 , 0.00185318, 0.00180981, 0.00186381, 0.00183928,
        0.00182424, 0.00179003, 0.00178397, 0.00179957, 0.0017738 ,
        0.0017829 , 0.00179914, 0.00181647, 0.00176279, 0.00187116,
        0.00177663, 0.00185499, 0.00177379, 0.0018613 , 0.00186126,
        0.00176272, 0.00181674, 0.00181173, 0.00185847, 0.00184033,
        0.00181156, 0.00181043, 0.00181597, 0.00184346, 0.00187183,
        0.0017654 , 0.00184344, 0.00177312, 0.00181371, 0.00184867,
        0.00177194, 0.00187453, 0.00186693, 0.00180358, 0.00187798,
        0.00182061, 0.0018509 , 0.0018145 , 0.00...
```
因为Z和A都是3x200的数组，所以我们只看一组数值：
```
Z[:,0]
array([0., 0., 0.49756787])
A[:,0]
array([0.00139904, 0.00139904, 0.00230102])
```
欸！不对欸！A是Z的Softmax结果，应该是：[0.27436978, 0.27436978, 0.45126044]，但debug出来却是：[0.00139904 , 0.00139904, 0.00203132]，这个明显不对！

在Debug进入Softmax2函数，检查到这一行：
```Python
A = exp_Z / np.sum(exp_Z)
```
后面的np.sum(exp_Z)的值等于405.3137756，是一个标量，然后计算出来的A就变成了exp_Z矩阵整个除以一个标量，这已经不是Softmax的原意了，应该是exp_Z只除以和自己相关的列的和才有意义，所以np.sum(exp_Z)应该也是一个数组才对，应该是一个1x200的数组，代表了每个样本（每列是一个样本）的sum值。

思路有了，改一下代码：
```Python
A = exp_Z / np.sum(exp_Z, axis=0)
```
axis=0是重点，它的意思是要把exp_Z针对每一列的3个值单独求和，得出一个200个元素的数组，代表了200个Z数据的每一列的和。图示如下：

$$
expZ = 
\begin{pmatrix}
z1,1 & z1,2 & z1,3 \dots z1,200 \\
z2,1 & z2,2 & z2,3 \dots z2,200 \\
z3,1 & z3,2 & z3,3 \dots z3,200 \\
\end{pmatrix}
$$

np.sum(expZ, axis=0)的结果：

$$
sum = \begin{pmatrix} (z1,1+z2,1+z3,1) & (z1,2+z2,2+z3,2) \dots (z1,200+z2,200+z3,200) \end{pmatrix}
$$

$$ 
=\begin{pmatrix}
z,1 & z,2 & z,3 \dots z,200
\end{pmatrix}
$$

然后再执行A = exp_Z / np.sum(exp_Z, axis=0)这一行代码，就会得到正确的A数组值。再看一下A[:,0]的结果：
```
[0.27436978 0.27436978 0.45126044]
```
Yes! Exactly what we want!

运算后可以看到Loss值不断在0.112附件徘徊，而diff_loss值不断减小，终于小于1e-10了！收敛！

# 提高训练速度

对于这一个线性分类的例子，虽然目标是3个分类，但是对于原始数据为200个样本这样的单位，训练的迭代次数达到50000次之多，而且最后的结果还不是很准确，这个很令人费解。

这让我想起了前面遇到过的数据归一化的问题，就是在《多入单出单层网络》这一章节，有三个维度的特征值，但是其取值范围相差太大，所以必须对三个维度分别做归一化，才能得到收敛的结果。那么本章的问题是不是也需要数据归一化呢？

从数据上看，温度取值范围是[0,40]，湿度取值范围是[0,80]，貌似都在[0,100]的区间内，还能接受，否则也不会得到收敛的训练结果。但是如果做了归一化，会不会得到额外的好处呢？试一下！

先加一个归一化的函数：

```Python
def Normalize(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    w_num = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        x_max = np.max(x)
        x_min = np.min(x)
        w_num[0,i] = x_max - x_min
        x_new = (x - x_min)/(x_max-x_min)
        X_new[i,:] = x_new
    return X_new, w_num
```

在主程序里增加归一化调用：
```Python
XData, Y = LoadData()
X, w_num = Normalize(XData)
num_features = X.shape[0]
num_samples = X.shape[1]
num_category = Y.shape[0]

assert(X.shape[1] == Y.shape[1])

W, B = InitialWeights(num_category, num_features)
eta = 0.1
max_iteration = 100

for iteration in range(max_iteration):
    for i in range(num_samples):
        Xm = X[:,i].reshape(num_features,1)
        Ym = Y[:,i].reshape(num_category,1)
        Z = Forward(W,Xm,B)
        A = Softmax(Z)
        dw,db = BackPropagation(Xm, Ym, A)
        W = W - eta * dw
        B = B - eta * db
    print(iteration)
print(W)
print(B)

show_result(X,Y,W,B,np.max(X[0,:]),eta,max_iteration,num_samples, True)
```
我们使用0.1的学习率，设置迭代次数为100次试试看：

![](
https://raw.githubusercontent.com/Microsoft/ai-edu/xiaowuhu/AddBlog6/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Images/6/200-100-010.png)


```
W:
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
B:
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```

瞬间完成！而且效果相当理想！增加迭代次数会得到更精确的结果。


