Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# Softmax函数的Python实现

# 反向传播公式推导

$$
Z = W*X+B \\
A=Softmax(Z) \\
Loss = -Y*lnA \\
\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}} \\
=-\frac{Y}{A} A(1-A)X^T=Y(A-1)X^T
$$

# 视觉化训练结果

如何画出分界线？

# 提高训练速度

对于这一个线性分类的例子，虽然目标是3个分类，但是对于原始数据为200个样本这样的单位，训练的迭代次数达到50000次之多，而且最后的结果还不是很准确，这个很令人费解。

这让我想起了前面遇到过的数据归一化的问题，就是在《多入单出单层网络》这一章节，有三个维度的特征值，但是其取值范围相差太大，所以必须对三个维度分别做归一化，才能得到收敛的结果。那么本章的问题是不是也需要数据归一化呢？

从数据上看，温度取值范围是[0,40]，湿度取值范围是[0,80]，貌似都在[0,100]的区间内，还能接受，否则也不会得到收敛的训练结果。但是如果做了归一化，会不会得到额外的好处呢？试一下！

先加一个归一化的函数：

```Python
def Normalize(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    w_num = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        x_max = np.max(x)
        x_min = np.min(x)
        w_num[0,i] = x_max - x_min
        x_new = (x - x_min)/(x_max-x_min)
        X_new[i,:] = x_new
    return X_new, w_num
```

在主程序里增加归一化调用：
```Python
XData, Y = LoadData()
X, w_num = Normalize(XData)
num_features = X.shape[0]
num_samples = X.shape[1]
num_category = Y.shape[0]

assert(X.shape[1] == Y.shape[1])

W, B = InitialWeights(num_category, num_features)
eta = 0.1
max_iteration = 100

for iteration in range(max_iteration):
    for i in range(num_samples):
        Xm = X[:,i].reshape(num_features,1)
        Ym = Y[:,i].reshape(num_category,1)
        Z = Forward(W,Xm,B)
        A = Softmax(Z)
        dw,db = BackPropagation(Xm, Ym, A)
        W = W - eta * dw
        B = B - eta * db
    print(iteration)
print(W)
print(B)

show_result(X,Y,W,B,np.max(X[0,:]),eta,max_iteration,num_samples, True)
```
我们使用0.1的学习率，设置迭代次数为100次试试看：


<img src=".\Images\6\200-100-010.png">

```
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```

# 损失函数
