Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# Softmax函数的Python实现

```Python
def softmax(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```


```Python
def softmax(z):
    shift_z = z - np.max(z)
    exp_z = np.exp(shift_z)
    A = exp_z / np.sum(exp_z, axis=0)
    return v
```
axis=0是重点！

# 反向传播公式推导

$$
\because
Z = W*X+B \\
A=Softmax(Z) \\
Loss = -Y*lnA \\
\therefore
\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}} \\
=-\frac{Y}{A} A(1-A)X^T=Y(A-1)X^T
$$

# 提高训练速度

对于这一个线性分类的例子，虽然目标是3个分类，但是对于原始数据为200个样本这样的单位，训练的迭代次数达到50000次之多，而且最后的结果还不是很准确，这个很令人费解。

这让我想起了前面遇到过的数据归一化的问题，就是在《多入单出单层网络》这一章节，有三个维度的特征值，但是其取值范围相差太大，所以必须对三个维度分别做归一化，才能得到收敛的结果。那么本章的问题是不是也需要数据归一化呢？

从数据上看，温度取值范围是[0,40]，湿度取值范围是[0,80]，貌似都在[0,100]的区间内，还能接受，否则也不会得到收敛的训练结果。但是如果做了归一化，会不会得到额外的好处呢？试一下！

先加一个归一化的函数：

```Python
def Normalize(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    w_num = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        x_max = np.max(x)
        x_min = np.min(x)
        w_num[0,i] = x_max - x_min
        x_new = (x - x_min)/(x_max-x_min)
        X_new[i,:] = x_new
    return X_new, w_num
```

在主程序里增加归一化调用：
```Python
XData, Y = LoadData()
X, w_num = Normalize(XData)
num_features = X.shape[0]
num_samples = X.shape[1]
num_category = Y.shape[0]

assert(X.shape[1] == Y.shape[1])

W, B = InitialWeights(num_category, num_features)
eta = 0.1
max_iteration = 100

for iteration in range(max_iteration):
    for i in range(num_samples):
        Xm = X[:,i].reshape(num_features,1)
        Ym = Y[:,i].reshape(num_category,1)
        Z = Forward(W,Xm,B)
        A = Softmax(Z)
        dw,db = BackPropagation(Xm, Ym, A)
        W = W - eta * dw
        B = B - eta * db
    print(iteration)
print(W)
print(B)

show_result(X,Y,W,B,np.max(X[0,:]),eta,max_iteration,num_samples, True)
```
我们使用0.1的学习率，设置迭代次数为100次试试看：

![](
https://raw.githubusercontent.com/Microsoft/ai-edu/xiaowuhu/AddBlog6/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Images/6/200-100-010.png)


```
W:
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
B:
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```

瞬间完成！而且效果相当理想！增加迭代次数会得到更精确的结果。


# 可视化训练结果

可视化的训练结果会对我们的调试、学习有巨大的帮助。我们的训练结果是个关于W,B的矩阵，如何画出分界线？

```
W:
[[  0.22420633 -18.07583433]
 [  2.8656193    0.27604907]
 [ -3.08982563  17.79978526]]
B:
[[ 8.51345929]
 [ 0.95025845]
 [-9.46371775]]
```
以这个训练结果为例。
回忆一下前向计算的过程：

$$
Z = W*X+B
$$
实际上是以下的组合：
$$
Z1 = W1*X+B1=\begin{pmatrix}w_{11} & w_{22}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b1=w_{11}*x_1 + w_{12}*x_2+b1\\
Z2 = W2*X+B2=\begin{pmatrix}w_{21} & w_{22}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b2=w_{21}*x_1 + w_{22}*x_2+b2 \\
Z3 = W3*X+B3=\begin{pmatrix}w_{31} & w_{32}\end{pmatrix}*\begin{pmatrix}x_1 \\ x_2 \end{pmatrix} + b3=w_{31}*x_1 + w_{32}*x_2+b3
$$

如果想画出Z1/Z2之间的分界线，那么令Z1 = Z2就可以了：
$$
Z1 = Z2 \\
w_{11}*x_1 + w_{12}*x_2+b1 = w_{21}*x_1 + w_{22}*x_2+b2 \\
x_2 = \frac{(w_{21}-w_{11})*x_1+b2-b1}{w_{12}-w_{22}}
$$
同理，Z2/Z3的分界线是：
$$
Z2 = Z3 \\
x_2 = \frac{(w_{31}-w_{21})*x_1+b3-b2}{w_{22}-w_{32}}
$$

然后把求得的W,B的矩阵值分别代入就可以了。这是上图中那条绿色的分割线：

$$
x_2 = \frac{(2.865-0.224)*x_1+0.950-8.513}{-18.076-0.276} \\
x_2 = -0.144x_1 + 0.412
$$
这是上图中红色的分割线：
$$
x_2 = \frac{(-3.089-2.865)*x_1-9.463-0.950}{0.276-17.799} \\
x_2 = 0.339x_1 + 0.594
$$


# 损失函数

