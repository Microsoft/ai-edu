Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 知识点

- 线性分类
- Softmax使用
- 交叉熵代价函数

# 提出问题

由于北京的特殊地理位置，政治经济文化医疗教育.......的中心，人口密集，城市拥挤，空气污染问题一直被人们所关注。下面这组数据，是不同温度、湿度下的PM2.5（可入肺颗粒物）、PM10（可吸入颗粒物）、TSP（总悬浮颗粒物）在地表人类活动层的主要占比。

下图中，红点代表PM2.5，蓝点代表PM10，绿点代表TSP。

<img src=".\Images\6\data.png">

举例来说，当温度是25摄氏度，湿度是40时，处于蓝色点区域，即PM10占比较大，并不是说这个条件下污染物只有PM10。

## 问题：当给你一个温度和湿度的组合值时，预测当前那种污染物占主要比重？

你可能会觉得这个太简单了，这不是有图吗？定位坐标值后一下子就找到对应的区域了。但是我们要求你用机器学习的方法来解决这个看似简单的问题，以便将来的预测行为是快速准确的，而不是拿个尺子在图上去比划。再说了，我们用这个例子，主要是想让大家对问题和解决方法都有一个视觉上的清晰认识，而这类可以可视化的问题，在实际生产环境中并不多见，是我们精心挑选的例子。

## 问题分析

从图示来看，似乎在三个颜色区间之间有两个比较明显的分界线，而且是直线，即线性可分的。我们如何通过神经网络精确地找到这两条分界线呢？

- 从视觉上判断是线性可分的，所以我们使用单层神经网络即可
- 输入特征是温度和湿度，所以我们在输入层设置两个输入X1=温度值，X2=湿度值
- 最后输出的是三个分类，分别是PM2.5，PM10，TSP，所以输出层有三个神经元

# 定义神经网络结构

这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。

<img src=".\Images\6\NN.jpg">

与前面的单层网络不同的是，本图最右侧的输出层还多出来一个Softmax分类函数（可以看作是激活函数），这是多分类任务中的标准配置。前面我们讲激活函数时提到过它，大家可以去复习一下相关知识。

## 输入层

## 权重矩阵W1/B1

W权重矩阵的尺寸，可以从后往前看，比如：输出层是3个神经元，输入层是2个特征，则W的尺寸就是3x2。

$$
W=\begin{pmatrix}
W1 \\
W2 \\
W3
\end{pmatrix}
=\begin{pmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22} \\
w_{31} & w_{32} 
\end{pmatrix}
$$

B的尺寸是3x1，行数永远和W一样，列数永远是1。

$$
B=\begin{pmatrix}
b_1 \\
b_2 \\
b_3 
\end{pmatrix}
$$

## 输出层

输出层三个神经元，再加上一个Softmax计算，最后有A1,A2,A3三个输出，写作：

$$
Z = \begin{pmatrix}Z1 \\ Z2 \\ Z3 \end{pmatrix},
A = \begin{pmatrix}A1 \\ A2 \\ A3 \end{pmatrix}
$$

其中，$Z=W*X+B，A = Softmax(Z)$

# 训练数据

## 下载数据
下载后拷贝到您要运行的Python文件所在的文件夹。

[点击下载训练样本数据X](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/WeatherXData.dat)

[点击下载训练标签数据Y](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/WeatherYData.dat)




样本数据集中一共有200个数据，每个数据有两个特征，温度和湿度。所以定义矩阵如下：

$$
X^m = 
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
$$

$X^m$表示第m个样本值，$x^m_n$表示第m个样本的第n个特征值。
样本数据集中一共有200个数据，每个数据有两个特征：温度和湿度。所以定义矩阵如下：

$$
X = 
\begin{pmatrix}
X^1 & X^2 \dots X^{200}
\end{pmatrix}=
\begin{pmatrix}
x^1_1 & x^2_1 \dots x^{200}_1 \\
\\
x^1_2 & x^2_2 \dots x^{200}_2 \\
 \end{pmatrix}
$$

样本标签数据：
$$
Y = 
\begin{pmatrix}
Y^1 & Y^2 \dots Y^m
\end{pmatrix}=
\begin{pmatrix}
y^1_1 & y^2_1 \dots y^{200}_1 \\
\\
y^1_2 & y^2_2 \dots y^{200}_2 \\
\\
y^1_3 & y^2_3 \dots y^{200}_3
\end{pmatrix}=
\begin{pmatrix}
1 & 0 \dots 0 \\
0 & 0 \dots 1 \\
0 & 1 \dots 0
\end{pmatrix}
$$

标签数据对应到每个样本数据上，列对齐，只有(1,0,0)，(0,1,0)，(0,0,1)三种组合，分别表示第一类(PM2.5)、第二类(PM10)和第三类(TSP)污染物所主要比重。当然我们不能排除某一时刻既有PM2.5又有PM10的情况，但两者总有一个占比重大，一个占比重小，所以就指定占比大的那个为1，其余两个为0。

## 加载数据
```Python
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def LoadData():
    Xfile = Path("WeatherXData.dat")
    Yfile = Path("WeatherYData.dat")
    if Xfile.exists() & Yfile.exists():
        XData = np.load(Xfile)
        YData = np.load(Yfile)
        return XData,YData
    
    return None,None
```

# 前向计算

```Python
def Forward(W, X, B):
    Z = np.dot(W,X) + B
    return Z
```

# 分类函数

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    A = exp_z / np.sum(exp_z)
    return A
```

# 反向传播
```Python
def BackPropagation(Xm, Ym, A):
    dloss_z = A - Ym
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db
```

# 初始化
```Python
def InitialWeights(num_category, num_features):
    W = np.zeros((num_category,num_features))
    B = np.zeros((num_category,1))
    return W, B
```

# 可视化训练结果
```Python
def show_result(X,Y,W,B,rangeX,eta,iteration,num_samples):
    for i in range(num_samples):
        if Y[0,i] == 0 and Y[1,i]==0 and Y[2,i]==1:
            plt.scatter(X[0,i], X[1,i], c='r')
        elif Y[0,i] == 0 and Y[1,i]==1 and Y[2,i]==0:
            plt.scatter(X[0,i], X[1,i], c='b')
        else:
            plt.scatter(X[0,i], X[1,i], c='g')
   
    b12 = (B[1,0] - B[0,0])/(W[0,1] - W[1,1])
    w12 = (W[1,0] - W[0,0])/(W[0,1] - W[1,1])

    b23 = (B[2,0] - B[1,0])/(W[1,1] - W[2,1])
    w23 = (W[2,0] - W[1,0])/(W[1,1] - W[2,1])

    x = np.linspace(0,rangeX,10)
    y = w12 * x + b12
    plt.plot(x,y)

    x = np.linspace(0,rangeX,10)
    y = w23 * x + b23
    plt.plot(x,y)

    title = str.format("eta:{0}, iteration:{1}, samples:{2}", eta, iteration, num_samples)
    plt.title(title)
    
    plt.xlabel("Temperature")
    plt.ylabel("Humidity")
    plt.show()
```

# 主程序
```Python
X, Y = LoadData()
num_features = X.shape[0]
num_samples = X.shape[1]
num_category = Y.shape[0]

assert(X.shape[1] == Y.shape[1])

W, B = InitialWeights(num_category, num_features)
eta = 0.1
max_iteration = 10000

for iteration in range(max_iteration):
    for i in range(num_samples):
        Xm = X[:,i].reshape(num_features,1)
        Ym = Y[:,i].reshape(num_category,1)
        Z = Forward(W,Xm,B)
        A = Softmax(Z)
        dw,db = BackPropagation(Xm, Ym, A)
        W = W - eta * dw
        B = B - eta * db
    print(iteration)
    print(W)
    print(B)

show_result(X,Y,W,B,np.max(X[0,:]),eta,max_iteration,num_samples, True)
```
# 预训练

在花大把的时间去训练之前，我们先确定一下eta的值，至少确认一个参数，能省些力气。
我们先设置max_iteration=1000，再把eta设置成0.1和0.01两个值来比较一下训练结果：

<img src=".\Images\6\200-1000-010.png">
```
W:
[[ 22.84027477 -25.92474751]
 [ 13.85650934  -2.91132852]
 [-36.69678411  28.83607603]]
B:
[[ 445.75414914]
 [  -0.49907866]
 [-445.25507048]]
```
<img src=".\Images\6\200-1000-001.png">
```
W:
[[ 2.14215401 -2.55960085]
 [ 1.72926958 -0.20627381]
 [-3.87142359  2.76587466]]
B:
[[ 42.08404299]
 [  0.98929102]
 [-43.07333401]]
```
从训练过程的打印结果看，并没有溢出一类的不幸事件发生，所以，用0.1学习率大一些没问题。但是结果数据来看，0.1的数据比0.01的数据要大10倍左右。

## 逐步增加迭代次数
确定好eta=0.1后，分别设置max_iteration为5000，10000，25000，50000，得到结果如下：

5000次时，两条线趋势良好：
<img src=".\Images\6\200-5000-010.png">

10000次时，更进一步：
<img src=".\Images\6\200-10000-010.png">

25000次时，下面那条线已经好了，上面的还差些：
<img src=".\Images\6\200-25000-010.png">

50000次时，下面的线有些变坏了，上面的线到位了：
<img src=".\Images\6\200-50000-010.png">

此时W,B的值为：
```
W:
[[ 10.39590958 -67.84869838]
 [ 14.60035459  -5.20816786]
 [-24.99626417  73.05686624]]
B:
[[ 2617.63780438]
 [  688.84199789]
 [-3306.47980227]]
```
看着这最终的数值有点儿眼晕，咱们再看看0.01的情况：

<img src=".\Images\6\200-50000-001.png">
```
W:
[[ 0.92055673 -6.57033366]
 [ 1.61453654 -0.41626998]
 [-2.53509327  6.98660364]]
B:
[[ 249.36700291]
 [  68.23608584]
 [-317.60308875]]
```

最后两组数据相比，从图示上看效果差不多，从数据上看0.01的W,B值要比0.1的W,B值小10倍左右，这是因为这个公式：$W=W-eta*dW$中dW的值，每次反馈叠加到W时，分别乘以0.1和0.01造成的10倍的差距。这也说明了另外一个问题：神经网络学习的结果并非是唯一的，很有可能因为参数不同而得到差别很大的两组权重值。但由于存在线性关系，两者都是正确的结果。

