Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

回归和分类，是神经网络擅长的两种工作。回归就是拟合，分类就是把样本通过训练分成指定的几个类别。老子说：一生二，二生三，三生万物。其中的“一”就是回归，“二”就是二分类，“三”就是多分类。二分类和多分类在实际使用种略有不同。

# Softmax 函数

softmax函数，是大名鼎鼎的在计算多分类问题时常使用的一个函数，他长成这个样子:

$$
\phi(z_j) = \frac{e^{z_j}}{\sum\limits_ie^{z_i}}
$$

也就是说把接收到的输入归一化成一个每个分量都在$(0,1)$之间并且总和为一的一个概率函数。

用一张图来形象说明这个过程

<img src=".\Images\7\softmax.png">

当输入的数据是3，1，-3时，按照图示过程进行计算，可以得出输出的概率分布是0.88，0.12，0。

试想如果我们并没有这样一个softmax的过程而是直接根据3，1，-3这样的输出，而我们期望得结果是1，0，0这样的概率分布结果，那传递给网络的信息是什么呢？我们要抑制正样本的输出，同时要抑制负样本的输出。正样本的输出和期望的差距是2，负样本1和期望的差距是0，所以网络要更加抑制正样本的结果！所以，在输出结果相对而言已经比较理想的情况下，我们给了网络一个相对错误的更新方向：更多的抑制正样本的输出结果。这显然是不可取的呀！

从继承关系的角度来说，softmax函数可以视作sigmoid的一个扩展，比如我们来看一个二分类问题，

$$
\phi(z_1) = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}} = \frac{1}{1 + e^{z_2} e^{- z_1}}
$$

是不是和sigmoid的函数形式非常像？比起原始的sigmoid函数，softmax的一个优势是可以用在多分类的问题中。另一个好处是在计算概率的时候更符合一般意义上我们认知的概率分布，体现出物体属于各个类别相对的概率大小。

既然采用了这个函数，那么怎么计算它的反向传播呢？

这里为了方便起见，假设标签对应的类别是第j类，也就是说正确的分类是第j类。将$\sum\limits_{i \neq j}e^{z_i}$记作$k$，我们来推导$\phi(z_j)$对$z_j$和$z_i$的导数。

- $\phi(z_j)$对$z_j$的导数

  $$
  \phi(z_j) = \frac{e^{z_j}}{\sum\limits_ie^{z_i}} = \frac{e^{z_j}}{k + e^{z_j}}
  $$

  $$
  \therefore \frac{\partial\phi(z_j)}{\partial z_j} = \frac{e^{z_j}(k + e^{z_j}) - e^{z_j} * e^{z_j}}{{(k + e^{z_j})}^2} = \frac{e^{z_j}}{k + e^{z_j}}\frac{k}{k + e^{z_j}} = softmax(z_j)(1 - softmax(z_j))
  $$

  也就是说，在$\phi(z_j)$对$z_j$的情况下，softmax的梯度就是$softmax(z_j)(1 - softmax(z_j))$，下面我们来看$\phi(z_j)$对$z_i$的导数

- $\phi(z_j)$对$z_i$的导数

  此时，i是指除了j的任意一类，但是是某一个确定的类，所以在这种情况下，i和j都是有明确定义的了。这里我们将用t来表示除了i之外的类别。还有一点修改是，我们修改一下k的定义，这里将$\sum\limits_{t \neq i}e^{z_t}$记作$k$。
  所以我们有:

  $$
  \phi(z_j) = \frac{e^{z_j}}{\sum\limits_te^{z_t}} = \frac{e^{z_j}}{k + e^{z_i}}
  $$

  $$
  \therefore \frac{\partial\phi(z_j)}{\partial z_i} = \frac{-e^{z_j}e^{z_i}}{(k+e^{z_i})^2} = - \frac{e^{z_j}}{k+e^{z_i}}\frac{e^{z_i}}{k+e^{z_i}} = - \frac{e^{z_j}}{\sum\limits_te^{z_t}}\frac{e^{z_i}}{\sum\limits_te^{z_t}} = - \phi(z_j) \phi(z_i) = - softmax(z_j)softmax(z_i)
  $$

# 二分类
在输出层使用单个神经元输出，使用Sigmoid激活函数和下面这种交叉熵损失函数：
$$
Z = W*X+B \\
A=Sigmoid(Z) \\
Loss = -[Y*lnA+(1-Y)*ln(1-A)] \\
$$

|<img src=".\Images\1\NeuranCell.png" width="400"/>|<img src=".\Images\1\activation.png" width="400"/>|
|---|---|

分类的方式是，可以指定当A > 0.5时是正例，A <= 0.5时就是范例。或者根据实际情况指定别的阈值比如0.3，0.8等等。

此时反向传播公式推导结果是：

$$
\frac{\partial{Loss}}{\partial{A}}=-[\frac{Y}{A}+\frac{1-Y}{1-A}*(-1)]=\frac{A-Y}{A(1-A)} \\
\frac{\partial{A}}{\partial{Z}}=A(1-A) \\
\frac{\partial{Z}}{\partial{W}}=X^T \\
\frac{\partial{Z}}{\partial{B}}=1 \\
$$

所以：

$$
\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}}=\frac{A-Y}{A(1-A)}*A(1-A)*X^T=(A-Y)*X^T \\
$$

$$
\frac{\partial{Loss}}{\partial{B}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{B}}=\frac{A-Y}{A(1-A)}*A(1-A)=A-Y
$$

# 多分类（大于两类）
在输出层使用多个神经元输出，并对多个神经元使用Softmax分类函数和下面这种交叉熵损失函数：

$$
Z = W*X+B \\
A=Softmax(Z) \\
Loss = -Y*lnA \\
$$

<img src=".\Images\6\NN.jpg">

Softmax的特点是：$A1+A2+A3=1$。分类的方式是：看A1/A2/A3三者谁的比重最大，就算是那一类。比如A1=0.6, A2=0.1, A3=0.3，则这个样本就算是第一类了。

此时反向传播公式推导结果是：

$$
\frac{\partial{Loss}}{\partial{A}}=-\frac{Y}{A} 
$$

针对Softmax的结果和Sigmoid类似，但是需要分情况讨论：

$$
\frac{\partial{A}}{\partial{Z}} = \begin{cases} a_i(1-a_i), & i = j \\ -a_ia_j, & i \neq j \end{cases}\\
$$

因此：

$$
\frac{\partial{Loss}}{\partial{Z}} = \frac{\partial{Loss}}{\partial{A}}\frac{\partial{A}}{\partial{Z}} = [\sum_{t}-\frac{Y}{a_t}\frac{a_t}{Z}] = [-\frac{y_j}{a_j}a_j(1-a_j) - \sum_{t \neq j}\frac{y_t}{a_t}(-a_ta_j)] = [a_j(\sum{y_t}) - y_j] = [a_j - y_j] = A - Y
$$

$$
\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}}=(A - Y)X^T\\
$$

$$
\frac{\partial{Loss}}{\partial{B}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{B}}=(A - Y)
$$


# 两种方式的差别
如果用二分类的反向传播公式放在多分类里，也是能工作的。因为笔者开始不知道二者的区别，所以两者都试验了一下。YONG
## 二分类

以下是二分类方式的code，但是也可以用于多分类：

```Python
def BackPropagation(Xm, Ym, A):
    # binary classification
    dloss_z = A - Ym
    # multiple classification
    #dloss_z = Ym*(A-1)
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db

def check_diff_cross(X, Y, W, B, count, prev_loss):
    Z = Forward(W,X,B)
    A = Softmax(Z)
    p1 = (1-Y) * np.log(1-A)
    p2 = Y * np.log(A)
    # binary classification
    LOSS = np.sum(-(p1 + p2))
    # multiple classification
    #LOSS = np.sum(-p2)
    loss = LOSS / count
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```
```
W:
[[  0.05289567 -47.1947454 ]
 [  6.49271075   1.47456342]
 [ -6.54560642  45.72018198]]
B:
[[ 23.16875247]
 [  2.04046582]
 [-25.20921829]]
```
<img src=".\Images\6\200-247-050.png">

## 多分类

以下是多分类方式的code：
```Python
def BackPropagation(Xm, Ym, A):
    # binary classification
    #dloss_z = A - Ym
    # multiple classification
    dloss_z = Ym*(A-1)
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db

def check_diff_cross(X, Y, W, B, count, prev_loss):
    Z = Forward(W,X,B)
    A = Softmax(Z)
    p1 = (1-Y) * np.log(1-A)
    p2 = Y * np.log(A)
    # binary classification
    #LOSS = np.sum(-(p1 + p2))
    # multiple classification
    LOSS = np.sum(-p2)
    loss = LOSS / count
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

```
W:
[[ 77.60630265  55.93788203]
 [ 80.91158789  96.75111314]
 [ 69.91092057 127.61236902]]
B:
[[189.03244096]
 [172.97176147]
 [153.91343173]]
```
<img src=".\Images\6\200-33-050.png">

### 结论
从结果上看：
1. 二分类的迭代次数较多，247次，多分类只用了33次。这说明了多分类每次迭代的效率较高，因为多分类每次做反向传播时，只把当前分类的误差回传，比如如果当前分类是第一类，它可能只回传[-0.4,0,0]，可以看到后两类的误差都是0，不做相应改变。而对二分类的回传是这样的值：[-0.4,0.2,0.2]，这样就会同时影响对其它两类的计算，造成多余计算，甚至是错误的回传信息，从而需要更多的迭代次数来保证精度。
2. 看图示，二分类的效果比较好，多分类有一点点误差。但是，这并不能说明二分类比多分类好，因为二者迭代次数不同。我们把多分类情况下的误差值调整为1e-12后，得到的结果是迭代77次，两条分界线非常准确，迭代次数还是比二分类少。
3. 看二者W/B的值的差异，二分类的值要小很多，比如B=[23.16875247, 2.04046582, -25.20921829]，而多分类的B值很大：[189.03244096, 172.97176147, 153.91343173]。但是这两组值是有规律的，比如它们分别满足一定的线性关系，只是基值不同。这也可以说明1) 这个问题有多组解，2)神经网络有可能不能得出最优解，虽然我们不知道那个是最优解，但如果假设一个是，那另一个肯定不是。

||二分类|多分类|
|---|---|---|
|迭代次数|多|少|
|准确度|好|好|
|上例中的反向传播回传值|[-0.4,0.2,0.2]|[-0.4,0,0]|
|W1|0.05289567 -47.1947454|77.60630265  55.93788203|
|W2|6.49271075   1.47456342|80.91158789  96.75111314|
|W3|-6.54560642  45.72018198|69.91092057 127.61236902|
|B|23.1687,2.0404,-25.2092|189.0324,172.9717,153.9134|

回传值中数字本身不关键，是不是0很关键，也就是是否影响其它两类样本的训练。


# Softmax函数的Python实现

第一种，直截了当按照公式写：
```Python
def Softmax1(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```
这个可能会发生的问题是，当x很大时，np.exp(x)很容易溢出，因为是指数运算。所以，有了下面这种改进的代码：

```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
测试一下：
```Python
Z = np.array([3,0,-3])
print(Softmax1(Z))
print(Softmax2(Z))
```
两个实现方式的结果一致：
```
[0.95033021 0.04731416 0.00235563]
[0.95033021 0.04731416 0.00235563]
```

为什么一样呢？从表面上看差好多啊！我们来证明一下：

假设有3个值a，b，c，并且a在三个数中最大，则b所占的Softmax比重应该这样写：

$$P(b)=\frac{e^b}{e^a+e^b+e^c}$$

如果减去最大值变成了a-a，b-a，c-a，则b'所占的Softmax比重应该这样写：

$$P(b') = \frac{e^{b-a}}{e^{a-a}+e^{b-a}+e^{c-a}}\\ 
=\frac{e^b/e^a}{e^a/e^a+e^b/e^a+e^c/e^a} \\
= \frac{e^b}{e^a+e^b+e^c} \\
\therefore P(b) == P(b')
$$

Softmax2的写法对一个一维的向量或者数组是没问题的，如果遇到Z是个MxN维(M,N>1)的矩阵的话，就有问题了，因为np.sum(exp_Z)这个函数，会把MxN矩阵里的所有元素加在一起，得到一个标量值，而不是相关列元素加在一起。

所以应该这么写：

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    #s = np.sum(exp_z)
    s = np.sum(exp_z, axis=0)
    A = exp_z / s
    return A
```

axis=0这个参数非常非常重要，具体原因在“6-扩展阅读”这篇文章里，这里假设我们的数据矩阵中，每个列是一个样本的N个特征值。如果是每个行是特征值组合的话，则axis=1。


