Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


## softmax 函数

softmax函数，是大名鼎鼎的在计算多分类问题时常使用的一个函数，他长成这个样子:

$$
\phi(z_j) = \frac{e^{z_j}}{\sum\limits_ie^{z_i}}
$$

也就是说把接收到的输入归一化成一个每个分量都在$(0,1)$之间并且总和为一的一个概率函数。

用一张图来形象说明这个过程

<img src=".\Images\7\softmax.png">

当输入的数据是3，1，-3时，按照图示过程进行计算，可以得出输出的概率分布是0.88，0.12，0。

试想如果我们并没有这样一个softmax的过程而是直接根据3，1，-3这样的输出，而我们期望得结果是1，0，0这样的概率分布结果，那传递给网络的信息是什么呢？我们要抑制正样本的输出，同时要抑制负样本的输出。正样本的输出和期望的差距是2，负样本1和期望的差距是0，所以网络要更加抑制正样本的结果！所以，在输出结果相对而言已经比较理想的情况下，我们给了网络一个相对错误的更新方向：更多的抑制正样本的输出结果。这显然是不可取的呀！

从继承关系的角度来说，softmax函数可以视作sigmoid的一个扩展，比如我们来看一个二分类问题，

$$
\phi(z_1) = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}} = \frac{1}{1 + e^{z_2} e^{- z_1}}
$$

是不是和sigmoid的函数形式非常像？比起原始的sigmoid函数，softmax的一个优势是可以用在多分类的问题中。另一个好处是在计算概率的时候更符合一般意义上我们认知的概率分布，体现出物体属于各个类别相对的概率大小。

既然采用了这个函数，那么怎么计算它的反向传播呢？

这里为了方便起见，将$\sum\limits_{i \neq j}e^{z_i}$记作$k$，那么，

$$
\phi(z_j) = \frac{e^{z_j}}{\sum\limits_ie^{z_i}} = \frac{e^{z_j}}{k + e^{z_j}}
$$

$$
\therefore \frac{\partial\phi(z_j)}{\partial z_j} = \frac{e^{z_j}(k + e^{z_j}) - e^{z_j} * e^{z_j}}{{(k + e^{z_j})}^2} = \frac{e^{z_j}}{k + e^{z_j}}\frac{k}{k + e^{z_j}} = softmax(z_j)(1 - softmax(z_j))
$$

也就是说，softmax的梯度就是$softmax(z_j)(1 - softmax(z_j))$，之后将这个梯度进行反向传播就可以大功告成啦~


# 损失函数与反向传播公式推导

回顾一下前向计算公式中，用于分类时，激活函数和损失函数有两种组合：
## 当二分类时
在输出层使用单个神经元输出，使用Sigmoid激活函数和下面这种交叉熵损失函数：
$$
Z = W*X+B \\
A=Sigmoid(Z) \\
Loss = -[Y*lnA+(1-Y)*ln(1-A)] \\
$$
此时反向传播公式推导结果是：
$$
\frac{\partial{Loss}}{\partial{A}}=-[\frac{Y}{A}+\frac{1-Y}{1-A}*(-1)]=\frac{A-Y}{A(1-A)} \\
\frac{\partial{A}}{\partial{Z}}=A(1-A) \\
\frac{\partial{Z}}{\partial{W}}=X^T \\
\frac{\partial{Z}}{\partial{B}}=1 \\
$$
$$
\therefore
\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}}=\frac{A-Y}{A(1-A)}*A(1-A)*X^T=(A-Y)*X^T \\
\frac{\partial{Loss}}{\partial{B}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{B}}=\frac{A-Y}{A(1-A)}*A(1-A)=A-Y
$$

## 当多分类（大于两类）时
在输出层使用多个神经元输出，并对多个神经元使用Softmax分类函数和下面这种交叉熵损失函数：
$$
Z = W*X+B \\
A=Softmax(Z) \\
Loss = -Y*lnA \\
$$
此时反向传播公式推导结果是：
$$
\frac{\partial{Loss}}{\partial{A}}=-\frac{Y}{A} 
$$
针对Softmax的结果和Sigmoid类似，但是含义不同：
$$
\frac{\partial{A}}{\partial{Z}}=A(1-A)\\
$$
$$
\therefore\frac{\partial{Loss}}{\partial{W}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{W}}=-\frac{Y}{A} A(1-A)X^T=Y(A-1)X^T\\
\frac{\partial{Loss}}{\partial{B}}=\frac{\partial{Loss}}{\partial{A}}*\frac{\partial{A}}{\partial{Z}}*\frac{\partial{Z}}{\partial{B}}=-\frac{Y}{A}A(1-A)=Y(A-1)
$$

## 两种方式的差别

理论上推导不明白，我们用code来验证。

### 二分类

以下是二分类方式的code，但是也可以用于多分类：

```Python
def BackPropagation(Xm, Ym, A):
    # binary classification
    dloss_z = A - Ym
    # multiple classification
    #dloss_z = Ym*(A-1)
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db

def check_diff_cross(X, Y, W, B, count, prev_loss):
    Z = Forward(W,X,B)
    A = Softmax(Z)
    p1 = (1-Y) * np.log(1-A)
    p2 = Y * np.log(A)
    # binary classification
    LOSS = np.sum(-(p1 + p2))
    # multiple classification
    #LOSS = np.sum(-p2)
    loss = LOSS / count
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```
```
W:
[[  0.05289567 -47.1947454 ]
 [  6.49271075   1.47456342]
 [ -6.54560642  45.72018198]]
B:
[[ 23.16875247]
 [  2.04046582]
 [-25.20921829]]
```
<img src=".\Images\6\200-247-050.png">

### 多分类

以下是多分类方式的code：
```Python
def BackPropagation(Xm, Ym, A):
    # binary classification
    #dloss_z = A - Ym
    # multiple classification
    dloss_z = Ym*(A-1)
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db

def check_diff_cross(X, Y, W, B, count, prev_loss):
    Z = Forward(W,X,B)
    A = Softmax(Z)
    p1 = (1-Y) * np.log(1-A)
    p2 = Y * np.log(A)
    # binary classification
    #LOSS = np.sum(-(p1 + p2))
    # multiple classification
    LOSS = np.sum(-p2)
    loss = LOSS / count
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

```
W:
[[ 77.60630265  55.93788203]
 [ 80.91158789  96.75111314]
 [ 69.91092057 127.61236902]]
B:
[[189.03244096]
 [172.97176147]
 [153.91343173]]
```
<img src=".\Images\6\200-33-050.png">

### 结论
从结果上看：
1. 二分类的迭代次数较多，247次，多分类只用了33次。这说明了多分类每次迭代的效率较高，因为多分类每次做反向传播时，只把当前分类的误差回传，比如如果当前分类是第一类，它可能只回传[-0.4,0,0]，可以看到后两类的误差都是0，不做相应改变。而对二分类的回传是这样的值：[-0.4,0.2,0.2]，这样就会同时影响对其它两类的计算，造成多余计算，甚至是错误的回传信息，从而需要更多的迭代次数来保证精度。
2. 看图示，二分类的效果比较好，多分类有一点点误差。但是，这并不能说明二分类比多分类好，因为二者迭代次数不同。我们把多分类情况下的误差值调整为1e-12后，得到的结果是迭代77次，两条分界线非常准确，迭代次数还是比二分类少。
3. 看二者W/B的值的差异，二分类的值要小很多，比如B=[23.16875247, 2.04046582, -25.20921829]，而多分类的B值很大：[189.03244096, 172.97176147, 153.91343173]。但是这两组值是有规律的，比如它们分别满足一定的线性关系，只是基值不同。这也可以说明1) 这个问题有多组解，2)神经网络有可能不能得出最优解，虽然我们不知道那个是最优解，但如果假设一个是，那另一个肯定不是。




# Softmax函数的Python实现

第一种，直截了当按照公式写：
```Python
def Softmax1(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```
这个可能会发生的问题是，当x很大时，np.exp(x)很容易溢出，因为是指数运算。所以，有了下面这种改进的代码：

```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
测试一下：
```Python
Z = np.array([3,0,-3])
print(Softmax1(Z))
print(Softmax2(Z))
```
两个实现方式的结果一致：
```
[0.95033021 0.04731416 0.00235563]
[0.95033021 0.04731416 0.00235563]
```

但实际上，应该这么写：

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    #s = np.sum(exp_z)
    s = np.sum(exp_z, axis=0)
    A = exp_z / s
    return A
```