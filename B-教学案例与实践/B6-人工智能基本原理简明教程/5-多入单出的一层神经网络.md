Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 知识点

- 多元线性回归
- 样本数据归一化

# 提出问题

大哥，北京的房价了解一下！

房价预测问题，成为了机器学习的一个入门话题。我们也不能免俗，但是，不要用美国的什么多少平方英尺，多少个房间的例子来说事儿了，中国人不能理解为啥多出来一个room还买得贵，羊毛不是出在羊身上吗？我们来个北京的例子！

影响北京房价的因素有很多，几个最重要的因子和它们的取值范围是：
- 朝向（在北方地区，窗户面向阳光的房子要抢手一些）：北=1，西=2，东=3，南=4
- 地理位置：二环，三环，四环，五环，六环，分别取值为2，3，4，5，6，二环的房子单价最贵
- 面积：40~120平米，连续值


影响房价的因素有很多，居住面积，地理位置，朝向，是其中三个比较重要的因素。当然还有学区房什么比较特殊的情况。关于房价，我们有1000个样本，每个样本有三个特征值，一个标签值，示例如下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|整套价格（万元）|434|500|321|482|...|410|

- 特征值1 - 窗户朝向
一共有”东”“南”“西”“北“四个值，用数字化表示是：
东：3
南：4
西：2
北：1
因为朝南的房子比较贵，其次为东，西，北。为啥东比西贵？难度是因为我们常说“东西”，东在前，西在后？瞎扯！是因为夏天时朝西的窗户西晒时间长，比较热。

- 特征值2 - 地理位置
二环：2 - 单价最贵
三环：3
四环：4
五环：5
六环：6 - 单价最便宜

- 特征值3 - 房屋面积
统计所有样本数据得到房屋的面积范围是[40,120]


### 问题：在北京东五环的一套窗户朝西的93平米的房子，大概是多少钱？

由于表中可能没有恰好符合上述条件的数据，因此我们必须根据1000个样本值来建立一个模型，来解决我们的问题。

先假设这个问题是个线性回归问题，而且是典型的多元线性回归。函数模型如下：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

为了方便大家理解，咱们具体化一下上面的公式，按照本系列文章的符号约定就是：

$$ 
Z = w_1x_1+w_2x_2+w_3x_3+b = W*X + B
$$

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。这个一层的神经网络没有中间层，只有输入项和输出层（输入项不算做一层），而且只有一个神经元，并且神经元有一个线性输出，不经过激活函数处理。亦即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

<img src=".\Images\5\setup.jpg" width="600">

矩阵运算过程：$W(1,3) * X(3,1) + B(1,1) => Z(1,1)$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(1,3)表示W是一个1行3列的矩阵。

## 输入层

假设一共有m个样本，每个样本n个特征值，X就是一个$n \times m$的矩阵，模样是这样紫的（n=3，m=1000，亦即3行1000列）：

$$
X = \\
\begin{pmatrix} 
X_1 & X_2 \dots X_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{2,1} & \dots & x_{1000,1} \\
\\
x_{1,2} & x_{2,2} & \dots & x_{1000,2} \\
\\
x_{1,3} & x_{2,3} & \dots & x_{1000,3}
\end{pmatrix} = 
\begin{pmatrix}
3 & 2 & \dots & 3 \\
\\
1 & 4 & \dots & 2 \\
\\
96 & 100 & \dots & 54
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y_1 & y_2 & \dots & y_m \\
\end{pmatrix}=
\begin{pmatrix}
434 & 500 & \dots & 410 \\
\end{pmatrix}
$$

单独看一个样本是这样的：

$$
x_1 =
\begin{pmatrix}
x_{1,1} \\
\\
x_{1,2} \\
\\
x_{1,3}
\end{pmatrix} = 
\begin{pmatrix}
3 \\
\\
1 \\
\\
96
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 434 \end{pmatrix}
$$

$X_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

## 权重W和B

有人问了，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？那样好像更符合思维习惯？

确实是！但是在实际的矩阵运算时，由于是$Z=W*X+B$，W在前面，X在后面，所以必须是这个样子的：
$$
\begin{pmatrix}
w_1 & w_2 & w_3
\end{pmatrix}
\times
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix}=
w_1*x_1+w_2*x_2+w_3*x_3
$$

假设每个样本x有n个特征向量，上式中的W就是一个$1 \times n$的向量，让每个w都对应一个x：
$$
\begin{pmatrix}w_1 & w_2 \dots w_n\end{pmatrix}
$$

B是个单值，因为只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

## 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。由于是线性的，所以没有用激活函数。

# 下载训练数据

[点击下载训练数据](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HouseXData.dat)

[点击下载标签数据](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HouseYData.dat)

# 读取文件数据
```Python
def LoadData():
    Xfile = Path("HouseXData.dat")
    Yfile = Path("HouseYData.dat")
    if Xfile.exists() & Yfile.exists():
        XData = np.load(Xfile)
        YData = np.load(Yfile)
        return XData,YData
    
    return None,None
```

# 前向计算

```Python
def ForwardCalculation(Xm,W,b):
    z = np.dot(W, Xm) + b
    return z
```
# 定义代价函数

我们依然用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。

```Python
def CheckLoss(w, b, X, Y, count, prev_loss):
    Z = ForwardCalculation(X, w, b)
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/count/2
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

# 反向传播

```Python
def BackPropagation(Xm,Y,Z):
    dZ = Z - Y
    dB = dZ
    dW = np.dot(dZ, Xm.T)
    return dW, dB

def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```
上面第一个求梯度函数，第二个函数用于每次迭代时更新w，b的值。
求解W和B的梯度方法与我们前面的文章“单入单出的一层神经网络”完全一样，所以求db和dw的逻辑也是相同的，重要区别在于：

第四章是：
```Python
dW = dZ * x
```
而本章是：
```Python
dW = np.dot(dZ, Xm.T)
```

原因是在第四章中，我们的样本值只有一维特征，所以用个标量计算就可以了。在本章中，样本值有三个特征值，所以Xm是个向量，需要用到矩阵运算。Xm.T表示样本值的转置矩阵，这个的公式推导在《基本数学导数公式》中。

# 预测房价
以下代码使用问题中的参数（朝西，五环，93平米）来预测房价：
```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(w, b):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, w, b)
    print(z1)
```

# 主程序
```Python
X, Y = ReadData()

m = X.shape[1]  # count of examples
n = X.shape[0]  # feature count
eta = 0.01   # learning rate
loss, diff_loss, prev_loss = 10, 10, 5
eps = 1e-10
max_iteration = 100 # 最多100次循环
# 初始化w,b
B = np.zeros((1,1))
W = np.zeros((1,n))

for iteration in range(max_iteration):
    for i in range(m):
        Xm = X[0:n,i].reshape(n,1)
        Ym = Y[0,i].reshape(1,1)
        Z = ForwardCalculation(Xm, W, B)
        dw, db = BackPropagation(Xm, Ym, Z)
        W, B = UpdateWeights(W, B, dw, db, eta)
        
        loss, diff_loss = CheckLoss(W,B,X,Y,m,prev_loss)
        if i%10==0:
            print(iteration, i, diff_loss)
        if diff_loss < eps:
            print(i)
            break
        prev_loss = loss
    print(iteration, W, B, diff_loss)
    if diff_loss < eps:
        break

print("W=", W)
print("B=", B)

PredicateTest(W, B)
```
怀着期待的心情用颤抖的右手按下了运行键......but......what happened?

```
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:61: RuntimeWarning: overflow encountered in square
  LOSS = (Z - Y)**2
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:63: RuntimeWarning: invalid value encountered in double_scalars
  diff_loss = abs(loss - prev_loss)
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:55: RuntimeWarning: invalid value encountered in subtract
  w = w - eta*dw
0 [[nan nan nan]] [[nan]] nan
1 [[nan nan nan]] [[nan]] nan
2 [[nan nan nan]] [[nan]] nan
3 [[nan nan nan]] [[nan]] nan
```
怎么会overflow呢？于是右手的颤抖没有停止，左手也开始颤抖了。

难度我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。别慌，擦干净头上的冷汗，让我们debug一下。

# 解决梯度爆炸

## 检查迭代中的数值变化情况

先把迭代中的关键值打印出来：

```
0 -----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[  93.8  187.6 3705.1]]
B: [[46.9]]
1 -----------
Z: [[289982.7]]
Y: [[464]]
dLoss/dZ: [[289518.7]]
dw: [[  579037.4         1158074.8        22582458.60000001]]
db: [[289518.7]]
W: [[  -57809.94  -115619.88 -2254540.76]]
B: [[-28904.97]]
2 -----------
Z: [[-2.62364972e+08]]
Y: [[634]]
dLoss/dZ: [[-2.62365606e+08]]
dw: [[-5.24731213e+08 -1.57419364e+09 -3.04344103e+10]]
db: [[-2.62365606e+08]]
W: [[5.24153113e+07 1.57303744e+08 3.04118649e+09]]
B: [[26207655.65900001]]
......
```
最开始的W,B的值都是0，三次迭代后，W,B的值已经大的超乎想象了。可以停止运行程序了，想一想为什么。

难道是因为学习率太大吗？目前是0.1，设置成0.01试试看：
```
0 ----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[ 0.938  1.876 37.051]]
B: [[0.469]]
1 -----------
Z: [[2899.827]]
Y: [[464]]
dLoss/dZ: [[2435.827]]
dw: [[  4871.654   9743.308 189994.506]]
db: [[2435.827]]
W: [[  -3.933654   -7.867308 -152.943506]]
B: [[-1.966827]]
2 ----------
Z: [[-17798.484679]]
Y: [[634]]
dLoss/dZ: [[-18432.484679]]
dw: [[  -36864.969358  -110594.908074 -2138168.222764]]
db: [[-18432.484679]]
W: [[  32.93131536  102.72760007 1985.22471676]]
B: [[16.46565768]]
```
没啥改进。

回想一个问题：为什么在“单入单出的一层神经网络”一文的代码中，我们没有遇到这种情况？把样本拿来看一看：

|样本序号|1|2|3|...|200|
|---|---|---|---|---|---|
|服务器数量(千)|0.928|0.0469|0.855|...|0.373|
|空调功率(千瓦)|4.824|2.950|4.643|...|3.594|

因为所有的X值（服务器数量）都是在[0,1]之间的，而本次的数据有三个特征值，全都是不是在[0,1]之间的，并且取值范围还不相同。我们不妨把本次样本数据也做一下这样的处理，亦即“归一化”。

## 数据归一化
### min-max标准化
也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：

$$
x_{new} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

其中max为样本数据的最大值，min为样本数据的最小值。
如果想要将数据映射到[-1,1]，则将公式换成：
$$
x_{new} = \frac{x-x_{mean}}{x_{max}-x_{min}}
$$
mean表示数据的均值。

### 样本分析

再把这个表拿出来分析一下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|价格（万元）|434|500|321|482|...|410|

我们用min-max标准化来归一以上的样本特征数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|0|0.667|0.333|1|...|0.33|
|地理位置（几环）|0.25|0|1|0.25|...|0.75|
|居住面积（平米）|0.7|0.75|0.175|0.4|...|0.36|
|价格(万元)|434|500|321|482|...|410|

注意，我们并没有归一化样本的标签数据，所以最后一行的价格还是保持不变。

### 归一化的实现

我们把归一化的函数写好：
```Python
def NormalizeByData(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    x_range = np.zeros((1,n))
    x_min = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        max_value = np.max(x)
        min_value = np.min(x)
        x_min[0,i] = min_value
        x_range[0,i] = max_value - min_value
        x_new = (x - x_min[0,i])/(x_range[0,i])
        X_new[i,:] = x_new
    return X_new, x_range, x_min

def NormalizeXY(XData, YData, flag):
    if flag=='x_only':
        X, X_range, X_min = NormalizeByData(XData)
        return X, X_range, X_min, YData, -1, -1
    elif flag=='x_and_y':
        X, X_range, X_min = NormalizeByData(XData)
        Y, Y_range, Y_min = NormalizeByData(YData)
        return X, X_range, X_min, Y, Y_range, Y_min
```
- x_range：表示样本特征值的取值范围
- x_min：表示样本特征值的取值范围的下限
- y_range：表示样本标签值的取值范围
- y_min：表示样本标签值的取值范围的下限

然后再改一下主程序的头两行，加上归一化的调用，'x_only'表示只对特征值归一化，不改变标签值：
```
flag = 'x_only' # only normalize X value
XData, YData = ReadData()
X, X_range, X_min, Y, Y_range, Y_min = NormalizeXY(XData, YData, flag)
```
用颤抖的双手同时按下Ctrl+F5，运行开始，结束，一眨眼！仔细看打印结果：
```
......
35 600 [[  5.99999997 -40.00000004 394.9999999 ]] [[292.00000009]] 344.8930674898838
35 700 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 284.6144928234677
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
[[36838.99999298]]
```
# 房屋价格预测

W，B的值都算出来了，说明网络收敛了，这是成功的第一步。再看最后的预测值，93平米的房子需要36839万元！难道是传说中的黄金屋？这不科学！

## 还原真实的W值

这次错在哪里了呢？我们唯一修改的地方就是样本数据特征值的归一化。假设在归一化之前，真实的样本值是X，真实的权重值是W，在归一化之后，样本值变成了X'，训练出来的权重值是W'，不考虑B值的情况下：
$$
Y = W*X \tag{标签值}
$$
$$
Z = W'*X' \tag{预测值}
$$

由于训练时标签值（房价）并没有做归一化，意味着我们是用真实的房价做的训练，所以预测值和标签值应该相等：
$$
因为：Y == Z \\
所以：W*X = W'*X'
$$

假设X'是X的1/N大小：
$$
N*X' = X
$$

把X代入等式左侧:
$$
W*N*X'=W'*X' => W*N = W' => 即：W = W'/N
$$

也就是说我们应该把训练的结果W'除以N倍后，就可以得到真实的W值。N是什么呢？就是X_range。对于本例来说，X_range是一个数组，有三个元素，分别代表三个特征向量值归一化前的取值范围。

## 还原真实的B值

既然W的值不是真实值，需要还原，那么B值一定也是如此。如果我们得到了W的真实值，那么B的真实值也很容易得到，我们还是利用样本数据来做这件事。

因为样本数据标签值$Y = W*X+B$，我们已经有了还原后的W的值，归一化前的样本X值也存在，标签值Y也存在，则：
$$B=Y - W*X$$

## 变成代码
```Python
# get real weights
def DeNormalizeWeights(X_range, XData, n):
    W_real = np.zeros((1,n))
    for i in range(n):
        W_real[0,i] = W[0,i] / X_range[0,i]
    print("W_real=", W_real)

    B_real = 0
    num_sample = XData.shape[1]
    for i in range(num_sample):
        xm = XData[0:n,i].reshape(n,1)
        zm = ForwardCalculation(xm, W_real, 0)
        ym = Y[0,i].reshape(1,1)
        B_real = B_real + (ym - zm)
    B_real = B_real / num_sample
    print("B_real=", B_real)
    return W_real, B_real
```
X_range是我们在做归一化时保留下来的样本的三个特征向量的取值范围，存放在一个三个元素的数组中。XData是原始值，与W_real相乘以后，再用标签值Y去减，就会得到B值。为了避免样本噪音，用了所有的样本来计算差值的总和，最后除以样本总数，就是B_real。

在主程序最后两行加一下这个逻辑，再次运行：

```Python
W_real, B_real = DeNormalizeWeights(X_range, XData, n)
PredicateTest(W_real, B_real)
```
运行结果如下：
```
W= [[  5.9996169  -40.00073494 394.99808248]]
B= [[292.00163197]]
W_real= [[  1.9998723  -10.00018374   4.99997573]]
B_real= [[110.00302237]]
result=[[528.99999999]]
```
W的真实值W_real=[1.99, -10, 4.99]，B的真实值B_real=110，那间93平米的房子的预测值=529万元。相当make sense！

至此，我们完美地解决了北京地区的房价预测问题！但是还没有解决自己可以有能力买一套北京的房子的问题......
