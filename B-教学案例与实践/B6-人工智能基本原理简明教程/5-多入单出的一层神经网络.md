Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 知识点

- 多元线性回归
- 样本数据归一化

# 提出问题

大哥，北京的房价了解一下！

房价预测问题，成为了机器学习的一个入门话题。我们也不能免俗，但是，不要用美国的什么多少平方英尺，多少个房间的例子来说事儿了，中国人不能理解。我们来个北京的例子！

影响北京房价的因素有很多，几个最重要的因子和它们的取值范围是：
- 朝向（在北方地区，窗户面向阳光的房子要抢手一些）：北=1，西=2，东=3，南=4
- 地理位置：二环，三环，四环，五环，六环，分别取值为2，3，4，5，6，二环的房子单价最贵
- 面积：40~120平米，连续值


影响房价的因素有很多，居住面积，地理位置，朝向，是其中三个比较重要的因素。当然还有学区房什么比较特殊的情况。关于房价，我们有1000个样本，每个样本有三个特征值，一个标签值，示例如下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积|96|100|54|72|...|69|
|整套价格|434|500|321|482|...|410|

- 特征值1 - 窗户朝向
一共有”东”“南”“西”“北“四个值，用数字化表示是：
东：3
南：4
西：2
北：1
因为朝南的房子比较贵，其次为东，西，北。为啥东比西贵？难度是因为我们常说“东西”，东在前，西在后？瞎扯！是因为夏天时朝西的窗户西晒时间长，比较热。

- 特征值2 - 地理位置
二环：2 - 单价最贵
三环：3
四环：4
五环：5
六环：6 - 单价最便宜

- 特征值3 - 房屋面积
统计所有样本数据得到房屋的面积范围是[40,120]


### 问题：在北京东五环的一套窗户朝西的93平米的房子，大概是多少钱？

由于表中可能没有恰好符合上述条件的数据，因此我们必须根据1000个样本值来建立一个模型，来解决我们的问题。

先假设这个问题是个线性回归问题，而且是典型的多元线性回归。函数模型如下：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

为了方便大家理解，咱们具体化一下上面的公式，按照本系列文章的符号约定就是：

$$ 
Z = w_1x_1+w_2x_2+w_3x_3+b = W*X + B
$$

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。这个一层的神经网络没有中间层，只有输入项和输出层（输入项不算做一层），而且只有一个神经元，并且神经元有一个线性输出，不经过激活函数处理。亦即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

<img src=".\Images\5\Setup.jpg" width="600">

矩阵运算过程：$W(1,3) * X(3,1) + B(1,1) => Z(1,1)$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(1,3)表示W是一个1行3列的矩阵。

## 输入层

假设一共有m个样本，每个样本n个特征值，X就是一个$n \times m$的矩阵，模样是这样紫的（n=3，m=1000，亦即3行1000列）：

$$
X = \\
\begin{pmatrix} 
X_1 & X_2 \dots X_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{2,1} & \dots & x_{1000,1} \\
\\
x_{1,2} & x_{2,2} & \dots & x_{1000,2} \\
\\
x_{1,3} & x_{2,3} & \dots & x_{1000,3}
\end{pmatrix} = 
\begin{pmatrix}
3 & 2 & \dots & 3 \\
\\
1 & 4 & \dots & 2 \\
\\
96 & 100 & \dots & 54
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y_1 & y_2 & \dots & y_m \\
\end{pmatrix}=
\begin{pmatrix}
434 & 500 & \dots & 410 \\
\end{pmatrix}
$$

单独看一个样本是这样的：

$$
x_1 =
\begin{pmatrix}
x_{1,1} \\
\\
x_{1,2} \\
\\
x_{1,3}
\end{pmatrix} = 
\begin{pmatrix}
3 \\
\\
1 \\
\\
96
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 434 \end{pmatrix}
$$

$X_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

## 权重W和B

有人问了，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？那样好像更符合思维习惯？

确实是！但是在实际的矩阵运算时，由于是$Z=W*X+B$，W在前面，X在后面，所以必须是这个样子的：

$$
\begin{pmatrix}
w_1 & w_2 & w_3
\end{pmatrix}
\times
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix}=
w_1*x_1+w_2*x_2+w_3*x_3
$$

假设每个样本x有n个特征向量，上式中的W就是一个$1 \times n$的向量，让每个w都对应一个x：
$$
\begin{pmatrix}w_1 & w_2 \dots w_n\end{pmatrix}
$$

B是个单值，因为只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

## 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。由于是线性的，所以没有用激活函数。

# 下载训练数据

[点击下载训练数据](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HouseXData.dat)

[点击下载标签数据](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HouseYData.dat)

# 读取文件数据
```Python
def LoadData():
    Xfile = Path("HouseXData.dat")
    Yfile = Path("HouseYData.dat")
    if Xfile.exists() & Yfile.exists():
        XData = np.load(Xfile)
        YData = np.load(Yfile)
        return XData,YData
    
    return None,None
```

# 前向计算

```Python
def ForwardCalculation(Xm,W,b):
    z = np.dot(W, Xm) + b
    return z
```
# 定义代价函数

我们依然用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。

```Python
def CheckLoss(w, b, X, Y, count, prev_loss):
    Z = w * X + b
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/count/2
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

# 反向传播

```Python
def BackPropagation(Xm,Y,Z):
    dloss_z = Z - Y
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db

def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```
上面第一个求梯度函数，第二个函数用于每次迭代时更新w，b的值。

# 主程序
```Python
XData, Y = ReadData()
X = XData

m = X.shape[1]  # count of examples
n = X.shape[0]  # feature count
eta = 0.01   # learning rate
loss, diff_loss, prev_loss = 10, 10, 5
eps = 1e-10
max_iteration = 100 # 最多100次循环
# 初始化w,b
B = np.zeros((1,1))
W = np.zeros((1,n))

for iteration in range(max_iteration):
    for i in range(m):
        Xm = X[0:n,i].reshape(n,1)
        Ym = Y[0,i].reshape(1,1)
        Z = ForwardCalculation(Xm, W, B)
        dw, db = BackPropagation(Xm, Ym, Z)
        W, B = UpdateWeights(W, B, dw, db, eta)
        
        loss, diff_loss = CheckLoss(W,B,Xm,Ym,m,prev_loss)
        if i%100==0:
            print(iteration, i, W, B, diff_loss)
        if diff_loss < eps:
            print(i)
            break
        prev_loss = loss
    print(iteration, W, B, diff_loss)
    if diff_loss < eps:
        break
```
怀着期待的心情用颤抖的右手按下了运行键......but......what happened?

```
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:61: RuntimeWarning: overflow encountered in square
  LOSS = (Z - Y)**2
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:63: RuntimeWarning: invalid value encountered in double_scalars
  diff_loss = abs(loss - prev_loss)
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:55: RuntimeWarning: invalid value encountered in subtract
  w = w - eta*dw
0 [[nan nan nan]] [[nan]] nan
1 [[nan nan nan]] [[nan]] nan
2 [[nan nan nan]] [[nan]] nan
3 [[nan nan nan]] [[nan]] nan
```
怎么会overflow呢？于是右手的颤抖没有停止，左手也开始颤抖了。

难度我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。别慌，擦干净头上的冷汗，让我们debug一下。

# 解决梯度爆炸

## 检查迭代中的数值变化情况

先把迭代中的关键值打印出来：

```
0 -----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[  93.8  187.6 3705.1]]
B: [[46.9]]
1 -----------
Z: [[289982.7]]
Y: [[464]]
dLoss/dZ: [[289518.7]]
dw: [[  579037.4         1158074.8        22582458.60000001]]
db: [[289518.7]]
W: [[  -57809.94  -115619.88 -2254540.76]]
B: [[-28904.97]]
2 -----------
Z: [[-2.62364972e+08]]
Y: [[634]]
dLoss/dZ: [[-2.62365606e+08]]
dw: [[-5.24731213e+08 -1.57419364e+09 -3.04344103e+10]]
db: [[-2.62365606e+08]]
W: [[5.24153113e+07 1.57303744e+08 3.04118649e+09]]
B: [[26207655.65900001]]
......
```
最开始的W,B的值都是0，三次迭代后，W,B的值已经大的超乎想象了。可以停止运行程序了，想一想为什么。

难道是因为学习率太大吗？目前是0.1，设置成0.01试试看：
```
0 ----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[ 0.938  1.876 37.051]]
B: [[0.469]]
1 -----------
Z: [[2899.827]]
Y: [[464]]
dLoss/dZ: [[2435.827]]
dw: [[  4871.654   9743.308 189994.506]]
db: [[2435.827]]
W: [[  -3.933654   -7.867308 -152.943506]]
B: [[-1.966827]]
2 ----------
Z: [[-17798.484679]]
Y: [[634]]
dLoss/dZ: [[-18432.484679]]
dw: [[  -36864.969358  -110594.908074 -2138168.222764]]
db: [[-18432.484679]]
W: [[  32.93131536  102.72760007 1985.22471676]]
B: [[16.46565768]]
```
没啥改进。

回想一个问题：为什么在“单入单出的一层神经网络”一文的代码中，我们没有遇到这种情况？因为所有的X值都是在[0,1]之间的，而神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说，样本的各个特征的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！这就提出了样本的归一化或者正则化的理论。

## 数据归一化

更多的数据归一化的问题，我们会另文给出，下面只提对我们解决当前问题有用的方法。

### min-max标准化
也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：

$$
x_{new} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

其中max为样本数据的最大值，min为样本数据的最小值。
如果想要将数据映射到[-1,1]，则将公式换成：
$$
x_{new} = \frac{x-x_{mean}}{x_{max}-x_{min}}
$$
mean表示数据的均值。

### 样本分析

再把这个表拿出来分析一下：
|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|样本特征值1：窗户朝向|1|3|2|4|...|2|
|样本特征值2：地理位置|3|2|6|3|...|4|
|样本特征值3：居住面积|96|100|54|72|...|69|
|样本标签值y：房价(万元)|434|500|321|482|...|410|


我们用min-max标准化来归一以上数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|样本特征值1：窗户朝向|0|0.667|0.333|1|...|0.33|
|样本特征值2：地理位置|0.25|0|1|0.25|...|0.75|
|样本特征值3：居住面积|0.7|0.75|0.175|0.4|...|0.36|
|样本标签值y：房价(万元)|434|500|321|482|...|410|



### 归一化的实现

我们把归一化的函数写好：
```Python
def Normalize(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    w_num = np.zeros((1,n))
    for i in range(n):
        v = X[i,:]
        max = np.max(v)
        min = np.min(v)
        w_num[0,i] = max - min
        v = (v - min)/(max-min)
        X_new[i,:] = v
    return X_new, w_num
```

然后再改一下主程序，加上归一化的调用：
```
m = 1000
XData, Y = create_sample_data(m)
X, W_num = Normalize(XData)
n = X.shape[0]
eta = 0.1
loss, diff_loss, prev_loss = 10, 10, 5
eps = 1e-10
max_iteration = 1

B = np.zeros((1,1))
W = np.zeros((1,n))

for iteration in range(max_iteration):
    for i in range(m):
        Xm = X[0:n,i].reshape(n,1)
        Ym = Y[0,i].reshape(1,1)
        Z = forward_calculation(Xm, W, B)
        dw, db = dJwb_single(Xm, Ym, Z)
        W, B = update_weights(W, B, dw, db, eta)
        loss, diff_loss = check_diff(W,B,Xm,Ym,1,prev_loss)
        if diff_loss < eps:
            print(i)
            break
        prev_loss = loss
    print(iteration, W, B, diff_loss)
    if diff_loss < eps:
        break
```
用颤抖的双手同时按下Ctrl+F5，运行开始，结束，一眨眼！仔细看打印结果：
```
[[ 5.94417016 -40.05847929 394.83396979]] [[292.15951172]]
```
$$
w1=5.94417016  \\
w2=-40.05847929  \\
w3=394.83396979 \\
b=292.15951172
$$

比较一下原始公式：
$$z = w1·x_1 + w2·(10-x_2)+w3·x_3+b \\
= 2x_1 - 10(10-x_2)+5x_3+10 \\
= 2x_1 - 10x_2+5x_3+110 \\
w1=2 \\
w2=-10 \\
w3=5 \\
b=110
$$

什么鬼！怎么相差这么多？！
