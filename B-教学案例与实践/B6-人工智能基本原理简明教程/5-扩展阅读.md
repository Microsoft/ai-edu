Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

“铁柱”是一名老师，在神经网络中穿梭多年，挂了满身的蜘蛛网。“木头”是一名刚入门者，木头木脑的，有问题经常向铁柱请教。

# 数据预处理

木头：老师，归一化原来这么重要，在理论上是怎么解释呢？

铁柱：理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说：
1）样本的各个特征的取值要符合概率分布，即[0,1]
2）样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！

木头：是不是说，数据中地理位置的取值范围是[2,6]，而房屋面积的取值范围为[40,120]，二者相差太远，根本不可以放在一起计算了？

铁柱：是的，你看$W1*X1+W2*X2$这个式子中，如果X1的取值是[2,6]，X2的取值是[40,120]，一是相差太远，而是都不在[0,1]之间，所以对于神经网络来说很难理解。下图展示了归一化前后的情况Loss值的等高图，意思是地理位置和房屋面积取不同的值时，作为组合来计算损失函数值时，形成的类似地图的等高图。左侧为归一化前，右侧为归一化后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

木头：我还听说有一些类似的词汇，什么标准化，这个和归一化有什么区别呢？

铁柱：有三个类似的概念，归一化，标准化，中心化。

## 归一化

把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。


归一化有三种方法：
- Min-Max归一化：
$$x_{new}=(x-MinValue)/(MaxValue - MinValue)$$
- 平均值归一化： 
$$x_{new} = (x - μ) / (MaxValue - MinValue)$$
- 非线性归一化：
$$对数转换y=log(x)，反余切转换y=atan(x)*2/π $$

## 标准化
把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。

Z-score规范化（标准差标准化 / 零均值标准化）：
$$x_{new} = (x - μ)／σ$$

## 中心化

平均值为0，无标准差要求：
$$x_{new} = x - μ$$


## 神经网络中的数据归一化的意义

木头：老师，那我们在数据归一化之前，根本无法进行训练，是怎么回事儿呢？

铁柱：神经网络中，数据归一化在理论上有重要意义。

1. 数值问题
归一化/标准化可以避免一些不必要的数值问题。输入变量的数量级未致于会引起数值问题吧，但其实要引起也并不是那么困难。因为tansig（tanh）的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tansig( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。
假如你的输入是421，你也许认为，这并不是一个太大的数，但因为有效权值大概会在1/421左右，例如0.00243，那么，在matlab里输入421·0.00243 == 0.421·2.43，会发现不相等，这就是一个数值问题。

2. 求解需要
a. 初始化：
    在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
b. 梯度：
    以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是誤差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
c. 学习率：
    由（2）中，知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 隐层到输出层的权值梯度可以写成 2ea，而输入层到隐层的权值梯度为 2ew(1-a^2)x ，受 x 和 w 的影响，各个梯度的数量级不相同，因此，它们需要的学习率数量级也就不相同。对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
d.搜索轨迹：前面的图已解释

你可以回去看一下这个链接：https://www.jianshu.com/p/95a8f035c86c

# 正确的预测房屋价格的方法

木头：在正文中，咱们用训练出来的模型预测房屋价格，还需要先还原W和B的值，我觉得有些有点儿麻烦，有没有别的更好的办法？

铁柱：在线性问题中，我们还能够还原W和B的值，在非线性问题中，我们可能根本做不到这一点。所以，咱们可以把需要预测的数据，也先做归一化，送进模型中做推理计算。

木头：哦！对啊！因为我们训练时并没有把样本标签值做改变，所以预测的值应该还是有效的，不会脱离样本标签值的范围。

铁柱：那你来看看应该怎么做？

木头：（劈里啪啦敲键盘）...（铁柱听着机械键盘的声音皱了皱眉）...（20分钟后）......我写了段代码，您给看看：
```Python
# normalize data by specified range and min_value
def NormalizeByRange(X, x_range, x_min):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    for i in range(n):
        x = X[i,:]
        x_new = (x-x_min[0,i])/x_range[0,i]
        X_new[i,:] = x_new
    return X_new

# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)
    print("z1=", z1)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)
    print("z2=", z2)
```
我先增加了一个函数，NormalizeByRange，根据样本训练时的样本归一化方法，同样地处理一下预测样本xt，得到归一化后的xt_new，然后用训练出来的W/B的值来计算，得到z2，结果是这样的：
```
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
W_real= [[  1.99999999 -10.00000001   5.        ]]
B_real= [[110.00000012]]
z1= [[528.99999999]]
z2= [[528.99999999]]
```
可以看到z1和z2的预测值是相同的，两种方法都有效，但第二种方法看起来比较容易理解。

# 对标签值归一化

铁柱：木头不错哦！

木头：一般一般，年级第三！

铁柱：吼吼，还挺押韵。再给你提个问题，你有没有注意，在计算Loss值时，会达到172.287，337.246这样大的数值，是什么原因呢？我们第四章的Loss值可都是小于1的数。

木头：哦！啊！这个......（喝了一口82年的雪碧压压惊）......我想想啊......

铁柱：心动不如行动，想什么想，动手！

木头：吼的！......（劈里啪啦敲键盘）.......（铁柱听着那机械键盘的声音觉得闹得慌）......（20分钟后）......老师，我把PredicateTest函数改了一下：

```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B, flag):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)

    if flag == 'x_only':
        print("xt,W_real,B_real:",z1)
        print("xt_new,W,B:",z2)
    elif flag == 'x_and_y':
        print("xt,W_real,B_real:", z1*Y_range+Y_min)
        print("xt_new,W,B:", z2*Y_range+Y_min)
```
铁柱：哦？当flag=='x_and_y'时，为什么z1,z2要做这样的转换呢？

木头：flag='x_and_y'，意思是在训练时我们对Y值也做了归一化，所以，我想试一下反归一化，因为：

$$y_{new} = \frac{y-y_{min}}{y_{max}-y_{min}} = \frac{y-y_{min}}{y_{range}}$$

所以：

$$y = y_{new}*y_{range}+y_{min}$$

然后，把主程序第一行的flag值改成：
```Python
flag = 'x_and_y'
```
运行......结果如下：
```
15 900 [[ 0.0137157  -0.09154976  0.90384324]] [[0.08242129]] 0.00011344288067411854
16 0 [[ 0.01371643 -0.09154922  0.90384595]] [[0.08241911]] 0.0009021195541084537
16 100 [[ 0.01371701 -0.0915483   0.90384837]] [[0.08241596]] 0.0017657981568301397
W= [[ 0.01371698 -0.09154826  0.90384839]]
B= [[0.08241599]]
W_real= [[ 0.00457233 -0.02288706  0.01144112]]
B_real= [[-0.33402734]]
xt,W_real,B_real: [[528.99654603]]
xt_new,W,B: [[528.99670957]]
```
哈哈！82年的雪碧果然厉害！我们只迭代了16轮就结束了，而且得到了正确的预测值！

铁柱：不错不错！我们来总结一下正确的归一化和反归一化的关系：

|归一化|Weight|Bias|预测值Xt|预测方法|
|---|---|---|---|---|
|只归一化X|反归一化为$W_{real}$|反归一化为$B_{real}$|不归一化$Xt$|$Z=W_{real}*Xt+B_{real}$|
|只归一化X|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W*Xt_{new}+B$|
|同时归一化X和Y|反归一化为$W_{real}$|反归一化为$B_{real}$|不归一化$Xt$|$Z=W_{real}*Xt+B_{real}$<br>$Z'=Z*Y_{range}+Y_{min}$|
|同时归一化X和Y|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W*Xt_{new}+B$ <br> $Z'=Z*Y_{range}+Y_{min}$|

木头：哦！这样一总结，就可以一目了然啦！我也简单总结一下：

1. X必须归一化，否则无法训练
2. 训练出的结果W和B，在推理时有两种使用方式：
a. 直接使用，此时必须把推理时输入的X也做相同规则的归一化
b. 反归一化为W,B的本来值$W_{real},B_{real}$，推理时输入的X不需要改动
3. Y可以归一化，好处是迭代次数少。如果结果收敛，也可以不归一化，如果不收敛（数值过大），就必须归一化
4. 如果Y归一化，先沿袭第2步的做法，对得出来的结果做关于Y的反归一化

铁柱：可以看到对标签值的归一化确实给我们带来了好处，迭代次数减少了一倍多。其实，咱们这个网络还很简单，这样折腾一下是为了让大家有深刻的理解，真正复杂的神经网络有个理论叫做"Batch Normalization"——批标准化，以后咱们遇到时再讲。

木头：好啊好啊！
