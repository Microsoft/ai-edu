Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 求针对W和B的梯度函数
求解W和B的梯度方法与我们前面的文章“单入单出的一层神经网络”完全一样，所以不再赘述，只说一下结论：
因为：

$$z = wx+b$$

$$loss = \frac{1}{2}(z-y)^2$$

所以我们用loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{w}} = (z-y)x
$$

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{b}} = z-y
$$

变成代码：
```Python
def dJwb_single(Xm,Y,Z):
    dloss_z = Z - Y
    db = dloss_z
    dw = np.dot(dloss_z, Xm.T)
    return dw, db
```



# 解决梯度爆炸

## 检查迭代中的数值变化情况

先把迭代中的关键值打印出来：

```
0 -----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[  93.8  187.6 3705.1]]
B: [[46.9]]
1 -----------
Z: [[289982.7]]
Y: [[464]]
dLoss/dZ: [[289518.7]]
dw: [[  579037.4         1158074.8        22582458.60000001]]
db: [[289518.7]]
W: [[  -57809.94  -115619.88 -2254540.76]]
B: [[-28904.97]]
2 -----------
Z: [[-2.62364972e+08]]
Y: [[634]]
dLoss/dZ: [[-2.62365606e+08]]
dw: [[-5.24731213e+08 -1.57419364e+09 -3.04344103e+10]]
db: [[-2.62365606e+08]]
W: [[5.24153113e+07 1.57303744e+08 3.04118649e+09]]
B: [[26207655.65900001]]
......
```
最开始的W,B的值都是0，三次迭代后，W,B的值已经大的超乎想象了。可以停止运行程序了，想一想为什么。

难道是因为学习率太大吗？目前是0.1，设置成0.01试试看：
```
0 ----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[ 0.938  1.876 37.051]]
B: [[0.469]]
1 -----------
Z: [[2899.827]]
Y: [[464]]
dLoss/dZ: [[2435.827]]
dw: [[  4871.654   9743.308 189994.506]]
db: [[2435.827]]
W: [[  -3.933654   -7.867308 -152.943506]]
B: [[-1.966827]]
2 ----------
Z: [[-17798.484679]]
Y: [[634]]
dLoss/dZ: [[-18432.484679]]
dw: [[  -36864.969358  -110594.908074 -2138168.222764]]
db: [[-18432.484679]]
W: [[  32.93131536  102.72760007 1985.22471676]]
B: [[16.46565768]]
```
没啥改进。

回想一个问题：为什么在“单入单出的一层神经网络”一文的代码中，我们没有遇到这种情况？因为所有的X值都是在[0,1]之间的，而神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说，样本的各个特征的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小。这就提出了样本的归一化或者正则化的理论。

## 数据归一化

更多的数据归一化的问题，我们会另文给出，下面只提对我们解决当前问题有用的方法。

### min-max标准化
也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：

$$
x_{new} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

其中max为样本数据的最大值，min为样本数据的最小值。
如果想要将数据映射到[-1,1]，则将公式换成：
$$
x_{new} = \frac{x-x_{mean}}{x_{max}-x_{min}}
$$
mean表示数据的均值。

### 样本分析

再把这个表拿出来分析一下：
|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|样本特征值1：窗户朝向|1|3|2|4|...|2|
|样本特征值2：地理位置|3|2|6|3|...|4|
|样本特征值3：居住面积|96|100|54|72|...|69|
|样本标签值y：房价(万元)|434|500|321|482|...|410|

- 特征值1 - 窗户朝向
一共有”东”“南”“西”“北“四个值，用数字化表示是：
东：3
南：4
西：2
北：1
因为超南的房子比较贵，其次为东，西，北。为啥东比西贵？因为夏天时朝西的窗户西晒时间长，比较热。

- 特征值2 - 地理位置
二环：2 - 单价最贵
三环：3
四环：4
五环：5
六环：6 - 单价最便宜

- 特征值3 - 房屋面积
统计所有样本数据得到房屋的面积范围是[40,120]

我们用min-max标准化来归一以上数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|样本特征值1：窗户朝向|0|0.667|0.333|1|...|0.33|
|样本特征值2：地理位置|0.25|0|1|0.25|...|0.75|
|样本特征值3：居住面积|0.7|0.75|0.175|0.4|...|0.36|
|样本标签值y：房价(万元)|434|500|321|482|...|410|

还有一个问题：标签值y是否需要归一化？没有任何理论说需要归一化标签值，所以我们先把这个问题放一放。

### 数据归一化的好处

下图展示了归一化前后的情况，左侧为前，右侧为后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

归一化后，地理位置和房屋面积二者都归一到[0,1]之间，变成了可比的了，而且寻找最优解的路径也很直接，节省了时间。
### 归一化的实现

我们把归一化的函数写好：
```Python
def Normalize(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    w_num = np.zeros((1,n))
    for i in range(n):
        v = X[i,:]
        max = np.max(v)
        min = np.min(v)
        w_num[0,i] = max - min
        v = (v - min)/(max-min)
        X_new[i,:] = v
    return X_new, w_num
```

然后再改一下主程序，加上归一化的调用：
```
XData, Y = ReadData()
X, W_num = Normalize(XData)

m = X.shape[1]  # count of examples
n = X.shape[0]  # feature count
eta = 0.01   # learning rate
loss, diff_loss, prev_loss = 10, 10, 5
eps = 1e-10
max_iteration = 100 # 最多100次循环
# 初始化w,b
B = np.zeros((1,1))
W = np.zeros((1,n))


for iteration in range(max_iteration):
    for i in range(m):
        Xm = X[0:n,i].reshape(n,1)
        Ym = Y[0,i].reshape(1,1)
        Z = ForwardCalculation(Xm, W, B)
        dw, db = BackPropagation(Xm, Ym, Z)
        W, B = UpdateWeights(W, B, dw, db, eta)
        
        loss, diff_loss = CheckLoss(W,B,Xm,Ym,m,prev_loss)
        if i%100==0:
            print(iteration, i, W, B, diff_loss)
        if diff_loss < eps:
            print(i)
            break
        prev_loss = loss
    print(iteration, W, B, diff_loss)
    if diff_loss < eps:
        break
```
用颤抖的双手同时按下Ctrl+F5，运行开始，结束，一眨眼！仔细看打印结果：
```
[[ 5.94417016 -40.05847929 394.83396979]] [[292.15951172]]
```
$$
w1=5.94417016  \\
w2=-40.05847929  \\
w3=394.83396979 \\
b=292.15951172
$$

比较一下原始公式：
$$z = w1·x_1 + w2·(10-x_2)+w3·x_3+b \\
= 2x_1 - 10(10-x_2)+5x_3+10 \\
= 2x_1 - 10x_2+5x_3+110 \\
w1=2 \\
w2=-10 \\
w3=5 \\
b=110
$$

什么鬼！怎么相差这么多？！

# 归一化的后遗症

仔细想想，训练居然收敛了，出结果了，但是和我们预期的不太一致。我们没有做归一化前，根本没结果。归一化后有结果，那么就是归一化起作用了，但它有什么副作用呢？一起逐个分析一下4个值。

w1是窗户朝向，取值范围[1,4]，即：$x_{min}=1, x_{max}=4$，目前$w1=5.94417$，试着变换一下：

$$
\frac{w1}{x_{max}-x_{min}} = \frac{5.94417}{4-1}=1.98139 \simeq 2
$$
是不是和w1=2这个值非常的接近呢？！WoW！我从澡盆里跳了出来，给阿基米德打了一个电话，说：“我找到啦！”

再看W2是地理位置：
$$
\frac{w2}{x_{max}-x_{min}} = \frac{-40.058}{6-2}=-10.0145 \simeq -10
$$

再看W3是房屋面积：
$$
\frac{w3}{x_{max}-x_{min}} = \frac{394.8339}{119-40}=4.997 \simeq 5
$$

代码如下：
```Python
W_rel = np.zeros((1,n))
for i in range(n):
    W_rel[0,i] = W[0,i] / W_num[0,i]
print(W_rel)
```
其中W_num是在调用Normalize函数时返回的三个特征值的范围，亦即(3,4,79)。

再看b值......"元芳，你怎么看？"

“卑职以为，b值不是样本值，没有经过归一化。”

“你说得很对！但我们怎么解出b呢？”

“大人，这个好办，我们如此这样这样......把一堆样本值代入w1,w2,w3的公式，令b=0，看看计算出来的结果是什么，再和样本标签值去比较就可以了！”
$$
z = w1*x_1+w2*x_2+w3*x_3 +0\\
b = y - z
$$

为了避免单个的样本误差，用100个样本做比较，得到差值之和，再平均，代码如下：

```Python
B = 0
for i in range(10):
    xm = XData[0:n,i].reshape(n,1)
    zm = forward_calculation(xm, W_rel, 0)
    ym = Y[0,i].reshape(1,1)
    B = B + (ym - zm)
b = B / 10
print(b)
```
我们在计算z的时候，故意把第三个参数设置为0，即令b=0。ym是每个样本的标签值，它与z的差值自然就是真正的b值。

上述两段代码得到的输出如下：
```
[[  1.98139005 -10.01461982   4.99789835]]
[[110.29389945]]
```

至此，我们完美地解决了北京地区的房价预测问题！但是还没有解决自己可以有能力买一套北京的房子的问题......

最后遗留一个问题，如果标签值y也归一化的话，也可以得到训练结果，但是怎么解释呢？大家有兴趣的话可以研究一下，分享出来。反正笔者是没研究出来。

