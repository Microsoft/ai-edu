Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 学习要点

 - 一元线性回归
  - 反向传播的实现方式

# 提出问题

在互联网建设初期，各大运营商需要解决的问题就是保证服务器所在的机房的温度，保持在23摄氏度左右。那么一个机房安装多少台空调或者提供多大功率的空调合适呢？这个东西虽然能通过热力学计算得到公式，但是总会有误差。因此人们往往会在机房里装一个温控器，来控制空调的开关或者风扇的转速或者制冷能力。通过一些统计数据，我们得到了这张表格：

|样本序号|1|2|3|...|200|
|---|---|---|---|---|---|
|服务器数量(千)|0.928|0.0469|0.855|...|0.373|
|空调功率(千瓦)|4.824|2.950|4.643|...|3.594|

### 问题：在一个新建的机房里，如果有346台服务器，我们如何配置空调的功率？

这个数据是二维的，所以我们可以用可视化的方式来展示：
<img src="./Images/4/Data.png"/>

于是，我们把热力学计算的问题转换成为了一个统计问题，因为实在是不能确定每块板块到底能产生多少热量。通过对上图的观察，我们可以判断它属于一个线性回归问题，而且是最简单的一元线性回归。


# 定义神经网络结构

我们是首次尝试建立神经网络，先搞一个最简单的单层单点神经元：

<img src=".\Images\4\Setup.jpg"> 

## 输入层

它在输入层只接受一个输入，经过参数w,b的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。下面，我们在这个最简的线性回归的例子中，来说明神经网络中最重要的反向传播和梯度下降的概念和过程以及编码实现。


最终，样本数据的样子是：

$$
X=\begin{pmatrix}
x_1 & x_2 & \dots & x_{200}
\end{pmatrix}
\\
Y=\begin{pmatrix}
y_1 & y_2 & \dots & y_{200}
\end{pmatrix}
$$

其中，x就是上图中红色点的横坐标值（服务器数），y是纵坐标值（空调功率）。

## 权重W/B
因为是一元线性问题，所以W/B都是一个标量，记为w和b，我们认为这组数据有这个关系：$y = w*x+b$

## 输出层

输出层1个神经元，是上述预测公式的直接输出，但定义上有所变化，应该是$z = w*x+b$，z是我们的预测输出，y是实际的样本标签值。

# 下载训练数据

[点击下载训练数据X](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/TemperatureControlXData.npy)

[点击下载标签数据Y](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/TemperatureControlYData.npy)

请把数据拷贝到你的python文件运行的当前目录。

# 读取文件数据
```Python
def ReadData():
    Xfile = Path(x_data_name)
    Yfile = Path(y_data_name)
    if Xfile.exists() & Yfile.exists():
        X = np.load(Xfile)
        Y = np.load(Yfile)
        return X,Y
    else:
        return None,None
```

# 前向计算
```Python
def ForwardCalculation(w,b,x):
    z = w * x + b
    return z
```
# 损失函数

一般我们不区分损失函数和代价函数两种说法，但实际上代价函数的意思是单个样本的误差，损失函数是所有样本的误差综合。

我们用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。我们使用所有样本参与训练，因此损失函数实际为：

$$loss = \frac{1}{2m}\sum_{i=1}^{m}(Z_i - Y_i) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

由于loss是所有样本的集合，我们先对其中的所有值求总和，样本数量是m，然后除以m来求一个平均值。

下面是Python的code，用于计算损失：

```Python
# w:weight, b:bias, X,Y:sample data, count: count of sample, prev_loss:last time's loss
def CheckLoss(w, b, X, Y, count, prev_loss):
    Z = w * X + b
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/count/2
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

我们计算这个loss值的目的是计算前后两次迭代的loss值差异，当足够小时，就结束训练。

# 反向传播

下面的代码是通过4-扩展阅读.md中的公式推导而得的。

```Python
def BackPropagation(x,y,z):
    dZ = z - y
    dB = dZ
    dW = dZ * x
    return dW, dB

def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```
上面第一个求梯度函数，第二个函数用于每次迭代时更新w，b的值。

# 帮助函数
第一个show_result函数用于最后输出结果。第二个print_progress函数用于训练过程中的输出。

```Python
def ShowResult(X, Y, w, b, iteration):
    # draw sample data
    plt.plot(X, Y, "b.")
    # draw predication data
    Z = w*X +b
    plt.plot(X, Z, "r")
    plt.title("Air Conditioner Power")
    plt.xlabel("Number of Servers(K)")
    plt.ylabel("Power of Air Conditioner(KW)")
    plt.show()
    print(iteration)
    print(w,b)
```

# 主程序
```Python
# 设置学习率（步长）=0.01
eta = 0.01
# set w,b=0, you can set to others values to have a try
w, b = 0, 0
# 终止条件，当前后两个loss值的差的绝对值小于1e-10时，结束迭代
eps = 1e-10
# 最大迭代次数为100轮，每轮的样本200个
iteration, max_iteration = 0, 100
# calculate loss to decide the stop condition
prev_loss, loss, diff_loss = 0,0,0
# get training data
X, Y = ReadData()
# count of example
num_example = X.shape[0]

# 外循环（迭代次数）
for iteration in range(max_iteration):
    # 内循环（遍历每个样本）
    for i in range(num_example):
        # 每次只用一个样本进行训练
        x = X[i]    # 样本特征
        y = Y[i]    # 标签
        # 前向计算得到预测值Z
        z = ForwardCalculation(w, b, x)
        # 通过比较预测值z与标签值y的差异来反向传播误差，得到梯度值
        dW, dB = BackPropagationSingle(x, y, z)
        # 用梯度值更新权重值，梯度值需要乘以学习率，避免更新步长太大
        w, b = UpdateWeights(w, b, dW, dB, eta)
        # 比较预测值与标签值，计算损失
        loss, diff_loss = CheckLoss(w,b,X,Y,num_example,prev_loss)
        if i%10==0:
            print(iteration,i,loss,diff_loss,w,b)
        # condition 1 to stop
        if diff_loss < eps:
            print(loss, diff_loss)
            break
        prev_loss = loss

    if diff_loss < eps:
        break

# predicate
x = 346/1000
result = Predicate(x, w, b)
print(result)
# show chart
ShowResult(X, Y, w, b, iteration)
```

# 运行结果

<img src="./Images/4/SGD.png"/>

```
Loss=0.005871560167933436 
Diff_loss=1.1068005886794019e-11
Iteration=5
w=1.8459035859541528 
b=3.0847579785840957
result=3.7234406193242324
```
最终我们得到了W=1.8459，B=3.0847，然后根据这两个值画出了上图中的红线$y=1.8459x+3.0847$。

预测时，已知有346台服务器，先要除以1000，因为横坐标是以K(千台)服务器为单位的，代入前向计算函数，得到的结果是3.723千瓦。

至此，我们初步完成了任务，但是，红线似乎和蓝点的吻合程度还可以改进。如何改进呢？请看本章的《扩展阅读》部分吧！
