Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 梯度下降的三种形式

木头：老师，我还有一个疑问，列在下表中：

|方法|w|b|
|----|----|----|
|最小二乘法|1.9962|3.0054|
|梯度下降法|1.9182|3.0778|
|神经网络法|1.9182|3.0778|

因为我猜这个问题的原始值是可能是$w=2，b=3$。貌似神经网络的准确率不够啊？从下图来的神经网络的训练结果来看，红色线是斜着穿过蓝色点区域的，并没有在正中央。

<img src="./Images/4/result.png"/>

铁柱：咱们初次使用神经网络，一定有水土不服的地方。最小二乘法是数学方法，所以它的结果是可信的。梯度下降法和神经网络法，实际是一回事儿，只是梯度下降没有使用神经元模型而已。所以，咱们一起看看如何调整这个神经网络的训练过程，先从最简单的梯度下降的三种形式说起。

# 单变量随机梯度下降

SDG(Stochastic Grident Descent)

正向计算过程：

$$Z^{n \times 1}=W^{n \times f} \cdot X^{f \times 1} + B^{n \times 1}$$
$$A^{n \times 1}=a(Z)$$

反向计算过程：

$$ \Delta Z^{n \times 1} = J'(W,B) = A^{n \times 1} - Y^{1 \times 1}$$
$$
W^{n \times f} = W^{n \times f} - \eta \cdot (\Delta Z^{n \times 1} \cdot X_T^{1 \times f})$$
$$
B^{n \times 1} = B^{n \times 1} - \eta \cdot \Delta Z^{n \times 1}$$

其中：

$$f=特征值数，m=样本数，n=神经元数，\eta=步长 \\
A=预测值，Y=标签值，X=输入值，X_T=X的转置$$

<img src="./Images/4/SGD-example.png"/>

- 训练样本：每次使用一个样本数据进行一次训练，更新一次梯度，重复以上过程。
- 优点：训练开始时损失值下降很快，随机性大，找到最优解的可能性大。
- 缺点：受单个样本的影响最大，损失函数值波动大，到后期徘徊不前，在最优解附近震荡。不能并行计算。

<img src="./Images/4/SGD-Loss.png"/>

上图，迭代时，Loss值毛刺波动非常大，受样本差异影响。

<img src="./Images/4/SGD-Trace.png"/>

上图，梯度下降时，开始收敛较快，到后期波动较大，找不到准确的方向。

# 全批量梯度下降 

Batch Gradient Descent

正向计算过程：

$$Z^{n \times m}=W^{n \times f} \cdot X^{f \times m} + B^{n \times 1}$$
$$A^{n \times m}=a(Z)$$

反向计算过程：

$$ \Delta Z^{n \times m} = J'(W,B) = A^{n \times m} - Y^{1 \times m}$$
$$
W^{n \times f} = W^{n \times f} - \eta \cdot \frac{1}{m} (\Delta Z^{n \times m} \cdot X_T^{m \times f})$$
$$
B^{n \times 1} = B^{n \times 1} - \eta \cdot \frac{1}{m} \sum (\Delta Z^{n \times m}, axis=1) \tag{按列相加变成nx1}$$

其中：

$$f=特征值数，m=样本数，n=神经元数，\eta=步长 \\
A=预测值，Y=标签值，X=输入值，X_T=X的转置$$

<img src="./Images/4/BGD-example.png"/>

- 训练样本：每次使用全部数据集进行一次训练，更新一次梯度，重复以上过程。
- 优点：受单个样本的影响最小，一次计算全体样本速度快，损失函数值没有波动，到达最优点平稳。方便并行计算。
- 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。

<img src="./Images/4/FBGD-Loss.png"/>

上图，迭代时，Loss值没有毛刺波动。

<img src="./Images/4/FBGD-Trace.png"/>

上图，梯度下降时，直接了当达到最佳点。

# 小批量梯度下降

Mini-Batch Gradient Descent

正向计算过程：

$$Z^{n \times k}=W^{n \times f} \cdot X^{f \times k} + B^{n \times 1}$$

$$A^{n \times k}=a(Z)$$

反向计算过程：

$$ \Delta Z^{n \times k} = J'(W,B) = A^{n \times k} - Y^{1 \times k}$$
$$
W^{n \times f} = W^{n \times f} - \eta \cdot \frac{1}{k} (\Delta Z^{n \times k} \cdot X_T^{k \times f})$$
$$
B^{n \times 1} = B^{n \times 1} - \eta \cdot \frac{1}{k} \sum (\Delta Z^{n \times k}, axis=1) \tag{按列相加变成N x 1}$$

其中：

$$f=特征值数，k=批样本数，n=神经元数，\eta=步长 \\
A=预测值，Y=标签值，X=输入值，X_T=X的转置$$

结合了前两种的优点，又避免了前两者的缺点。

<img src="./Images/4/MBGD-example.png"/>

- 训练样本：选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度/
- 优点：不受单样本噪声影响，训练速度较快。
- 缺点：batch size的数值选择很关键，会影响训练结果。

一些概念：

- Batch Size：批大小，一次训练的样本数量。
- Iteration：迭代，一次正向+一次反向。
- Epoch：所有样本被使用了一次，叫做一个Epoch。

假设一共有样本1000个，batch size=20，则一个Epoch中，需要1000/20=50次Iteration才能训练完所有样本。

<img src="./Images/4/MBGD-Loss.png"/>

上图，迭代时，Loss值毛刺波动较小。

<img src="./Images/4/MBGD-Trace.png"/>

上图，梯度下降时，在接近中心时有小波动。图太小看不清楚，可以用matplot工具放大局部来观察。和全批量的图比较，靠近中心的线比较粗，说明有微小波动，而全批量是一根单直线。

小批量的大小通常由以下几个因素决定：

- 更大的批量会计算更精确的梯度，但是回报却是小于线性的
- 极小批量通常难以充分利用多核架构。这决定了最小批量的数值，低于这个值的小批量处理不会减少计算时间
- 如果批量处理中的所有样本可以并行地处理，那么内存消耗和批量大小成正比。对于多硬件设施，这是批量大小的限制因素。
- 某些硬件上使用特点大小的数组时，运行时间会更少，尤其是GPU，通常使用2的幂数作为批量大小可以更快，如32 ~ 256，大模型时尝试用16
- 可能是由于小批量在学习过程中加入了噪声，会带来一些正则化的效果。泛化误差通常在批量大小为1时最好。因为梯度估计的高方差，小批量使用较小的学习率，以保持稳定性，但是降低学习率会使迭代次数增加

在实际工程中，我们通常使用小批量梯度下降形式。

【课堂练习：用纸笔推算一下矩阵运算的维度】
假设：
- X (2x4)
- W (3x2)
- B (3x1)

【课后作业：调整以下参数，通过观察结果来理解神经网络的超参】
- 学习率 η
- 批大小 batch size
- w和b的初始值
- 最大循环次数 max epoch


代码位置：ch04, Level4