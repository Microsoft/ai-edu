Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 6.5 用双曲正切函数做二分类函数

此节为扩展内容，通过对源代码的一些列修改，最终可以实现用双曲正切函数做分类函数的目的。虽然这个“需求”是我们虚构出来的，但是通过动手实践这一过程，可以加深对分类函数、损失函数、反向传播等基本工作原理的理解，由此可以达到举一反三的效果，在今后的学习中游刃有余。

### 6.5.1 提出问题

在二分类问题中，使用Sigmoid函数作为分类函数，配合二分类交叉熵损失函数：

$$a(z) = \frac{1}{1 + e^{-z}} \tag{1}$$

$$loss(w,b)=-[y \ln a + (1-y) \ln (1-a)] \tag{2}$$

还有一个长得非常像的函数，双曲正切函数，公式如下：

$$a(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1 \tag{3}$$

**提出问题：能不能用双曲正切函数作为分类函数呢？**

看二者的函数图像：

|Sigmoid的函数图像|Tanh的函数图像|
|---|---|
|<img src="../Images/8/sigmoid.png">|<img src="../Images/8/tanh.png">|
|分界线output=0.5|分界线output=0|

当输出为大于0的数值时，可以认为是正类，小于0时认为是负类。

### 6.5.2 修改前向计算和反向传播函数

#### 增加双曲正切分类函数

```Python
def Tanh(z):
    a = 2.0 / (1.0 + np.exp(-2*z)) - 1.0
    return a
```


#### 修改前向计算方法

软件开发的一个原则是：对“增加”开放，对“修改”封闭。所谓的开放封闭原则，是防止改出bug来。为了不修改已有的NeuralNet类的代码，我们派生出一个子类来，在子类中增加新的方法。

```Python
class TanhNeuralNet(NeuralNet):
    def forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Sigmoid().forward(Z)
            return A
        elif self.params.net_type == NetType.BinaryTanh:
            A = Tanh().forward(Z)
            return A
        else:
            return Z

```
子类的新方法Overwrite掉以前的前向计算方法，通过判断网络类型调用Tanh函数。相应地，要增加一个网络类别：BinaryTanh，意为用Tanh做分类。

```Python
class NetType(Enum):
    Fitting = 1,
    BinaryClassifier = 2,
    MultipleClassifier = 3,
    BinaryTanh = 4,
```

#### 修改反向传播方法

正向计算容易改，反向传播肿么办？需要自己推公式！

对公式2的交叉熵函数求导：

$$
{\partial{loss} \over \partial{a_i}}= {y_i-a_i \over a_i(1-a_i)} \tag{4}
$$

以前是用损失函数对Sigmoid函数求导，现在改成对公式3的Tanh函数求导：

$$
{\partial{a_i} \over \partial{z_i}}=(1-a_i)(1+a_i) \tag{5}
$$

用链式法则结合公式4，5：

$$
{\partial{loss} \over \partial{z_i}}={\partial{loss} \over \partial{a_i}}{\partial{a_i} \over \partial{z_i}}
$$
$$
= {y_i-a_i \over a_i(1-a_i)} \cdot (1+a_i)(1-a_i)
$$
$$
{(a_i-y_i)(1+a_i) \over a_i}\tag{6}
$$

代码实现，同样在TanhNeuralNet类中，覆盖backwardBatch方法：

```Python
class TanhNeuralNet(NeuralNet):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        m = batch_x.shape[0]
        dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
        dB = dZ.sum(axis=0, keepdims=True)/m
        dW = np.dot(batch_x.T, dZ)/m
        return dW, dB
```

仔细地再推导一遍公式，确认无误后，我们可以试着运行：

```
epoch=0
Level4_TanhAsBinaryClassifier.py:29: RuntimeWarning: divide by zero encountered in true_divide
  dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
Level4_TanhAsBinaryClassifier.py:29: RuntimeWarning: invalid value encountered in true_divide
  dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
0 1 nan
0 3 nan
0 5 nan
......
```

不出意外，出错了！看第一个错误应该是除数为0，即batch_a值为0。为什么以前没有出过这样的异常呢？原因有二：

1. 以前用Sigmoid函数，输出值域为(0,1)，所以a值永远会大于0。而Tanh函数的输出值域是(-1,1)，有可能是0；
2. 以前的dZ=batch_a-batch_y，并没有除法项。

第一个原因我们无法解决，因为那是函数本身的特性，那我们考虑第二个原因吧，能不能把A从dZ中去掉呢？Tanh函数的导数是固定形式的(1+A)(1-A)，不能修改，如果修改了就不是Tanh函数了。所以需要修改交叉熵函数，使得损失函数对A的导数中含有(1+A)(1-A)，这样就可以消掉了。根据这样的思路把交叉熵函数修改一下，我们用简写方式，方便推导。

### 6.5.3 修改损失函数

交叉熵函数原公式为：

$$loss=-[y \ln a + (1-y) \ln (1-a)] \tag{2}$$

改成：

$$loss=-[(1+y) \ln (1+a) + (1-y) \ln (1-a)] \tag{7}$$

对公式7求导：

$$
{\partial loss \over \partial a} = {2(a-y) \over (1+a)(1-a)} \tag{8}
$$


结合公式5的tanh的导数：

$${\partial loss \over \partial z}={\partial loss \over \partial a}{\partial a \over \partial z}$$
$$ ={2(a-y) \over (1+a)(1-a)} (1+a)(1-a)$$
$$=2(a-y) \tag{9}$$

好，我们成功地把分母消除了！现在我们需要同时修改损失函数和反向传播函数。

#### 增加新的损失函数

```Python
class LossFunction(object):
    def CE2_tanh(self, A, Y, count):
        p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
        LOSS = np.sum(-p)
        loss = LOSS / count
        return loss
```
在原LossFunction类中，新增加了一个叫做CE2_tanh的损失函数，完全按照公式7实现。

#### 修改反向传播方法

```Python
class NeuralNet(object):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        m = batch_x.shape[0]
        # setp 1 - use original cross-entropy function
#        dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
        # step 2 - modify cross-entropy function
        dZ = 2 * (batch_a - batch_y)
        dB = dZ.sum(axis=0, keepdims=True)/m
        dW = np.dot(batch_x.T, dZ)/m
        return dW, dB
```
注意我们注释掉了step1的代码，代替为step2的代码。

第二次运行，结果只运行了一轮就停止了。看打印信息和损失函数值，损失函数居然是个负数！

```
epoch=0
0 1 -0.1882585728753378
W= [[0.04680528]
 [0.10793676]]
B= [[0.16576018]]
A= [[0.28416676]
 [0.24881074]
 [0.21204905]]
w12= -0.4336361115243373
b12= -1.5357156668786782
```
如果disable停止条件的话，可以看到如下的损失函数历史图：

<img src="../Images/6/tanh_loss_2.png">

从表面上看损失值不断下降，好像是收敛了，但是从交叉熵损失函数的定义来看，其值本身应该是永远大于0的。难道是因为改了损失函数形式从而得到了负值吗？答案是肯定的。

损失函数以前的形式是：

$$loss=-[y \ln(a)+(1-y) \ln (1-a)] \tag{2}$$

由于使用Sigmoid函数，所以a的值永远在(0,1)之间，那么1-a也在(0,1)之间，所以$ln(a)$和$ln(1-a)$都是负数。而y的值是0或1，两项相加也是负数。最前面有个负号，所以最终loss的结果是个正数。

改成1+a后：

$$loss=-[(1+y) \ln (1+a) + (1-y) \ln (1-a)] \tag{7}$$

Tanh函数输出值a为(-1,1)，这样$1+a \in (0,2)$，$1-a \in (0,2)$，$ln(1+a)$和$ln(1-a)$的值很有可能大于0，导致loss为负数。如果仍然想用交叉熵函数，必须满足其原始设计，让1+a和1-a都在(0,1)值域内！

### 6.5.4 再次修改损失函数代码

所以，让我们把对数内的表达式除以2，使其仍处于(0,1)之间：

$$loss=-[(1+y) \ln ({1+a \over 2})+(1-y) \ln ({1-a \over 2})] \tag{9}$$

虽然分母有个2，但是对导数公式没有影响，最后的结果仍然是公式8的形式：

$${\partial loss \over \partial z} =2(a-y) \tag{8}$$

```Python
class LossFunction(object):
    def CE2_tanh(self, A, Y, count):
        #p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
        p = (1-Y) * np.log((1-A)/2) + (1+Y) * np.log((1+A)/2)
        LOSS = np.sum(-p)
        loss = LOSS / count
        return loss
```

注意我们注释掉了第一次修改的代码，增加了分母为2的代码。

第三次运行：

|损失函数值|分类结果|
|-----|-----|
|<img src="../Images/6/tanh_loss_3.png">|<img src="../Images/6/tanh_result_3.png">|

这次的loss值曲线非常好，值域正确并且收敛了。可是看分类结果，为什么分界线整体向右偏移了呢？

这个偏移让我们想起了最开始的那张图，比较Sigmoid和Tanh的区别，Tanh的值域在(-1,1)之间，而Sigmoid的值域在(0,1)之间，是有偏移和拉伸的。这与上图的偏移有没有关系呢？

### 6.5.5 修改样本数据标签值

Sigmoid是假设所有正例的标签值都是1，负例的标签值都是0。而Tanh要求的是-1和1，所以我们要把标签值改一下。

在SimpleDataReader类上派生出子类SimpleDataReader_tanh，增加一个ToZeroOne()方法，目的是把原来的[0/1]标签变成[-1/1]标签。

```Python
class SimpleDataReader_tanh(SimpleDataReader):
    def ToZeroOne(self):
        Y = np.zeros((self.num_train, 1))
        for i in range(self.num_train):
            if self.YTrain[i,0] == 0:     # 第一类的标签设为0
                Y[i,0] = -1
            elif self.YTrain[i,0] == 1:   # 第二类的标签设为1
                Y[i,0] = 1
            # end if
        # end for
        self.YTrain = Y
        self.YRaw = Y
```
同时不要忘记把预测函数里的0.5变成0：

```Python
def draw_predicate_data(net):
    x = np.array([0.58,0.92,0.62,0.55,0.39,0.29]).reshape(3,2)
    a = net.inference(x)
    print("A=", a)
    for i in range(3):
        # if a[i,0] > 0.5:
        if a[i,0] > 0:
            plt.scatter(x[i,0], x[i,1], marker='^', c='g', s=100)
        else:
            plt.scatter(x[i,0], x[i,1], marker='^', c='r', s=100)
        #end if
    #end for
```

别忘记在主程序里调用这个方法：

```Python
if __name__ == '__main__':
    # data
    reader = SimpleDataReader_tanh()
    reader.ReadData()
    reader.ToZeroOne()
    # net
    params = HyperParameters(eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.BinaryTanh)
    num_input = 2
    num_output = 1
    net = TanhNeuralNet(params, num_input, num_output)
    net.train(reader, checkpoint=0.1)

    # show result
    draw_source_data(net, reader)
    draw_predicate_data(net)
    draw_split_line(net)
    plt.show()
```

第四次运行！......Perfect！无论是打印输出还是最终的可视化结果都很完美。

<img src="../Images/6/tanh_result_4.png">

最后我们对比一下Sigmoid和Tanh以及它们对应的交叉熵函数的图像：

|分类函数|交叉熵函数|
|---|---|
|<img src="../Images/6/sigmoid_seperator.png">|<img src="../Images/3/crossentropy2.png"/>|
|输出值域a在(0,1)之间，分界线为a=0.5，标签值为y=0/1|y=0为负例，y=1为正例，输入值域a在(0,1)之间|
|<img src="../Images/6/tanh_seperator.png">|<img src="../Images/6/modified_crossentropy.png"/>|
|输出值域a在(-1,1)之间，分界线为a=0，标签值为y=-1/1|y=-1为负例，y=1为正例，输入值域a在(-1,1)之间|

可以总结出，当使用Tanh函数后，相当于把Sigmoid的输出值域范围拉伸到2倍，下边界从0变成1；而对应的交叉熵函数，是把输入值域的范围拉伸到2倍，左边界从0变成-1，完全与分类函数匹配。

### 代码位置

ch06, Level5
