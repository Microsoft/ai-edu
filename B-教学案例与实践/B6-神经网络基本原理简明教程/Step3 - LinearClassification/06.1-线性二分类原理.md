Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 6.1 线性二分类原理

### 6.1.1 线性分类和线性回归的异同

此原理对线性和非线性二分类都适用。

回忆一下前面学习过的线性回归，通过用均方差函数的误差反向传播的方法，不断矫正拟合直线的角度（Weights）和偏移（Bias），因为均方差函数能够准确地反映出当前的拟合程度。那么在线性分类中，我们能不能采取类似的方法呢？

线性分类，试图在含有两种样本的空间中划出一条分界线，让双方截然分开，就好像是中国象棋的棋盘中的楚河汉界一样。与线性回归相似的地方是，两者都需要划出那条“直线”来，但是不同的地方也很明显：

||线性回归|线性分类|
|---|---|---|
|相同点|需要在样本群中找到一条直线|需要在样本群中找到一条直线|
|不同点|用直线来拟合所有样本，使得各个样本到这条直线的距离尽可能最短|用直线来分割所有样本，使得正例样本和负例样本尽可能分布在直线两侧|

可以看到线性回归中的目标--“距离最短”，还是很容易理解的，但是线性分类的目标--“分布在两侧”，用数学方式如何描述呢？我们可以有代数和几何两种方式来描述：

- 代数方式：通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5
- 几何方式：让所有正例样本处于直线的上方，所有负例样本处于直线的下方

<img src="..\Images\6\linear_binary_analysis.png">

### 6.1.2 二分类函数

对率函数Logistic Function，即是激活函数，又可以当作二分类的分类函数。而在很多不太正规的文字材料中，把这两个概念混用了，比如下面这个说法：“我们在最后使用Sigmoid激活函数来做二分类”。在本书中，我们会根据不同的任务一直区分激活函数和分类函数这两个概念，在二分类任务中，使用“对率函数”的叫法。

- 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

- 导数

$$a^{'}(z) = a(z)(1 - a(z))$$

- 输入值域

$$[-\infty, \infty]$$

- 输出值域

$$[0,1]$$

- 函数图像

<img src="../Images/8/sigmoid.png">

- 使用方式

此函数实际上是一个概率计算，它把$[-\infty, \infty]$之间的任何数字都压缩到[0,1]之间，返回一个概率值。这就是它的工作原理。

训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Sigmoid后，输出一个[0,1]之间的预测值。我们假设这个样本的标签值为0（属于负类，另外一类是第1类属于正类），如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。

推理时，我们预先设定一个阈值，比如上图中的红线，我们设置阈值=0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反。

### 6.1.3 二分类过程

下面我们以单样本双特征为例来说明神经网络的二分类过程。

1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$

用下图举例来说明计算过程：

<img src="../Images/6/sample.png">

假设：

1. 绿色点为正例，标签值y=1，坐标值为(1,3)
2. 红色点为负例，标签值y=0，坐标值分别为(1,5,3)和(3,1.5)
3. 初始值$w_1=-1，w_2=1，b=0$，b值=0，所以有$z=-1\cdot x_1+1 \cdot x_2 + 0$，即$z=-x_1+x_2$，如果令分割线Z=0，则有$x_2=x_1$

1. 把绿色点代入公式1，2，3表示的计算过程：

$$z=3-1=2$$
$$a=\frac{1}{1+e^{-2}}=0.88$$
$$loss=-[1 \cdot \ln 0.88 + (1-1) \ln (1-0.731)]=0.127$$

2. 把左上方的红色点代入公式：

$$z=3-1.5=1.5$$
$$a=\frac{1}{1+e^{-1.5}}=0.817$$
$$loss=-[0 \cdot \ln 0.817 + (1-0) \ln (1-0.817)]=1.7$$

3. 把右下方的红色点代入公式：

$$z=1.5-3=-1.5$$
$$a=\frac{1}{1+e^{1.5}}=0.182$$
$$loss=-[0 \cdot \ln 0.182 + (1-0) \ln (1-0.182)]=0.2$$



### 6.1.3 对数几率回归

木头：哦！明白了，经过数学推导后可以知道，神经网络实际也是在做这样一件事：经过调整w和b的值，把所有正例的样本都归纳到大于0.5的范围内，所有负例都小于0.5。

铁柱：对，只说大于或者小于，无法做准确的量化计算，所以用一个对率函数来模拟。

木头：啊，说到对率函数，还有一个问题，它为啥叫做“对数几率”函数呢？从哪里看出是“对数”了？“几率”是什么意思呢？

铁柱：我们来看一下神经网络的前向计算公式......

$$z = wx+b  \tag{1}$$
$$a={1 \over 1 + e^{-z}} \tag{2}$$
组合二者：
$$a={1 \over 1 + e^{-(wx+b)}}$$
做公式变形：
$$e^{-(wx+b)}={1-a \over a}$$
两侧取对数，再乘以-1：
$$\ln {a \over 1-a} = wx+b \tag{12}$$

上式中，如果a是把样本x的预测为正例的可能性，那么1-a就是其负例的可能性，a/(1-a)就是正负例的比值，称为几率(odds)，反映了x作为正例的相对可能性，而对几率取对数就叫做对数几率(log odds, logit)。

木头：欸！公式12不就是我们前面学过的线性回归模型吗？原来线性分类也是用了类似的原理。

铁柱：是的，公式12就是用线性回归模型的预测结果来逼近样本分类的对数几率。这就是为什么它叫做逻辑回归(logistic regression)，但其实是分类学习的方法。这种方法的优点如下：

- 直接对分类可能性建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题
- 不仅预测出类别，而是得到了近似的概率，这对许多需要利用概率辅助决策的任务很有用
- 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解



## 神经网络的线性二分类工作原理

木头：老师，我有个问题！如下图所示，我们假设绿色方块为正类：标签值$y=1$，红色三角形为负类：标签值$y=0$。

<img src="..\Images\6\linear_binary_analysis.png">

从直观上理解，如果我们有一条直线，其公式为：$z = w \cdot x1+b$，如图中的虚线所示，则所有正类的样本的x2都大于z，而所有的负类样本的x2都小于z，那么这条直线就是我们需要的解决方案。用正例的样本来表示：
$$
x2 > z，即：x2 > w \cdot x1 + b \tag{11}
$$

那么神经网络用矩阵运算+分类函数+损失函数这么复杂的流程，其工作原理是什么呢？

铁柱：好问题！经典机器学习中的SVM确实就是用这种思路来解决这个问题的，即一个类别的所有样本在分割线的一侧，而负类样本都在线的另一侧。神经网络的正向公式是这样：

$$Z = w1 \cdot x1 + w2 \cdot x2 + b$$

$$A = sigmoid(Z) = {1 \over 1 + e^{-Z}}$$

<img src="..\Images\6\sigmoid_binary.png">

当A>0.5时，为正类。当A<0.5时，为负类。我们用正类来举例子：

$$A = sigmoid(Z) = {1 \over 1 + e^{-Z}} > 0.5$$

做公式变形，再两边取自然对数，可以得到：

$$Z > 0$$

即：
$$
Z = w1 \cdot x1 + w2 \cdot x2 + b > 0
$$
对上式做一下变形，把x2放在左侧，其他项放在右侧：
$$
x2 > - {w1 \over w2}x1 - {b \over w2} \tag{12}
$$
简化一下两个系数，令w'=-w1/w2，b'=-b/w2：
$$
x2 > w'x1 + b' \tag{13}
$$
比较一下公式11和13，
$$
x2 > w \cdot x1 + b \tag{11}
$$

一模一样！这就说明神经网络的数学原理和我们在二维平面上的直观感觉是相同的。由此，我们还得到了一个额外的收获，即：

$$w = w' = - w1 / w2 \tag{14}$$

$$b = b' = -b/w2 \tag{15}$$

我们可以使用神经网络计算出w1,w2,B以后，换算成w,b来在二维平面上画那根直线，来直观地判断神经网络的正确性。



参考：周志华老师的西瓜书


