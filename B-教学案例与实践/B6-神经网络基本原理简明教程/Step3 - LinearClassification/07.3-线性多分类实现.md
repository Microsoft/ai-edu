Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 7.3 线性多分类的神经网络实现  

### 7.3.1 定义神经网络结构

从图示来看，似乎在三个颜色区间之间有两个比较明显的分界线，而且是直线，即线性可分的。我们如何通过神经网络精确地找到这两条分界线呢？

- 从视觉上判断是线性可分的，所以我们使用单层神经网络即可
- 输入特征是两个，X1=经度，X2=纬度
- 最后输出的是三个分类，分别是魏蜀吴，所以输出层有三个神经元

如果有三个以上的分类同时存在，我们需要对每一类别分配一个神经元，这个神经元的作用是根据前端输入的各种数据，先做线性处理（Y=WX+B)，然后做一次非线性处理，计算每个样本在每个类别中的预测概率，再和标签中的类别比较，看看预测是否准确，如果准确，则奖励这个预测，给与正反馈；如果不准确，则惩罚这个预测，给与负反馈。两类反馈都反向传播到神经网络系统中去调整参数。

这个网络只有输入层和输出层，由于输入层不算在内，所以是一层网络。

<img src="..\Images\7\MultipleClassifierNN.png">

与前面的单层网络不同的是，本图最右侧的输出层还多出来一个Softmax分类函数，这是多分类任务中的标准配置，可以看作是输出层的激活函数，并不单独成为一层，与二分类中的Logistic函数一样。

#### 输入层

输入经度(x1)和纬度(x2)两个特征：

$$
X=\begin{pmatrix}
x_1 & x_2
\end{pmatrix}
$$

#### 权重矩阵W1/B1

W权重矩阵的尺寸，可以从后往前看，比如：输出层是3个神经元，输入层是2个特征，则W的尺寸就是3x2。

$$
W=\begin{pmatrix}
w_{11} & w_{12} & w_{13}\\
w_{21} & w_{22} & w_{23} 
\end{pmatrix}
$$

B的尺寸是1x3，列数永远和神经元的数量一样，行数永远是1。

$$
B=\begin{pmatrix}
b_1 & b_2 & b_3 
\end{pmatrix}
$$

#### 输出层

输出层三个神经元，再加上一个Softmax计算，最后有A1,A2,A3三个输出，写作：

$$
Z = \begin{pmatrix}Z1 & Z2 & Z3 \end{pmatrix}
$$
$$
A = \begin{pmatrix}A1 & A2 & A3 \end{pmatrix}
$$

其中，$Z=X \cdot W+B，A = Softmax(Z)$

### 7.3.2 样本数据

使用SimpleDataReader类读取数据后，观察一下数据的基本属性：

```Python
reader.XRaw.shape
(140, 2)
reader.XRaw.min()
0.058152279749505986
reader.XRaw.max()
9.925126526921046

reader.YRaw.shape
(140, 1)
reader.YRaw.min()
1.0
reader.YRaw.max()
3.0
```
- 训练数据X，140个记录，两个特征，最小值0.058，最大值9.925
- 标签数据Y，140个记录，一个分类值，取值范围是[1,2,3]

#### 样本标签数据

一般来说，在标记样本时，我们会用1，2，3这样的标记，来指明是哪一类。所以样本数据中是这个样子的：
$$
Y = 
\begin{pmatrix}
y_1 \\ y_2 \\ ... \\ y_{140}
\end{pmatrix}=
\begin{pmatrix}3 & 2 & \dots & 1\end{pmatrix}
$$

在有Softmax的多分类计算时，我们用下面这种等价的方式，俗称One-Hot，就是在一个向量中只有一个数据是1，其它都是0。
$$
Y = 
\begin{pmatrix}
y_1 \\ y_2 \\ \dots \\ y_{140}
\end{pmatrix}=
\begin{pmatrix}
0 & 0 & 1 \\
0 & 1 & 0 \\
... & ... & ... \\
1 & 0 & 0
\end{pmatrix}
$$

OneHot的意思，在这一列数据中，只有一个1，其它都是0。1所在的列数就是这个样本的分类类别。

标签数据对应到每个样本数据上，列对齐，只有(1,0,0)，(0,1,0)，(0,0,1)三种组合，分别表示第一类、第二类和第三类。

在SimpleDataReader中实现ToOneHot()方法，把原始标签转变成One-Hot编码：

```Python
class SimpleDataReader(object):
    def ToOneHot(self, num_category, base=0):
        count = self.YRaw.shape[0]
        self.num_category = num_category
        y_new = np.zeros((count, self.num_category))
        for i in range(count):
            n = (int)(self.YRaw[i,0])
            y_new[i,n-base] = 1
```

### 7.3.3 代码实现

绝大部分代码可以从上一章的HelperClass中拷贝出来，但是需要我们为了本章的特殊需要稍加修改。

```Python
class Softmax(object):
    def forward(self, z):
        shift_z = z - np.max(z, axis=1, keepdims=True)
        exp_z = np.exp(shift_z)
        a = exp_z / np.sum(exp_z, axis=1, keepdims=True)
        return a
```

### 前向计算

前向计算需要增加分类函数调用：

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z, axis=0)
    exp_z = np.exp(shift_z)
    A = exp_z / np.sum(exp_z, axis=0)
    return A

# 前向计算
def ForwardCalculationBatch(W, B, batch_X):
    Z = np.dot(W, batch_X) + B
    A = Softmax(Z)
    return A
```

### 计算损失函数值

损失函数不再是均方差了，而是交叉熵函数对于多分类的形式。

```Python
def CheckLoss(W, B, X, Y):
    m = X.shape[1]
    A = ForwardCalculationBatch(W,B,X)
    p1 = np.log(A)
    p2 =  np.multiply(Y, p1)
    LOSS = np.sum(-p2) 
    loss = LOSS / m
    return loss
```

### 推理函数

```Python
    xt_normalized = NormalizePredicateData(xt, X_norm)
    A = ForwardCalculationBatch(W,B,xt_normalized)
    r = np.argmax(A,axis=0)+1
    return A, xt_normalized, r
```

最后一个函数np.argmax的作用是比较A里面的几个数据的值，返回最大的那个数据的行数或者列数，0-based。比如A=(1.02,-3,2.2)时，会返回2，因为2.2最大，所以我们再加1，变成1，2，3类。

### 主程序

```Python
# 主程序
if __name__ == '__main__':
    # SGD, MiniBatch, FullBatch
    method = "SGD"
    # read data
    XData,YData = ReadData(x_data_name, y_data_name)
    X, X_norm = NormalizeData(XData)
    num_category = 3
    Y = ToOneHot(YData, num_category)
    W, B = train(method, X, Y, ForwardCalculationBatch, CheckLoss)

    print("W=",W)
    print("B=",B)
    xt = np.array([5,1,7,6,5,6,2,7]).reshape(2,4,order='F')
    a, xt_norm, r = Inference(W,B,X_norm,xt)
    print("Probility=", a)
    print("Result=",r)
```

### 运行结果

```
epoch=99, iteration=115, loss=0.065409
W= [[-3.53207999 11.31094203]
 [-7.2749168  -7.00354565]
 [10.80699679 -4.30739638]]
B= [[-4.30753322]
 [ 8.51783018]
 [-4.21029697]]
Probility= [[1.06821435e-04 2.42947816e-01 4.84891526e-01 6.73531781e-01]
 [9.64663416e-01 1.02809223e-01 4.51424722e-01 3.26269149e-01]
 [3.52297621e-02 6.54242961e-01 6.36837515e-02 1.99069931e-04]]
Result= [2 3 1 1]
```
注意，Probility的结果，对于每个测试样本的结果，是按列看的，即第一列是第一个测试样本的分类结果。

1. 经纬度相对值为(5,1)时，属于2，蜀国
2. 经纬度相对值为(7,6)时，属于3，吴国
3. 经纬度相对值为(5,6)时，属于1，魏国
4. 经纬度相对值为(2,7)时，属于1，魏国

### 损失函数历史记录

<img src="..\Images\7\multiple_loss.png">


### 代码位置

ch07, Level1
