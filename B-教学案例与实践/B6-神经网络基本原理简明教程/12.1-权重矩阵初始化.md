Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 权重矩阵初始化

我们用第8章的眼镜蛇拟合的例子来说明权重初始化的问题。

## 零初始化

即把所有层的W值的初始值都设置为0。

$$
W = 0
$$


## 随机初始化

把W初始化均值为0，方差为1的矩阵：

$$
W \sim G \begin{bmatrix} 0, 1 \end{bmatrix}
$$

## Xavier初始化方法

条件：正向传播时，激活值的方差保持不变；反向传播时，关于状态值的梯度的方差保持不变。

$$
W \sim U \begin{bmatrix} -\sqrt{{6 \over n_{input} + n_{output}}}, \sqrt{{6 \over n_{input} + n_{output}}} \end{bmatrix}
$$

假设激活函数关于0对称，且主要针对于全连接神经网络。适用于tanh和softsign。

即权重矩阵参数应该满足在该区间内的均匀分布。其中的W是权重矩阵，U是Uniform分布，即均匀分布。

[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

by Xavier Glorot, Yoshua Bengio in AISTATS 2010.

Abstract:

Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We find that the logistics Sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.

简译：神经网络在2006年之前不能很理想地工作，很大原因在于权重矩阵初始化方法上。Sigmoid函数不太适合于深度学习，因为会导致梯度饱和。基于以上原因，我们提出了一种可以快速收敛的参数初始化方法。

[这是中译版](https://blog.csdn.net/victoriaw/article/details/73000632)，感谢译者。

Xavier初始化方法比直接用高斯分布进行初始化W的优势所在： 

一般的神经网络在前向传播时神经元输出值的方差会不断增大，而使用Xavier等方法理论上可以保证每层神经元输入输出方差一致。 

在深度为5层的网络中的表现情况：

||各层的激活值|各层的反向传播梯度|
|---|---|---|
|Normal|<img src=".\Images\13\normal_activation.jpg">|<img src=".\Images\13\normal_bp.jpg">|
|Xavier|<img src=".\Images\13\xavier_activation.png">|<img src=".\Images\13\xavier_bp.png">|

## MSRA初始化方法

又叫做He方法，因为作者姓何。

条件：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变。


[原文](https://arxiv.org/pdf/1502.01852.pdf)，何凯明，Microsoft Research Asia，2015。

Abstract:

Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [29]). To our knowledge,our result is the ﬁrst to surpass human-level performance(5.1%, [22])on this visual recognition challenge.

网络初始化是一件很重要的事情。但是，传统的固定方差的高斯分布初始化，在网络变深的时候使得模型很难收敛。VGG团队是这样处理初始化的问题的：他们首先训练了一个8层的网络，然后用这个网络再去初始化更深的网络。

“Xavier”是一种相对不错的初始化方法，但是，Xavier推导的时候假设激活函数是线性的，显然我们目前常用的ReLU和PReLU并不满足这一条件。所以MSRA初始化主要是想解决使用Relu激活函数后，方差会发生变化，因此初始化权重的方法也应该变化。

只考虑输入个数时，MSRA初始化是一个均值为0，方差为2/n的高斯分布，适合于ReLU激活函数：

$$
W \sim N \begin{bmatrix} 0, \sqrt{2 \over n} \end{bmatrix}
$$

其中的W为权重矩阵，G表示高斯分布，Gaussian Distribution，也叫做正态分布，Normal Distribution。

对于Leaky ReLU：

$$
W \sim N \begin{bmatrix} 0, \sqrt{2 \over (1+\alpha^2) \hat n_i} \end{bmatrix}
\\ \hat n_i = h_i \cdot w_i \cdot d_i
\\ h_i: 卷积核高度，w_i: 卷积核宽度，d_i: 卷积核个数
$$


|20层深的网络|30层深的网络|
|---|---|
|<img src=".\Images\13\msra.jpg" width="500">|<img src=".\Images\13\msra2.jpg" width="520">|


##  几种初始化方法的比较

1. 零值初始化，只适用于单层线性问题
2. [0,1]正态分布随机值初始化
3. Xavier均匀分布初始化
4. MSRA初始化

|初始化方法|损失函数值|训练结果|
|---|---|---|
|零值初始化|<img src=".\Images\13\init_zero_loss.png">|<img src=".\Images\13\init_zero_result.png">|
|正态分布初始化|<img src=".\Images\13\init_normal_loss.png">|<img src=".\Images\13\init_normal_result.png">|
|Xavier初始化|<img src=".\Images\13\init_xavier_loss.png">|<img src=".\Images\13\init_xavier_result.png">|
|MSRA初始化|<img src=".\Images\13\init_msra_loss.png">|<img src=".\Images\13\init_msra_result.png">|

木头：为什么零值初始化不能得到正确的结果呢？

铁柱：我们只训练了50000个epoch，如果训练更多的轮数，也不会得到正确的结果。看下面的零值初始化的权重矩阵值打印输出：
```
[[-10.29778156]
 [-10.29778156]
 [-10.29778156]
 [-10.29778156]]
[[9.92847462]
 [9.92847462]
 [9.92847462]
 [9.92847462]]
[[-0.29563053 -0.29563053 -0.29563053 -0.29563053]]
[[0.93438753]]
```
可以看到W1和W2的值内部4个单元都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有w的值都同步更新，没有差别。这样的话，无论多少论，最终的结果也不会准确。

正态分布初始化的结果：

```
[[ -1.0711346 ]
 [-19.75850005]
 [ -8.84552296]
 [  5.08410339]]
[[ 1.81361968]
 [16.69671287]
 [ 4.25394174]
 [-3.49831165]]
[[ 2.47519669 -2.01738001 -2.73550422 -4.55522827]]
[[2.50646588]]
```

Xavier初始化的结果：

```
[[ -5.86643409]
 [-12.80232309]
 [ -0.94429957]
 [ 10.36952416]]
[[ 4.13811338]
 [10.71442528]
 [-0.27816434]
 [-4.92664038]]
[[ 5.65247181 -3.34531482  0.9183063   2.53985718]]
[[-2.84968849]]
```

后三者最后都得到了准确的结果，但是Xavier的效果最好。

代码位置：ch10, Level0, Level1