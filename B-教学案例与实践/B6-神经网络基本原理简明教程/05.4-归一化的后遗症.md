Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 5.4 归一化的后遗症

### 5.4.1 对比结果

在上一节中，我们使用了如下超参进行神经网络的训练：

```Python
params = HyperParameters(eta=0.1, max_epoch=10, batch_size=1, eps = 1e-5)
```

我们再把每次checkpoint的W和B的值打印出来：

```
9 0 437.5399553941636 [[-35.46926435] [399.01136072]] [[252.69305588]]
9 100 420.78580862641473 [[-36.93198181] [400.03047293]] [[251.26503706]]
9 200 398.58439997901917 [[-39.90602892] [390.9923031 ]] [[253.77229392]]
9 300 393.4058623386585 [[-31.26023019] [389.38500924]] [[247.81021777]]
9 400 380.95014666219294 [[-41.71204444] [400.49621558]] [[243.90381925]]
9 500 402.3345372333071 [[-50.16424871] [400.57038807]] [[242.88921572]]
9 600 419.2032196399209 [[-38.64935779] [397.40267036]] [[235.76347754]]
9 700 388.91219270279 [[-41.87540883] [406.51486971]] [[245.11439119]]
9 800 387.30767281965444 [[-40.57188118] [407.41384495]] [[237.77896547]]
9 900 413.7210407763991 [[-36.67601742] [406.55322285]] [[246.8067483]]
```
打印结果中每列的含义：

1. epoch
2. iteration
3. loss
4. w1
5. w2
6. b

可以看到loss值、w1、w2、b的值，每次跳跃都很大，怀疑是学习率过高导致的梯度下降在最优解附近徘徊。所以，我们先把超参修改一下：

```Python
params = HyperParameters(eta=0.01, max_epoch=500, batch_size=10, eps=1e-5)
```

做了三处修改：
- 学习率缩小10倍，变成0.01
- max_epoch扩大50倍，让网络得到充分训练
- batch_size=10，使用mini-batch批量样本训练，提高精度，减缓个别样本引起的跳跃程度

运行结果：

```
499 9 380.9733976063486 [[-40.0502582 ] [399.59874166]] [[245.01472597]]
499 19 380.91972396603296 [[-39.96834496] [399.55957677]] [[244.92705677]]
499 29 380.6255377532388 [[-40.31047769] [399.26167586]] [[244.19126217]]
499 39 380.6057213728372 [[-40.2563536 ] [399.35785505]] [[244.53062721]]
499 49 380.657163633654 [[-40.16087354] [399.36180641]] [[244.67728494]]
499 59 380.59442069555746 [[-40.32063337] [399.48881984]] [[244.37834746]]
499 69 380.92999531800933 [[-40.57175379] [399.16255261]] [[243.81211148]]
499 79 380.687742276159 [[-40.4266247 ] [399.30514719]] [[244.0496554]]
499 89 380.62299460835936 [[-40.2782923 ] [399.34224968]] [[244.14309928]]
499 99 380.5935045560184 [[-40.26440193] [399.39472352]] [[244.3928586]]
```

可以看到达到了我们的目的，loss、w1、w2、b的值都很问题。我们使用这批结果做为分析基础。首先列出W和B的训练结果：

|结果|W1|W2|B|
|---|---|---|---|
|正规方程|-2.018|5.055|46.235|
|神经网络|-40.2|399.3|244.5|
|比值|19.92|78.99|5.288|

再列出归一化后的数据：

|特征|地理位置|居住面积|
|----|----|---|
|最小值|2.02|40|
|最大值|21.96|119|
|差值|19.94|79|

通过对比我发现，关于W的结果，第一张表最后一行的数据，和第二张表最后一行的数据，有惊人的相似之处！这是为什么呢？

### 5.4.2 还原真实的W,B值

我们唯一修改的地方，就是样本数据特征值的归一化，我们并没有修改标签值。可以大概猜到W的值和样本特征值的缩放有关系，而且缩放倍数非常相似，甚至可以说一致。下面推导一下这种现象的数学基础。

假设在归一化之前，真实的样本值是$X$，真实的权重值是$W$；在归一化之后，样本值变成了$X'$，训练出来的权重值是$W'$：

$$
y = x_1 w_1 + x_2 w_2 + b \tag{y是标签值}
$$

$$
z = x_1' w_1' + x_2' w_2' + b' \tag{z是预测值}
$$

由于训练时标签值（房价）并没有做归一化，意味着我们是用真实的房价做的训练，所以预测值和标签值应该相等，所以：
$$
y == z $$
$$
x_1 w_1 + x_2 w_2 + b = x_1' w_1' + x_2' w_2' + b' \tag{1}
$$

归一化的公式是：
$$
x' = {x - x_{min} \over x_{max}-x_{min}} \tag{2}
$$

为了简化书写，我们令$xm=x_{max}-x_{min}$，把公式2代入公式1：

$$
x_1 w_1 + x_2 w_2 + b  = {x_1 - x_{1min} \over xm_1} w_1' + {x_2 - x_{2min} \over xm_2} w_2' + b' 
$$
$$
=x_1 \frac{w_1'}{xm_1} + x_2 \frac{w_2'}{xm_2}+b'-\frac{w_1'x_{1min}}{xm_1}-\frac{w_2'x_{2min}}{xm_2} \tag{3}
$$

公式3中，x1,x2是变量，其它都是常量，如果想让公式3等式成立，则变量项和常数项分别相等，即：

$$
w_1 = \frac{w_1'}{xm_1} \tag{4}
$$
$$
w_2 = \frac{w_2'}{xm_2} \tag{5}
$$
$$ 
b = b'-\frac{w_1'x_{1min}}{xm_1}-\frac{w_2'x_{2min}}{xm_2} \tag{6}
$$

下面我们用实际数值代入公式4,5,6：

$$
w_1 = {w_1' \over x_{1max}-x_{1min}} = {-40.2 \over (21.96-2.02)} = -2.016 \tag{7}
$$
$$
w_2 = {w_2' \over x_{2max}-x_{2min}} = {399.3 \over (119-40)} = 5.054 \tag{8}
$$
$$
b=244.5-(-2.016) \cdot 2.02 - 5.054 \cdot 40=46.412 \tag{9}
$$

可以看到公式7、8、9的计算结果（神经网络的训练结果的变换值）与正规方程的计算结果(-2.018, 5.055, 46.235)基本相同，基于神经网络是一种近似解的考虑，可以认为这种推导是合理的。


## 变成代码
```Python
# get real weights
def DeNormalizeWeights(X_range, x_min, w, b):
    n = w.shape[1]
    W_real = np.zeros((n,))
    for i in range(n):
        W_real[i] = w[0,i] / X_range[0,i]

    B_real = b
    for i in range(n):
        tmp = w[0,i] * x_min[0,i] / X_range[0,i]
        B_real = B_real - tmp
    return W_real, B_real
```

X_range是我们在做归一化时保留下来的样本的三个特征向量的取值范围，x_min是三个特征向量的最小值。

在主程序最后两行加一下这个逻辑，再次运行：

```Python
W_real, B_real = DeNormalizeWeights(X_range, X_min, w, b)
print("W_real=", W_real)
print("B_real=", B_real)

```
运行结果如下：
```
w= [[  5.9964012  -40.00429782 394.98808748]]
b= [[292.0105229]]
epoch=14, iteration=99, loss=0.000008
W_real= [  1.9988004  -10.00107445   4.99984921]
B_real= [[110.01990306]]
```

木头：老师，这次我们算是成功了吗？

铁柱：对于简单的线性问题来说，这么做可以。但是如果遇到非线性问题，或者深层网络，这么做就不行了。咱们下回分解！

代码位置：ch05, Level4
