Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 参数调整

我们使用如下参数做第一次的训练：

|参数|缺省值|是否可调|
|---|---|---|
|输入层神经元数|1|No|
|隐层神经元数|4|Yes|
|输出层神经元数|1|No|
|学习率|0.1|Yes|
|批样本量|10|Yes|
|最大epoch|50000|Yes|
|损失门限值|0.001|No|
|损失函数|MSE|No|
|参数初始化方法|xavier|Yes|

木头：买嘎哒！怎么这么多参数！

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

## 权重初始化的方法的调整

我们提供有三种方法初始化权重矩阵：

1. 零值初始化
2. [0,1]正态分布随机值初始化（均值为0，方差为1）
3. [0,1]Xavier均匀分布随机值初始化

|初始化方法|损失函数值|训练结果|
|---|---|---|
|零值初始化|<img src=".\Images\8\zero_loss.png">|<img src=".\Images\8\zero_result.png">|
|正态分布初始化|<img src=".\Images\8\norm_loss.png">|<img src=".\Images\8\norm_result.png">|
|Xavier初始化|<img src=".\Images\8\xavier_loss.png">|<img src=".\Images\8\xavier_result.png">|

木头：为什么零值初始化不能得到正确的结果呢？

铁柱：我们只训练了50000个epoch，如果训练更多的轮数，也不会得到正确的结果。看下面的零值初始化的权重矩阵值打印输出：
```
[[-10.29778156]
 [-10.29778156]
 [-10.29778156]
 [-10.29778156]]
[[9.92847462]
 [9.92847462]
 [9.92847462]
 [9.92847462]]
[[-0.29563053 -0.29563053 -0.29563053 -0.29563053]]
[[0.93438753]]
```
可以看到W1和W2的值内部4个单元都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有w的值都同步更新，没有差别。这样的话，无论多少论，最终的结果也不会准确。

正态分布初始化的结果：

```
[[ -1.0711346 ]
 [-19.75850005]
 [ -8.84552296]
 [  5.08410339]]
[[ 1.81361968]
 [16.69671287]
 [ 4.25394174]
 [-3.49831165]]
[[ 2.47519669 -2.01738001 -2.73550422 -4.55522827]]
[[2.50646588]]
```

Xavier初始化的结果：

```
[[ -5.86643409]
 [-12.80232309]
 [ -0.94429957]
 [ 10.36952416]]
[[ 4.13811338]
 [10.71442528]
 [-0.27816434]
 [-4.92664038]]
[[ 5.65247181 -3.34531482  0.9183063   2.53985718]]
[[-2.84968849]]
```

后两者最后都得到了准确的结果，但是Xavier的epoch次数是13166，比第二种方法的29096少了一倍多。所以我们推荐使用Xavier初始化方法。


## 学习率的调整

|学习率|0.1|0.5|0.9|
|------|---|---|---|
|Loss|<img src=".\Images\8\eta01_loss.png">|<img src=".\Images\8\eta05_loss.png">|<img src=".\Images\8\eta09_loss.png">|
|epoch|13166|5940|2599|

对于这个问题，较大的学习率可以带来很快速的收敛速度，但并不是对所有问题都这样。


## 批大小的调整

|批大小|5|10|20|
|------|---|---|---|
|Loss|<img src=".\Images\8\bz5_loss.png">|<img src=".\Images\8\bz10_loss.png">|<img src=".\Images\8\bz20_loss.png">|
|epoch|3278|5940|11894|

批样本量越小，epoch次数越少，收敛越快。

## 中间层神经元数量的影响

|||
|---|---|
|<img src=".\Images\8\ne4_loss.png">|<img src=".\Images\8\ne6_loss.png">|
|ne=4, epoch=5940|ne=6, epoch=2007|
|<img src=".\Images\8\ne8_loss.png">|<img src=".\Images\8\ne10_loss.png">|
|ne=8, epoch=3271|ne=10, epoch=3575|

隐层神经元个数为6时，收敛速度最快。
