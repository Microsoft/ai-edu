Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 10.4 逻辑异或门的工作原理

我们在上一节课的代码基础上再增加些东西，来理解神经网络针对这个异或问题的工作原理。

### 10.4.1 理解隐层神经元数量的影响

一般来说，隐层的神经元数量要大于等于输入特征的数量，在本例中是2。我们从下图可以看到，如果隐层只有一个神经元的话，是不能完成分类任务的。

|||
|---|---|
|<img src='../Images/10/xor_n1.png'/>|<img src='../Images/10/xor_n2.png'/>|
|1个神经元|2个神经元，迭代6040次到达精度要求|
|<img src='../Images/10/xor_n3.png'/>|<img src='../Images/10/xor_n4.png'/>|
|3个神经元，迭代5228次到达精度要求|4个神经元，迭代4331次到达精度要求|
|<img src='../Images/10/xor_n8.png'/>|<img src='../Images/10/xor_n16.png'/>|
|8个神经元，迭代4107次到达精度要求|16个神经元，迭代4402次到达精度要求|

以上各情况的迭代次数是在Xavier初始化的情况下测试一次得到的数值，并不意味着7个神经元一定比6个神经元差。但可以推理的是，并不是隐层的神经元越多越好，合适才好。2个神经元肯定是足够的，4个神经元肯定要轻松一些，更多的神经元也并不是更轻松。

# 隐层有两个神经元的工作原理

以下是隐层为两个神经元时的结果输出：

```
w=[[-7.00777143 -7.01121059]
 [ 5.51518649  5.51451102]]
b=[[ 2.86885647]
 [-8.53863829]]
```

我们使用上面的权重矩阵结果，把4个样本数据代入到前向计算公式中，依次求得Z1,A1,Z2,A2的值，并列表如下：

||1|2|3|4|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|0|1|1|0|
|Z1|2.86885647|-4.14235412|-4.13891495|-11.15012554|
||-8.53863829|-3.02412727|-3.0234518|2.49105922|
|A1|9.46285253e-01|1.56370110e-02|1.56900366e-02 |1.43732759e-05|
||1.95718330e-04|4.63477089e-02|4.63775738e-02|9.23512657e-01|
|Z2|-5.45851003|5.20347907|5.20247396|-5.3417112|
|A2|0.00424183|0.99453265|0.99452718|0.00476486|

## 完成了分割任务的二维图示分析

|||
|---|---|
|<img src='../Images/10/xor_source_data.png'/>|<img src='../Images/10/xor_z1.png'/>|
|<img src='../Images/10/xor_a1.png'/>|<img src='../Images/10/xor_a2z2.png'/>|

|1) 原始x1做横轴，x2做纵轴的4个点，处于4个角落|2) 线性变换结果，用Z1[0,0]做横轴，Z1[1,0]做纵轴的4个点，两个绿点接近重合|


|3) Z1的Sigmoid结果，用A1[0,0]做横轴，A1[1,0]做纵轴的4个点，两个绿点接近重合|4) A1点经过第二次线性变化，4个点归类到横轴上的0点两侧，位置大概是-5.7和+5.7处（Z2的位置）。A2可以匹配到Sigmoid曲线上|


## 完成学习的过程

### 损失函数值的变化与分类效果对比

|Z1的演变|A1的演变|
|---|---|
|<img src='../Images/10/xor_z1_500.png'/>|<img src='../Images/10/xor_a1_500.png'/>|
|<img src='../Images/10/xor_z1_1500.png'/>|<img src='../Images/10/xor_a1_1500.png'/>|
|<img src='../Images/10/xor_z1_2000.png'/>|<img src='../Images/10/xor_a1_2000.png'/>|
|<img src='../Images/10/xor_z1_2500.png'/>|<img src='../Images/10/xor_a1_2500.png'/>|
|<img src='../Images/10/xor_z1_6000.png'/>|<img src='../Images/10/xor_a1_6000.png'/>|

|分类函数值的演变|分类结果的演变|
|---|---|
|<img src='../Images/10/xor_logistic_500.png'/>|<img src='../Images/10/xor_result_500.png'/>|
|<img src='../Images/10/xor_logistic_1500.png'/>|<img src='../Images/10/xor_result_1500.png'/>|
|<img src='../Images/10/xor_logistic_2000.png'/>|<img src='../Images/10/xor_result_2000.png'/>|
|<img src='../Images/10/xor_logistic_2500.png'/>|<img src='../Images/10/xor_result_2500.png'/>|
|<img src='../Images/10/xor_logistic_6000.png'/>|<img src='../Images/10/xor_result_6000.png'/>|


# 深度挖掘工作原理

但是，神经网络真的是在平面上工作的吗？我们再深度挖掘一下！

平面上分割两类的直线，实际上是我们的想象：使用0.5为门限值像国界一样把两部分数据分开。但实际上，神经网络的输出是个概率，即，它可以告诉你某个点属于某个类别的概率是多少，我们人为地设定为当概率大于0.5时属于正类，小于0.5时属于负类。所以我们可以把过渡区也展示出来，让大家更好地理解。

基本思路是：

1. 在[0,1]区间定义50x50个坐标点，这样就有了X和Y
2. 加载用Level1训练好的Weights and Bias for W1, B1, W2, B2，但是需要的网络配置是隐层有两个神经元
3. 把X,Y,W,B代入前向计算公式（此时相当于做Inference），计算每个点的Z值（就是输出的A2值，经过Sigmoid的，所以是个[0,1]之间的数值）
4. 在3D图上画出X,Y,Z的曲面

如下图所示：

|上方视角|斜侧视角|
|---|---|
|<img src='../Images/10/binary_result_3D_1.png'/>|<img src='../Images/10/binary_result_3D_2.png'/>|

生成的3D图，从上方看，和二维平面上的效果一样。从侧面看，是一个立体的门。异或问题的四个点，其中两个对角顶点处于蓝色区域位置，另外两个对角点处于红色区域部分，这样神经网络就可以在Z=0.5出画一个平面，完美地分开对角顶点。

# 思考题

在隐层有两个神经元时，可以看到两条边界线的起点终点，都会接近0.5的位置，即：$[0,0.42]\sim[0.42,0]及[1,0.42]\sim[0.42,1]$，如下图：

<img src='../Images/10/binary_result_2.png' height="400" width="400"/>

请试验如果再增加迭代次数，会不会达到0.5中点位置？


代码位置：ch10, Level2(两个神经元的情况), Level3(三个神经元的情况)
