Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 正规方程 Normal Equations

对于线性回归问题，除了前面提到的最小二乘法可以解决一元线性回归的问题外，对于多元线性回归，可以用正规方程来解决，也就是得到一个数学上的解析解。它可以解决下面这个公式描述的问题：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

或者写成神经网络模型的方式：

$$ 
Z = w_1x_1+w_2x_2+w_3x_3+b = W*X + B
$$


## 简单的推导方法

在做函数拟合（回归）时，我们假设函数H为：

$$H(w,b) = b + w_1x_1+w_2x_2+...+w_nx_n$$

令$b=w_0$，则：

$$H(w) = w_0 \cdot 1 + w_1 \cdot x_1+w_2 \cdot x_2+...+w_n \cdot x_n = W \cdot X = X \cdot W$$
其中：$n=特征数，m=样本数$

$$
X^{m \times (n+1)} = \begin{pmatrix} 
1 & x_{1,1} & x_{1,2} & \dots & x_{1,n} \\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,n} \\
\dots \\
1 & x_{m,1} & x_{m,2} & \dots & x_{m,n}
\end{pmatrix}
$$
$$
W^{(n+1) \times 1}= \begin{pmatrix}
w_0 \\
w_1 \\
\dots \\
 w_n
\end{pmatrix}
$$
$$
Y^{m \times 1}= \begin{pmatrix}
y_1 \\
y_2 \\
\dots \\
y_m
\end{pmatrix}
$$

我们期望假设函数的输出与真实值一致，则有：

$$H(w) = X \cdot W = Y$$

直观上看，W = Y/X，但是这里三个值都是矩阵，1/X不一定存在，所以要先把左侧变成方阵，就会有逆矩阵存在了：

$$X^T X W = X^T Y$$

其中，$X^T$是X的转置矩阵，$X^T X$一定是个方阵，并且假设其存在逆矩阵：

$$W = \frac{X^T Y}{X^T X}=(X^T X)^{-1}{X^T Y}$$

## 复杂的推导方法


我们仍然使用均方差损失函数：

$$J(w,b) = \sum (z_i - y_i)^2$$

把b看作是一个恒等于1的feature，并把z=XW计算公式带入：

$$J(w) = \sum (x_i w_i -y_i)^2=(XW - Y)^T \cdot (XW - Y) \tag{变成矩阵形式}$$

对w求导，令导数为0，就是最小值解：

$${\partial J(w) \over \partial w} = {\partial \over \partial w}[(XW - Y)^T \cdot (XW - Y)]$$$$
={\partial \over \partial w}[(X^TW^T - Y^T) \cdot (XW - Y)]$$$$
={\partial \over \partial w}[(X^TXW^TW -X^TW^TY - Y^TXW + Y^TY)]$$

第一项的结果是：$2X^TXW$

第二项和第三项的结果都是：$X^TY$

第四项的结果是：0

令导数为0：
$$
J'(w)=2X^TXW -X^TY - X^TY=0$$$$
2X^TXW = 2X^TY $$$$
X^TXW = X^TY $$$$
W={X^TY \over X^TX}=(X^TX)^{-1}X^TY
$$

以上推导的基本公式可以参考第0章的公式60-69。

逆矩阵$(X^TX)^{-1}$可能不存在的原因是：
1. 特征值冗余，比如$x_2=x^2_1$，即正方形的边长与面积的关系，不能做为两个特征同时存在
2. 特征数量过多，比如特征数n比样本数m还要大

## 应用计算

```Python
if __name__ == '__main__':
    X,Y = LoadData()
    num_example = X.shape[1]
    one = np.ones((num_example,1))
    x = np.column_stack((one, (X[:,0:num_example]).T))
    y = (Y[:,0:num_example]).T

    a = np.dot(x.T, x)
    b = np.asmatrix(a)
    c = np.linalg.inv(b)
    d = np.dot(c, x.T)
    e = np.dot(d, y)
    print(e)
    print("w1=%f,w2=%f,w3=%f,b=%f"%(e[1],e[2],e[3],e[0]))
```
结果：
```
w1=2.000000,w2=-10.000000,w3=5.000000,b=110.000000
```
至此，我们得到了解析解。我们可以用这个做为标准答案，去验证我们的神经网络的训练结果。



