Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 如何用双曲正切函数做二分类函数

木头：在二分类问题中，使用Sigmoid函数作为分类函数，配合二分类交叉熵损失函数：

$$a(z) = \frac{1}{1 + e^{-z}} \tag{1}$$

$$J(w,b)=-\sum_{i=1}^m [y_i \log a_i + (1-y_i) \log (1-a_i)] \tag{2}$$



|Sigmoid的函数图像||
|---|---|
|<img src=".\Images\7\sigmoid.png">|<img src="./Images/3/crossentropy2.png"/>|

还有一个长得非常像的函数，双曲正切函数，图像如下：

$$a(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1 \tag{3}$$

<img src=".\Images\7\tanh.png">


木头：我的问题是，我们能不能用双曲正切函数作为分类函数呢？

铁柱：你的猜想的依据是什么呢？

木头：可以用y=0当分界线，把正负类分开啊！

铁柱：好想法！不过要提醒你，改变分类函数，需要改变其它一系列的网络设置，你可以一步步试试。

木头：好！

## 改变前向计算函数

```Python
# 实现双曲正切函数
def Tanh(z):
    a = 2.0 / (1.0 + np.exp(-2*z)) - 1.0
    return a

# 前向计算
def ForwardCalculationBatch(W, B, batch_X):
    Z = np.dot(W, batch_X) + B
    A = Tanh(Z)
    return A
```

正向计算容易改，反向传播肿么办？需要自己推公式啦！

对公式2求导：
$$
{\partial{J} \over \partial{a_i}}= -{1 \over m} \sum^m_{i=1} ({y_i \over a_i} - {1-y_i \over 1-a_i})=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \tag{4}
$$

对公式3求导：
$$
{\partial{a_i} \over \partial{z_i}}=(1-a_i)(1+a_i) \tag{5}
$$

用链式法则结合公式4,5：

$$
{\partial{J} \over \partial{z_i}}={\partial{J} \over \partial{a_i}}{\partial{a_i} \over \partial{z_i}}
$$

$$
=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \cdot (1+a_i)(1-a_i)
$$

$$
={1 \over m} \sum^m_{i=1}{(a_i-y_i)(1+a_i) \over a_i}\tag{6}
$$


## 改变反向传播函数
```Python
def BackPropagationBatch(batch_X, batch_Y, A):
    m = batch_X.shape[1]
    dZ = (A - batch_Y) * (1 + A) / A
    # dZ列相加，即一行内的所有元素相加
    dB = dZ.sum(axis=1, keepdims=True)/m
    dW = np.dot(dZ, batch_X.T)/m
    return dW, dB
```

貌似这样就改好啦！第一次运行：

```
Try_TanhAsBinaryClassifier.py:84: RuntimeWarning: divide by zero encountered in true_divide
  dZ = (A - batch_Y)*(1+A)/A
Try_TanhAsBinaryClassifier.py:51: RuntimeWarning: divide by zero encountered in log
  p = (1-Y) * np.log(1-A) + Y * np.log(A)
Try_TanhAsBinaryClassifier.py:51: RuntimeWarning: invalid value encountered in multiply
  p = (1-Y) * np.log(1-A) + Y * np.log(A)
```

哦！一脸黑线！出错了！看第一个错误应该是除数为0，即A值为0。

能不能把A从dZ中去掉呢？Tanh函数的导数是固定形式，所以需要修改交叉熵函数，使得损失函数对A的导数中含有(1+A)(1-A)，这样就可以消掉了。根据这样的思路把交叉熵函数修改一下，我们用简写方式，方便推导：

$$J=-[Y \log A + (1-Y) \log (1-A)] \tag{2}$$

改成：

$$J=-[(1+Y) \log (1+A) + (1-Y) \log (1-A)] \tag{7}$$

对公式7求导：

$$
{\partial J \over \partial A} = -({1+Y \over 1+A} - {1-Y \over 1-A})
$$

$$
= -{(1+Y)(1-A)-(1-Y)(1+A) \over (1+A)(1-A)}
$$

$$
= {2(A-Y) \over (1+A)(1-A)}
$$

结合公式5：

$${\partial J \over \partial Z}={\partial J \over \partial A}{\partial A \over \partial Z}$$
$$ ={2(A-Y) \over (1+A)(1-A)} (1+A)(1-A)$$
$$=2(A-Y) \tag{8}$$

好，现在我们需要同时修改损失函数和反向传播函数：

```Python
def CheckLoss(W, B, X, Y):
    m = X.shape[1]
    A = ForwardCalculationBatch(W,B,X)
    p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
    LOSS = np.sum(-p)  #binary classification
    loss = LOSS / m
    return loss

def BackPropagationBatch(batch_X, batch_Y, A):
    m = batch_X.shape[1]
    dZ = 2*(A - batch_Y)
    # dZ列相加，即一行内的所有元素相加
    dB = dZ.sum(axis=1, keepdims=True)/m
    dW = np.dot(dZ, batch_X.T)/m
    return dW, dB
```

第二次运行！...... 一脸迷茫！看打印信息和损失函数图，居然损失函数是个负数！

```
99 180 -0.7081894060282795 [[-3.18144618  0.97507924]] [[2.66383491]]
99 190 -0.7622300699620478 [[-3.28030406  0.9501165 ]] [[2.47940822]]
[[-3.35385892  0.86326858]] [[2.41883365]]
epoch=85, iteration=78, loss=-0.771139
W= [[-3.35385892  0.86326858]]
B= [[2.41883365]]
result= [[0.67791877 0.82810272 0.82495719]]
[[1. 1. 1.]]
```
<img src=".\Images\7\try_tanh_loss_2.png">

木头：咋回事呢？难道是因为改了损失函数形式吗？

铁柱：当然啦！损失函数以前的形式是

$$J=-(Ylog(A)+(1-Y)log(1-A))$$

由于使用Sigmoid函数，所以A的值永远在(0,1)之间，所以$log(A)$和$log(1-A)$都是负数。而Y的值是0或1，最终J的结果是个正数。

改成1+A后：

$$J=-((1+Y)log(1+A)+(1-Y)log(1-A))$$

Tanh函数输出值A为(-1,1)，这样$1+A \in (0,2)$，$1-A \in (0,2)$，$log(1+A)$和$log(1-A)$的值很有可能大于0，导致J为负数。如果仍然想用交叉熵函数，必须满足其原始设计，让1+A和1-A都在(0,1)值域内。

木头：哦！明白了，那好办，我把它们哥儿俩都除以2就好了！

$$J=-((1+Y)log({1+A \over 2})+(1-Y)log({1-A \over 2})) \tag{9}$$

虽然分母有个2，但是对导数公式没有影响，最后的结果仍然是：

$${\partial J \over \partial Z} =2(A-Y) \tag{8}$$

修改损失函数代码如下：

```Python
def CheckLoss(W, B, X, Y):
    m = X.shape[1]
    A = ForwardCalculationBatch(W,B,X)
    p = (1-Y) * np.log((1-A)/2) + (1+Y) * np.log((1+A)/2)
    LOSS = np.sum(-p)  #binary classification
    loss = LOSS / m
    return loss
```

第三次运行：

|损失函数值|分类结果|
|-----|-----|
|<img src=".\Images\7\try_tanh_loss_3.png">|<img src=".\Images\7\try_tanh_result_3.png">|

这次的loss值曲线非常好，值域正确并且收敛了。可是看分类结果，为什么分界线整体向右偏移了呢？

一脸雾水！

铁柱：提示一下，你再比较一下Sigmoid和Tanh的输出值域吧！

木头：(半分钟后)......哦！明白啦！Tanh的输出值域是(-1,1)，Sigmoid的输出值域是(0,1)。从前面讲过的二分类原理看，Sigmoid是假设所有正类的标签值都是，负类的标签值都是0。而Tanh要求的是-1和1，所以我们要把标签值改一下。


```Python
def ToBool(YData):
    num_example = YData.shape[1]
    Y = np.zeros((1, num_example))
    for i in range(num_example):
        if YData[0,i] == 0:     # 第一类的标签设为0
            Y[0,i] = -1
        elif YData[0,i] == 1:   # 第二类的标签设为1
            Y[0,i] = 1
        # end if
    # end for
    return Y
```
注意上面的Y[0,i]=-1，原本是0。

第四次运行！......Perfect!

<img src=".\Images\7\try_tanh_result_4.png">

最后我们对比一下Sigmoid和Tanh以及它们对应的交叉熵函数的图像：

|激活函数|交叉熵函数|
|---|---|
|<img src=".\Images\7\sigmoid_seperator.png">|<img src="./Images/3/crossentropy2.png"/>|
|上图：输出值域a在(0,1)之间，分界线为a=0.5, 标签值为y=0,1|上图：y=0为负例, y=1为正例 ,a在(0,1)之间|
|<img src=".\Images\7\tanh_seperator.png">|<img src="./Images/7/modified_crossentropy.png"/>|
|上图：输出值域a在(-1,1)之间，分界线为a=0.0，标签值为y=-1,1|上图：y=-1为负例, y=1为正例, a在(-1,1)之间|

代码位置：ch07, Level3_TanhAsBinaryClassifier, Level3_TanhWithCrossEntropy
