Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 算法模拟效果比较

为了简化起见，我们先用一个简单的二元二次函数来模拟损失函数的等高线图，测试一下我们在前面实现的各种优化器。

$$z = {x^2 \over 10} + y^2 \tag{1}$$

公式1是模拟均方差函数的形式，它的正向计算和反向计算的Python代码如下：

```Python
def f(x, y):
    return x**2 / 10.0 + y**2

def derivative_f(x, y):
    return x / 5.0, 2.0*y
```

我们依次测试4种方法：

- 普通SGD, 学习率0.95
- 动量Momentum, 学习率0.1
- RMPSProp，学习率0.5
- Adam，学习率0.5

每种方法都迭代20次，记录下每次反向过程的(x,y)坐标点，绘图如下：

<img src=".\Images\14\Optimizers_sample.png">

- SGD算法，每次迭代完全受当前梯度的控制，所以会以折线方式前进
- Momentum算法，学习率只有0.1，每次继承上一次的动量方向，所以会以比较平滑的曲线方式前进，不会出现突然的转向
- RMSProp算法，有历史梯度值参与做指数加权平均，所以可以看到比较平缓，不会波动太大，都后期步长越来越短也是符合学习规律的
- Adam算法，

# 算法真实效果比较


<img src=".\Images\14\Optimizers_real.png">

<img src=".\Images\14\Optimizers_zoom.png">



# 各种不同的算法的比较图

以下图片来自[Alec Radford](https://twitter.com/alecrad).

## 在Beale函数上的表现
<img src=".\Images\14\1.gif">

Beale函数的形式是：

$$f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2$$

## 在鞍点上的表现

<img src=".\Images\14\2.gif">

鞍点上一个维度的梯度向上，另一个维度向下。SGD无法前进，一直在那里徘徊。动量和Nag最终逃脱了鞍点。其它算法很快找对了方向。


**课后作业**

使用第九章的非线性三分类数据，运行各种优化算法，找到最佳初始学习率。




代码位置：ch14, Level2, Level3