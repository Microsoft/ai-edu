Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 9.7 参数调优

### 9.7.1 可调的参数

我们使用如下参数做第一次的训练：

|参数|缺省值|是否可调|注释|
|---|---|---|---|
|输入层神经元数|1|No|
|隐层神经元数|4|Yes|影响迭代次数|
|输出层神经元数|1|No|
|学习率|0.1|Yes|影响迭代次数|
|批样本量|10|Yes|影响迭代次数|
|最大epoch|10000|Yes|影响终止条件,建议不改动|
|损失门限值|0.001|Yes|影响终止条件,建议不改动|
|损失函数|MSE|No|
|权重矩阵初始化方法|Xavier|Yes|参看15.1|

上述表格中的参数，最后可以调节的其实只有三个：
- 隐层神经元数
- 学习率
- 批样本量

另外还有一个权重矩阵初始化方法需要特别注意，我们在下一个段落中讲解。

最大epoch数，根据不同的模型和案例会有所不同，在本例中10000次足以承载所有的超参组合了。损失门限值是一个后验数值，也就是说笔者通过试验，事先知道了当eps=0.001时，会训练出精度可接受的模型来，这个问题在实践中也只能摸着石头过河，因为在训练一个特定模型之前，谁也不能假设它能到达的精度值是多少，损失函数值的下限也是通过多次试验，通过历史记录的趋势来估算出来的。

如果读者不了解神经网络中的基本原理，那么所谓“调参”就是碰运气了。今天咱们可以试着改变几个参数，来看看训练结果，以此来增加对神经网络中各种参数的了解。

### 9.7.2 权重矩阵初始化

权重矩阵中的参数，是神经网络要学习的参数，所以不能称作超参数。

权重矩阵初始化是神经网络训练非常重要的环节之一，不同的初始化方法，甚至是相同的方法但不同的随机值，都会给结果带来或多或少的影响。

在后面的几组比较中，都是用Xavier方法初始化的。另外，两次使用Xavier初始化，也会得到不同的结果，为了避免这个随机性，我们在代码WeightsBias.py中使用了一个小技巧，调用下面这个函数：

```Python
class WeightsBias(object):
    def InitializeWeights(self, folder, create_new):
        self.folder = folder
        if create_new:
            self.__CreateNew()
        else:
            self.__LoadExistingParameters()
        # end if

    def __CreateNew(self):
        self.W, self.B = WeightsBias.InitialParameters(self.num_input, self.num_output, self.init_method)
        self.__SaveInitialValue()
        
    def __LoadExistingParameters(self):
        file_name = str.format("{0}\\{1}.npz", self.folder, self.initial_value_filename)
        w_file = Path(file_name)
        if w_file.exists():
            self.__LoadInitialValue()
        else:
            self.__CreateNew()
```

第一次调用InitializeWeights()时，会得到一个随机初始化矩阵。以后再次调用时，如果设置create_new=False，只要隐层神经元数量不变并且初始化方法不变，就会用第一次的初始化结果，否则后面的各种参数调整的结果就没有可比性了。

### 9.7.3 学习率的调整

我们固定其它参数，改变学习率，下面是损失函数值的曲线：

<img src="..\Images\9\eta.png">

|学习率|迭代次数|说明|
|----|----|----|
|0.1|9540|学习率小，收敛最慢|
|0.3|4360|学习率增大，收敛增快|
|0.5|2780|最快|
|0.7|3040|学习率进一步增大，但收敛不一定快|

对于拟合曲线这个特定问题，较大的学习率可以带来很快的收敛速度，但是有两点：
- 但并不是对所有问题都这样，有的问题可能需要0.001或者更小的学习率
- 学习率大时，开始时收敛快，但是到了后来有可能会错失最佳解

### 9.7.4 批大小的调整

我们固定其它参数，调整批大小，比较结果如下：

<img src="..\Images\9\batchsize.png">

|批大小|迭代次数|说明|
|----|----|----|
|1|4680|批数据量小到1，收敛慢|
|5|2540|批数据量增大，收敛最快|
|10|2780|批数据量进一步增大，收敛变慢|
|20|4670|批数据量太大，反而会降低收敛速度|

合适的批样本量会带来较快的收敛，前提是我们固定了学习率。如果想用较大的批数据，底层数据库计算的速度较快，但是需要同时调整学习率，才会相应地提高收敛速度。

这个结论的前提是我们用了0.5的学习率，如果用0.1的话，将会得到不同结论。

### 9.7.5 隐层神经元数量的调整

这次我们调整隐层神经元的数量：

<img src="..\Images\9\neuron_number.png">

|隐层神经元数量|迭代次数|说明|
|---|---|---|
|2|9990|神经元数量少，拟合能力低|
|4|2540|神经元数量对于这个问题最合适|
|6|4200|神经元多了不一定能帮上忙，还有可能帮倒忙|
|8|3470|再多一些神经元会有一些用处|

对于这个特定问题，隐层神经元个数为4时，收敛速度最快。

### 思考与练习

使用下列参数设置，找到批大小和学习率的关系：

- 隐层神经元：4
- 初始化：Xavier
- 批大小选择：1，5，10，15，20，25，30
- 学习率选择：0.1，0.3，0.5，0.7

得到下表：

|批大小|1|5|10|15|20|
|---|---|---|---|---|---|
|0.1|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.3|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.5|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.7|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.9|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|

从而得到批大小与学习率的最佳组合。

### 代码位置

ch09, Level5