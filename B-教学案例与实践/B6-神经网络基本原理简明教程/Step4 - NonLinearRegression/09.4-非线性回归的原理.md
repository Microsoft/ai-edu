Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 9.3 双层神经网络实现非线性回归

### 9.3.1 通用近似定理

这里有一篇论文，Kurt Hornik在1991年发表的，说明了含有一个隐层的神经网络能拟合任意复杂函数。

https://www.sciencedirect.com/science/article/pii/089360809190009T

简言之：两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它能以任意精度来近似拟合任意连续函数。当然这个函数需要是单调递增有界的。

注意，它要求的是挤压性质的激活函数，也就是类似Sigmoid的函数，如果用ReLU函数不能实现这个定理。

#### 直观解释

还有一种直观的解释是这样的，用无数个矩形的拼接来近似一条曲线：

|粗粒度|细粒度|
|---|---|
|<img src="..\Images\9\histogram1.jpg">|<img src="..\Images\9\histogram2.jpg">|

它的理论基础是：

<img src="..\Images\9\histogram3.jpg">

假设在隐层有两个神经元，都配置有Sigmoid激活函数。第一个神经元在-0.01处产生一个阶跃，第二个神经元在+0.01处产生一个阶跃，都是用Sigmoid函数完成的，然后用第一个神经元的输出减去第二个神经元的输出（设置权重值为[1,-1]）,就会形成一个“门”。无数个这样的“门”就能模拟出一条曲线。

对于三维空间，它是这个样子的：

|阶跃面|封闭塔|
|---|---|
|<img src="../Images/9/histogram4.jpg">|<img src="../Images/9/histogram5.jpg">|

在三维空间中，两个有不同偏置值的sigmoid激活函数相减，我们将得到左侧的等效曲线。如果我们采用另一个水平垂直的塔架到现在组合的曲线上。在叠加这两个水平垂直的开放式塔时，我们就可以得到封闭的塔。然后就可以用封闭塔模拟任何三维曲面。

### 参考资料

- https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586

### 9.3.2 双层神经网络的结构

## 前向计算

在本章中，为了完成复杂函数拟合任务，我们将使用如下正向过程：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$


以及均方差损失函数：

$$J(w,b) = \frac{1}{2m}\sum^m_{i=1}(z_i-y_i)^2$$

前向计算图：

<img src="..\Images\9\forward.png">

木头：这里有三个疑问：

1. 为什么要用两层神经网络？
2. 为什么在神经网络的第二层，没有用到激活函数？
3. 为什么我们用均方差损失函数，而不是交叉熵损失函数？

铁柱：这先要理解神经网络的原理，我们一个个地回答你的问题。

## 为什么要用两层神经网络？

首先，一层神经网络肯定不能完成这个复杂函数的拟合过程。因为一层神经网络，只能完成线性任务。这里的“线性任务”的定义，从简单到复杂，列表如下：

|名称|形式|能力
|---|---|---|
|单变量线性回归|$y=w_0+w_1·x$|拟合二维平面直线|
|多变量线性回归|$y=w_0+w_1·x_1+w_2·x_2...$|拟多高维空间直线或平面|
|高阶线性回归|$y=w_0+w_1·x_1+w_2·x^2_1...$|拟合二维平面高阶曲线|
|多变量高阶线性回归|$y=w_0+w_1·x_1+w_2·x_2+w_3·x^2_1+w_4·x^2_2...$|拟合多维空间高阶曲线或曲面|

所谓的“高阶”，指的是特征变量其实只有一个x1，但是把x1的平方也算作第二个特征向量。比如一栋房子的长度x1，宽度x2，占地面积x3=x1*x2。这里的x3并不是独立存在的，真正的自变量只有x1和x2。

这些高次线性回归问题，可以用单层的神经网络来解决，但是是有前提条件的，即假设函数必须和实际问题吻合。满足这个条件的实际工程问题并不多见，并且这种情况完全可以用两层的神经网络来解决，所以我们没有在单层的神经网络中涉及这个问题。

## 为什么在输出层没有用到激活函数？

神经网络不管有多少层，最后的输出层决定了这个神经网络能干什么。在单层神经网络中，我们学习到了以下示例：

|网络|输入|输出|激活函数|功能|
|---|---|---|---|---|
|单层|单变量|单输出|无|二维线性回归/拟合|
|单层|多变量|单输出|无|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|二分类|
|单层|多变量|多输出|多分类函数|多分类|

对于多层神经网络也是如此，我们要完成拟合任务，而不是分类，所以用不到激活/分类函数。通常把激活函数和分类函数混淆在一起说，如果明确地区分二者，则可以这样说：**神经网络的最后一层不用激活函数**，只可能用到分类函数。Sigmoid既是激活函数，又是分类函数，是个特例。

神经网络的拟合原理是这样的：在第一层神经网络，通过$W1*X+B1$的计算做线性变化，把非线性问题转换成线性问题；在第二层神经网络做线性回归。所以在第二层是不需要激活函数的，否则就没法画出一条直线来。这个可以想象两个独立的神经网络，第一个网络已经把数据处理成线性的了，以便让我们使用第4章的方法，做一次线性回归就好了。

简言之：

1. 神经网络最后一层不需要激活函数
2. 激活函数只用于连接前后两层神经网络

## 为什么用均方差而不是交叉熵损失函数？

我们把上面的表格拿来再扩充一下：

|网络|输入|输出|激活函数|损失函数|功能|
|---|---|---|---|---|---|
|单层|单变量|单输出|无|均方差|二维线性回归/拟合|
|单层|多变量|单输出|无|均方差|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|交叉熵|二分类|
|单层|多变量|多输出|多分类函数|交叉熵|多分类|

交叉熵函数是用于分类的，均方差函数是用于拟合的，可以理解为计算拟合的点和样本标签点的距离之平方和。

## 反向传播

木头：由于是第一次接触两层的神经网络，我看到反向传播的函数体中，比起一层的网络复杂了很多。您能详细解释一下吗？

铁柱：看一下计算图，然后用链式求导法则反推。

<img src="..\Images\9\backward.png">

蓝色的箭头线表示正向计算过程，黄色的箭头线表示反向的传播过程。

### 梯度生成

对损失函数求导，可以得到损失函数对输出层的梯度值，即上图中的Z2部分。

因为：

$$J(w,b) = \frac{1}{2m} \sum (z_i-y_i)^2$$

$${\partial{J} \over \partial{z_i}}=\frac{1}{2m} \sum {\partial{(z_i-y_i)^2} \over \partial{z_i}}=\frac{1}{m} \sum (z_i-y_i)$$

用于矩阵运算，可以简写为：

$$dZ2 = \frac{\partial{J}}{\partial{Z2}} = Z2-Y \tag{1}$$

### 求W2的梯度

$$Z2 = W2 \cdot A1+B2$$

$${\partial{Z2} \over \partial{W2}}=\frac{\partial{(W2 \cdot A1+B2)}}{\partial{W2}}$$

$$=\frac{\partial{(W2 \cdot A1)}}{\partial{W2}}+\frac{\partial{(B2)}}{\partial{W2}}$$

$$=A1^T+0=A1^T$$

结合损失函数对Z2的偏导结果，使用链式法则：

$$
dW2 = \frac{\partial{J}}{\partial{W2}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{W2}}
$$
$$=(Z2-Y)A1^T \tag{2}$$

### 求B2的梯度

$$dB2 = \frac{\partial{J}}{\partial{B2}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{B2}}$$

$$=(Z2-Y) \cdot 1=Z2-Y \tag{3}$$

### 求损失函数对隐层的梯度

对于深度神经网络，需要把梯度从最后一层逐层向前传递，经过激活函数的导数，直接达到线性计算部分，即下图中的Z1部分：

<img src="..\Images\9\backward.png">

链式求导公式如下：

$$
\frac{\partial{J}}{\partial{Z1}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{A1}} \cdot \frac{\partial{A1}}{\partial{Z1}}
$$

公式(1)已经有了第一项的结果，现在来解决后面两项：

$$\frac{\partial{Z2}}{\partial{A1}} = \frac{\partial{(W2 \cdot A1 + B2)}}{\partial{A1}}=W2^T$$

$$\frac{\partial{A1}}{\partial{Z1}}=\frac{\partial{(Sigmoid(Z1))}}{\partial{Z1}}=A1 \odot (1-A1)$$

所以：

$$dZ1=\frac{\partial{J}}{\partial{Z1}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{A1}} \cdot \frac{\partial{A1}}{\partial{Z1}}$$
$$=W2^T \times dZ2 \odot A1 \odot (1-A1) \tag{4}$$

### 求W1的梯度

$$\frac{\partial{Z1}}{\partial{W1}} = \frac{\partial{(W1 \cdot X+B1)}}{\partial{W1}} = X^T$$

$$
dW1=\frac{\partial{J}}{\partial{W1}} = \frac{\partial{J}}{\partial{Z1}} \frac{\partial{Z1}}{\partial{W1}}= dZ1 \times X^T \tag{5}
$$

### 求B1的梯度

$$\frac{\partial{Z1}}{\partial{B1}} = \frac{\partial{(W1 \cdot X+B1)}}{\partial{B1}} = 1$$

$$
dB1=\frac{\partial{J}}{\partial{B1}} = \frac{\partial{J}}{\partial{Z1}} \frac{\partial{Z1}}{\partial{B1}}= dZ1 \tag{6}
$$


