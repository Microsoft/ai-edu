Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
在这一章，我们将简要介绍一下激活函数，因为在下一章中将要使用激活函数构造神经网络。

- [挤压（饱和）型激活函数](07.1-挤压型激活函数.md)
- [半线性（非饱和）激活函数](07.2-半线性激活函数.md)
- [用双曲正切函数分类](07.3-用双曲正切函数分类.md)
- [实现逻辑与门和或门](07.4-实现逻辑与门和或门.md)

# 激活函数

看神经网络中的一个神经元，为了简化，假设该神经元接受三个输入，分别为$x_1, x_2, x_3$,那么$z=\sum\limits_{i}w_ix_i+b_i$,

<img src="..\Images\1\NeuranCell.png">

激活函数也就是$A=a(Z)$这一步了，他有什么作用呢？主要是给神经网络增加非线性因素，这个问题在第1章《神经网络基本工作原理》中已经讲过了。

激活函数的基本性质：

+ 非线性：线性的激活函数和没有激活函数一样。

+ 可导性：做误差反向传播和梯度下降，必须要保证激活函数的可导性。

+ 单调性：单一的输入会得到单一的输出，较大值的输入得到较大值的输出

在物理试验中使用的继电器，是最初的激活函数的原型：当输入电流大于一个阈值时，会产生足够的磁场，从而打开下一级电源通道，如下图所示：

<img src="..\Images\8\step.png">

用到神经网络中的概念，用‘1’来代表一个神经元被激活，‘0’代表一个神经元未被激活。

这个函数有什么不好的地方呢？主要的一点就是，他的梯度（导数）恒为零（个别点除外)。反向传播公式中，梯度传递用到了链式法则，如果在这样一个连乘的式子其中有一项是零，这样的梯度就会恒为零，是没有办法进行反向传播的。



## 为什么在输出层没有用到激活函数？

神经网络不管有多少层，最后的输出层决定了这个神经网络能干什么。在单层神经网络中，我们学习到了以下示例：

|网络|输入|输出|激活函数|功能|
|---|---|---|---|---|
|单层|单变量|单输出|无|二维线性回归/拟合|
|单层|多变量|单输出|无|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|二分类|
|单层|多变量|多输出|多分类函数|多分类|

对于多层神经网络也是如此，我们要完成拟合任务，而不是分类，所以用不到激活/分类函数。通常把激活函数和分类函数混淆在一起说，如果明确地区分二者，则可以这样说：**神经网络的最后一层不用激活函数**，只可能用到分类函数。Sigmoid既是激活函数，又是分类函数，是个特例。

神经网络的拟合原理是这样的：在第一层神经网络，通过$W1*X+B1$的计算做线性变化，把非线性问题转换成线性问题；在第二层神经网络做线性回归。所以在第二层是不需要激活函数的，否则就没法画出一条直线来。这个可以想象两个独立的神经网络，第一个网络已经把数据处理成线性的了，以便让我们使用第4章的方法，做一次线性回归就好了。

简言之：

1. 神经网络最后一层不需要激活函数
2. 激活函数只用于连接前后两层神经网络

## 为什么用均方差而不是交叉熵损失函数？

我们把上面的表格拿来再扩充一下：

|网络|输入|输出|激活函数|损失函数|功能|
|---|---|---|---|---|---|
|单层|单变量|单输出|无|均方差|二维线性回归/拟合|
|单层|多变量|单输出|无|均方差|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|交叉熵|二分类|
|单层|多变量|多输出|多分类函数|交叉熵|多分类|

交叉熵函数是用于分类的，均方差函数是用于拟合的，可以理解为计算拟合的点和样本标签点的距离之平方和。

