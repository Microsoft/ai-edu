Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 解决训练不收敛问题

仔细分析一下屏幕打印信息：
```
0 0 48904435754.68428 [[  93.8  187.6 3705.1]] [[46.9]]
0 1 1.8166533368107932e+16 [[  -57809.94  -115619.88 -2254540.76]] [[-28904.97]]
```
前两次迭代的损失值已经是天文数字了，后面的W和B的值也在不断变大，说明网络发散了。

[参考资料: My Nueral Network isn't working! What should I do?](http://theorangeduck.com/page/neural-network-not-working)

难度我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。别慌，擦干净头上的冷汗，踩着自己的尸体继续前行！


回想一个问题：为什么在第4章中，我们没有遇到这种情况？把样本拿来看一看：

|样本序号|1|2|3|...|200|
|---|---|---|---|---|---|
|服务器数量(千)|0.928|0.0469|0.855|...|0.373|
|空调功率(千瓦)|4.824|2.950|4.643|...|3.594|

因为所有的X值（服务器数量）都是在[0,1]之间的，而本次的数据有三个特征值，全都是不是在[0,1]之间的，并且取值范围还不相同。我们不妨把本次样本数据也做一下这样的处理，亦即“归一化”。

**其实，数据归一化是深度学习的必要步骤之一，已经是魔法师们家喻户晓的技能，也因此它很少被各种博客/文章所提及，以至于麻瓜们经常被坑。**

# 为什么要做归一化

木头：老师，归一化原来这么重要，在理论上是怎么解释呢？和飞行员的身高要求是一个道理吗？

铁柱：理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说：

1. 样本的各个特征的取值要符合概率分布，即[0,1]
2. 样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！
3. 神经网络假设所有的输入输出数据都是标准差为1，均值为0，包括权重值的初始化，激活函数的选择，以及优化算法的的设计。

4. 数值问题

    归一化/标准化可以避免一些不必要的数值问题。因为sigmoid/tanh的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tanh( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。

5. 初始化
    
    在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
    
6. 梯度
    
    以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是误差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
    
7. 学习率：
   
    知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
    

参考链接：https://www.jianshu.com/p/95a8f035c86c


木头：是不是说，数据中地理位置的取值范围是[2,6]，而房屋面积的取值范围为[40,120]，二者相差太远，根本不可以放在一起计算了？

铁柱：是的，你看$W_1X_1+W_2X_2$这个式子中，如果X1的取值是[2,6]，X2的取值是[40,120]，一是相差太远，而是都不在[0,1]之间，所以对于神经网络来说很难理解。下图展示了归一化前后的情况Loss值的等高图，意思是地理位置和房屋面积取不同的值时，作为组合来计算损失函数值时，形成的类似地图的等高图。左侧为归一化前，右侧为归一化后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

木头：我还听说有一些类似的词汇，什么标准化，这个和归一化有什么区别呢？

铁柱：先看几个基本概念：

- 均值mean：

$$\bar{x}=\frac{1}{n}\sum_i^nx_i \tag{1}$$

- 标准差stdandard deviation：

$$
std=\sqrt{\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}} \tag{2}
$$

- 方差variance

$$
var=\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}=s^2 \tag{3}
$$


- 协方差covariance
$$
cov(X,Y)=\frac{1}{n-1} \sum_i^n{[(x_i-\bar{x})(y_i-\bar{y})]} \tag{4}
$$
结果为正，表示X,Y是正相关。


铁柱：有三个类似的概念，归一化，标准化，中心化。

### 归一化

把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。

归一化有三种方法：
- Min-Max归一化：
$$x_{new}={x-MinValue \over MaxValue - MinValue} \tag{5}$$
- 平均值归一化： 
$$x_{new} = {x - \bar{x} \over MaxValue - MinValue} \tag{6}$$

- 非线性归一化：
$$对数转换y=log(x)，反余切转换y=atan(x) \cdot 2/π  \tag{7}$$

### 标准化
把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。

Z-score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：
$$x_{new} = (x - \bar{x})／std \tag{8}$$

### 中心化

平均值为0，无标准差要求：
$$x_{new} = x - \bar{x} \tag{9}$$


# 数据归一化

## 样本分析

再把这个表拿出来分析一下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|价格（万元）|434|500|321|482|...|410|

我们用min-max标准化来归一以上的样本特征数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|0|0.667|0.333|1|...|0.33|
|地理位置（几环）|0.25|0|1|0.25|...|0.75|
|居住面积（平米）|0.7|0.75|0.175|0.4|...|0.36|
|价格(万元)|434|500|321|482|...|410|

注意：
1. 我们并没有归一化样本的标签Y数据，所以最后一行的价格还是保持不变
2. 我们是对三行特征值分别做归一化处理的

## 归一化的Python实现

我们把归一化的函数写好：
```Python
def NormalizeByData(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    x_range = np.zeros((1,n))
    x_min = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        max_value = np.max(x)
        min_value = np.min(x)
        x_min[0,i] = min_value
        x_range[0,i] = max_value - min_value
        x_new = (x - x_min[0,i])/(x_range[0,i])
        X_new[i,:] = x_new
    return X_new, x_range, x_min

def NormalizeXY(XData, YData, flag):
    if flag=='x_only':
        X, X_range, X_min = NormalizeByData(XData)
        return X, X_range, X_min, YData, -1, -1
    elif flag=='x_and_y':
        X, X_range, X_min = NormalizeByData(XData)
        Y, Y_range, Y_min = NormalizeByData(YData)
        return X, X_range, X_min, Y, Y_range, Y_min
```
- x_range：表示样本特征值的取值范围
- x_min：表示样本特征值的取值范围的下限
- y_range：表示样本标签值的取值范围
- y_min：表示样本标签值的取值范围的下限

然后再改一下主程序的头两行，加上归一化的调用，'x_only'表示只对特征值归一化，不改变标签值：
```
flag = 'x_only' # only normalize X value
XData, YData = ReadData()
X, X_range, X_min, Y, Y_range, Y_min = NormalizeXY(XData, YData, flag)
```
用颤抖的双手同时按下Ctrl+F5，运行开始，结束，一眨眼！仔细看打印结果：
```
......
35 600 [[  5.99999997 -40.00000004 394.9999999 ]] [[292.00000009]] 344.8930674898838
35 700 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 284.6144928234677
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
[[36838.99999298]]
```
# 房屋价格预测

W，B的值都算出来了，说明网络收敛了，这是成功的第一步。再看最后的预测值，93平米的房子需要36839万元！难道是传说中的黄金屋？这不科学！

## 还原真实的W值

这次错在哪里了呢？我们唯一修改的地方就是样本数据特征值的归一化。假设在归一化之前，真实的样本值是X，真实的权重值是W，在归一化之后，样本值变成了X'，训练出来的权重值是W'，不考虑B值的情况下：
$$
Y = W \cdot X \tag{标签值}
$$
$$
Z = W' \cdot X' \tag{预测值}
$$

由于训练时标签值（房价）并没有做归一化，意味着我们是用真实的房价做的训练，所以预测值和标签值应该相等：
$$
因为：Y == Z \\
所以：W \cdot X = W' \cdot X'
$$

假设X'是X的1/N大小：
$$
N*X' = X
$$

把X代入等式左侧:
$$
W \cdot N \cdot X'=W' \cdot X' => W \cdot N = W' => 即：W = W'/N
$$

也就是说我们应该把训练的结果W'除以N倍后，就可以得到真实的W值。N是什么呢？就是X_range。对于本例来说，X_range是一个数组，有三个元素，分别代表三个特征向量值归一化前的取值范围。

## 还原真实的B值

既然W的值不是真实值，需要还原，那么B值一定也是如此。如果我们得到了W的真实值，那么B的真实值也很容易得到，我们还是利用样本数据来做这件事。

因为样本数据标签值$Y = W \cdot X+B$，我们已经有了还原后的W的值，归一化前的样本X值也存在，标签值Y也存在，则：
$$B=Y - W*X$$

## 变成代码
```Python
# get real weights
def DeNormalizeWeights(X_range, XData, n):
    W_real = np.zeros((1,n))
    for i in range(n):
        W_real[0,i] = W[0,i] / X_range[0,i]
    print("W_real=", W_real)

    B_real = 0
    num_sample = XData.shape[1]
    for i in range(num_sample):
        xm = XData[0:n,i].reshape(n,1)
        zm = ForwardCalculation(xm, W_real, 0)
        ym = Y[0,i].reshape(1,1)
        B_real = B_real + (ym - zm)
    B_real = B_real / num_sample
    print("B_real=", B_real)
    return W_real, B_real
```
X_range是我们在做归一化时保留下来的样本的三个特征向量的取值范围，存放在一个三个元素的数组中。XData是原始值，与W_real相乘以后，再用标签值Y去减，就会得到B值。为了避免样本噪音，用了所有的样本来计算差值的总和，最后除以样本总数，就是B_real。

在主程序最后两行加一下这个逻辑，再次运行：

```Python
W_real, B_real = DeNormalizeWeights(X_range, XData, n)
PredicateTest(W_real, B_real)
```
运行结果如下：
```
W= [[  5.9996169  -40.00073494 394.99808248]]
B= [[292.00163197]]
W_real= [[  1.9998723  -10.00018374   4.99997573]]
B_real= [[110.00302237]]
result=[[528.99999999]]
```
W的真实值W_real=[1.99, -10, 4.99]，B的真实值B_real=110，那间93平米的房子的预测值=529万元。相当make sense！

至此，我们完美地解决了北京地区的房价预测问题！但是还没有解决自己可以有能力买一套北京的房子的问题......

Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

“铁柱”是一名老师，在神经网络中穿梭多年，挂了满身的蜘蛛网。“木头”是一名刚入门者，木头木脑的，有问题经常向铁柱请教。


# 正确的预测房屋价格的方法

木头：在正文中，咱们用训练出来的模型预测房屋价格，还需要先还原W和B的值，我觉得有些有点儿麻烦，有没有别的更好的办法？

铁柱：在线性问题中，我们还能够还原W和B的值，在非线性问题中，我们可能根本做不到这一点。所以，咱们可以把需要预测的数据，也先做归一化，送进模型中做推理计算。

木头：哦！对啊！因为我们训练时并没有把样本标签值做改变，所以预测的值应该还是有效的，不会脱离样本标签值的范围。

铁柱：那你来看看应该怎么做？

木头：（劈里啪啦敲键盘）...（铁柱听着机械键盘的声音皱了皱眉）...（10分钟后）......我写了段代码，您给看看：
```Python
# normalize data by specified range and min_value
def NormalizeByRange(X, x_range, x_min):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    for i in range(n):
        x = X[i,:]
        x_new = (x-x_min[0,i])/x_range[0,i]
        X_new[i,:] = x_new
    return X_new

# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)
    print("z1=", z1)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)
    print("z2=", z2)
```
我先增加了一个函数，NormalizeByRange，根据样本训练时的样本归一化方法，同样地处理一下预测样本xt，得到归一化后的xt_new，然后用训练出来的W/B的值来计算，得到z2，结果是这样的：
```
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
W_real= [[  1.99999999 -10.00000001   5.        ]]
B_real= [[110.00000012]]
z1= [[528.99999999]]
z2= [[528.99999999]]
```
可以看到z1和z2的预测值是相同的，两种方法都有效，但第二种方法看起来比较容易理解。

# 对标签值归一化

铁柱：木头不错哦！

木头：一般一般，年级第三！

铁柱：吼吼，还挺押韵。再给你提个问题，你有没有注意，在计算Loss值时，会达到172.287，337.246这样大的数值，是什么原因呢？我们第四章的Loss值可都是小于1的数。

木头：哦！啊！这个......（喝了一口82年的雪碧压压惊）......我想想啊......

铁柱：心动不如行动，想什么想，动手！

木头：吼的！......（劈里啪啦敲键盘）.......（铁柱听着那机械键盘的声音觉得闹得慌）......（20分钟后）......老师，我把PredicateTest函数改了一下：

```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B, flag):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)

    if flag == 'x_only':
        print("xt,W_real,B_real:",z1)
        print("xt_new,W,B:",z2)
    elif flag == 'x_and_y':
        print("xt,W_real,B_real:", z1*Y_range+Y_min)
        print("xt_new,W,B:", z2*Y_range+Y_min)
```
铁柱：哦？当flag=='x_and_y'时，为什么z1,z2要做这样的转换呢？

木头：flag='x_and_y'，意思是在训练时我们对Y值也做了归一化，所以，我想试一下反归一化，因为：

$$y_{new} = \frac{y-y_{min}}{y_{max}-y_{min}} = \frac{y-y_{min}}{y_{range}}$$

所以：

$$y = y_{new}*y_{range}+y_{min}$$

然后，把主程序第一行的flag值改成：
```Python
flag = 'x_and_y'
```
运行......结果如下：
```
15 900 [[ 0.0137157  -0.09154976  0.90384324]] [[0.08242129]] 0.00011344288067411854
16 0 [[ 0.01371643 -0.09154922  0.90384595]] [[0.08241911]] 0.0009021195541084537
16 100 [[ 0.01371701 -0.0915483   0.90384837]] [[0.08241596]] 0.0017657981568301397
W= [[ 0.01371698 -0.09154826  0.90384839]]
B= [[0.08241599]]
W_real= [[ 0.00457233 -0.02288706  0.01144112]]
B_real= [[-0.33402734]]
xt,W_real,B_real: [[528.99654603]]
xt_new,W,B: [[528.99670957]]
```
哈哈！82年的雪碧果然厉害！我们只迭代了16轮就结束了，而且得到了正确的预测值！

铁柱：不错不错！我们来总结一下正确的归一化和反归一化的关系：

|归一化|Weight|Bias|预测值Xt|预测方法|
|---|---|---|---|---|
|只归一化X|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$|
|只归一化X|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$|
|同时归一化X和Y|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$<br>$Z'=Z * Y_{range} + Y_{min}$|
|同时归一化X和Y|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$ <br> $Z'=Z * Y_{range}+Y_{min}$|

木头：哦！这样一总结，就可以一目了然啦！我也简单总结一下：

1. X必须归一化，否则无法训练
2. 训练出的结果W和B，在推理时有两种使用方式：
a. 直接使用，此时必须把推理时输入的X也做相同规则的归一化
b. 反归一化为W,B的本来值$W_{Real},B_{Real}$，推理时输入的X不需要改动
3. Y可以归一化，好处是迭代次数少。如果结果收敛，也可以不归一化，如果不收敛（数值过大），就必须归一化
4. 如果Y归一化，先沿袭第2步的做法，对得出来的结果做关于Y的反归一化

铁柱：可以看到对标签值的归一化确实给我们带来了好处，迭代次数减少了一倍多。其实，咱们这个网络还很简单，这样折腾一下是为了让大家有深刻的理解，真正复杂的神经网络有个理论叫做"Batch Normalization"——批标准化，以后咱们遇到时再讲。

木头：好啊好啊！


# 高(中)数(学)回忆

### 均值mean：

$$\bar{x}=\frac{1}{n}\sum_i^nx_i$$

### 标准差stdandard deviation：

$$
std=\sqrt{\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}}
$$

### 方差variance

$$
var=\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}=s^2
$$


### 协方差covariance
$$
cov(X,Y)=\frac{1}{n-1} \sum_i^n{[(x_i-\bar{x})(y_i-\bar{y})]}
$$
结果为正，表示X,Y是正相关。

# 数据预处理

木头：老师，归一化原来这么重要，在理论上是怎么解释呢？

铁柱：理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说：
1. 样本的各个特征的取值要符合概率分布，即[0,1]
2. 样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！

木头：是不是说，数据中地理位置的取值范围是[2,6]，而房屋面积的取值范围为[40,120]，二者相差太远，根本不可以放在一起计算了？

铁柱：是的，你看$W1*X1+W2*X2$这个式子中，如果X1的取值是[2,6]，X2的取值是[40,120]，一是相差太远，而是都不在[0,1]之间，所以对于神经网络来说很难理解。下图展示了归一化前后的情况Loss值的等高图，意思是地理位置和房屋面积取不同的值时，作为组合来计算损失函数值时，形成的类似地图的等高图。左侧为归一化前，右侧为归一化后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

木头：我还听说有一些类似的词汇，什么标准化，这个和归一化有什么区别呢？

铁柱：有三个类似的概念，归一化，标准化，中心化。

## 归一化

把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。


归一化有三种方法：
- Min-Max归一化：
$$x_{new}=(x-MinValue)/(MaxValue - MinValue) \tag{5.1}$$
- 平均值归一化： 
$$x_{new} = (x - \bar{x}) / (MaxValue - MinValue) \tag{5.2}$$

- 非线性归一化：
$$对数转换y=log(x)，反余切转换y=atan(x) \cdot 2/π  \tag{5.4}$$

## 标准化
把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。

Z-score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：
$$x_{new} = (x - \bar{x})／std \tag{5.5}$$

## 中心化

平均值为0，无标准差要求：
$$x_{new} = x - \bar{x} \tag{5.7}$$

## 神经网络中的数据归一化的意义

木头：老师，那我们在数据归一化之前，根本无法进行训练，是怎么回事儿呢？

铁柱：神经网络中，数据归一化在理论上有重要意义。

1. 数值问题

    归一化/标准化可以避免一些不必要的数值问题。输入变量的数量级未致于会引起数值问题吧，但其实要引起也并不是那么困难。因为tanh的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tanh( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。
    假如你的输入是421，你也许认为，这并不是一个太大的数，但因为有效权值大概会在1/421左右，例如0.00243，那么，在matlab里输入421x0.00243 == 0.421x2.43，会发现不相等，这就是一个数值问题。

2. 求解需要
    
    a. 初始化：
    
    在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
    
    b. 梯度：
    
    以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是误差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
    
    c. 学习率：
    
    由（2）中，知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 隐层到输出层的权值梯度可以写成 2ea，而输入层到隐层的权值梯度为 2ew(1-a^2)x ，受 x 和 w 的影响，各个梯度的数量级不相同，因此，它们需要的学习率数量级也就不相同。对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
    
    d.搜索轨迹：前面的图已解释

你可以回去看一下这个链接：https://www.jianshu.com/p/95a8f035c86c

# 正确的预测房屋价格的方法

木头：在正文中，咱们用训练出来的模型预测房屋价格，还需要先还原W和B的值，我觉得有些有点儿麻烦，有没有别的更好的办法？

铁柱：在线性问题中，我们还能够还原W和B的值，在非线性问题中，我们可能根本做不到这一点。所以，咱们可以把需要预测的数据，也先做归一化，送进模型中做推理计算。

木头：哦！对啊！因为我们训练时并没有把样本标签值做改变，所以预测的值应该还是有效的，不会脱离样本标签值的范围。

铁柱：那你来看看应该怎么做？

木头：（劈里啪啦敲键盘）...（铁柱听着机械键盘的声音皱了皱眉）...（10分钟后）......我写了段代码，您给看看：
```Python
# normalize data by specified range and min_value
def NormalizeByRange(X, x_range, x_min):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    for i in range(n):
        x = X[i,:]
        x_new = (x-x_min[0,i])/x_range[0,i]
        X_new[i,:] = x_new
    return X_new

# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)
    print("z1=", z1)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)
    print("z2=", z2)
```
我先增加了一个函数，NormalizeByRange，根据样本训练时的样本归一化方法，同样地处理一下预测样本xt，得到归一化后的xt_new，然后用训练出来的W/B的值来计算，得到z2，结果是这样的：
```
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
W_real= [[  1.99999999 -10.00000001   5.        ]]
B_real= [[110.00000012]]
z1= [[528.99999999]]
z2= [[528.99999999]]
```
可以看到z1和z2的预测值是相同的，两种方法都有效，但第二种方法看起来比较容易理解。

# 对标签值归一化

铁柱：木头不错哦！

木头：一般一般，年级第三！

铁柱：吼吼，还挺押韵。再给你提个问题，你有没有注意，在计算Loss值时，会达到172.287，337.246这样大的数值，是什么原因呢？我们第四章的Loss值可都是小于1的数。

木头：哦！啊！这个......（喝了一口82年的雪碧压压惊）......我想想啊......

铁柱：心动不如行动，想什么想，动手！

木头：吼的！......（劈里啪啦敲键盘）.......（铁柱听着那机械键盘的声音觉得闹得慌）......（20分钟后）......老师，我把PredicateTest函数改了一下：

```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B, flag):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)

    if flag == 'x_only':
        print("xt,W_real,B_real:",z1)
        print("xt_new,W,B:",z2)
    elif flag == 'x_and_y':
        print("xt,W_real,B_real:", z1*Y_range+Y_min)
        print("xt_new,W,B:", z2*Y_range+Y_min)
```
铁柱：哦？当flag=='x_and_y'时，为什么z1,z2要做这样的转换呢？

木头：flag='x_and_y'，意思是在训练时我们对Y值也做了归一化，所以，我想试一下反归一化，因为：

$$y_{new} = \frac{y-y_{min}}{y_{max}-y_{min}} = \frac{y-y_{min}}{y_{range}}$$

所以：

$$y = y_{new}*y_{range}+y_{min}$$

然后，把主程序第一行的flag值改成：
```Python
flag = 'x_and_y'
```
运行......结果如下：
```
15 900 [[ 0.0137157  -0.09154976  0.90384324]] [[0.08242129]] 0.00011344288067411854
16 0 [[ 0.01371643 -0.09154922  0.90384595]] [[0.08241911]] 0.0009021195541084537
16 100 [[ 0.01371701 -0.0915483   0.90384837]] [[0.08241596]] 0.0017657981568301397
W= [[ 0.01371698 -0.09154826  0.90384839]]
B= [[0.08241599]]
W_real= [[ 0.00457233 -0.02288706  0.01144112]]
B_real= [[-0.33402734]]
xt,W_real,B_real: [[528.99654603]]
xt_new,W,B: [[528.99670957]]
```
哈哈！82年的雪碧果然厉害！我们只迭代了16轮就结束了，而且得到了正确的预测值！

铁柱：不错不错！我们来总结一下正确的归一化和反归一化的关系：

|归一化|Weight|Bias|预测值Xt|预测方法|
|---|---|---|---|---|
|只归一化X|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$|
|只归一化X|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$|
|同时归一化X和Y|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$<br>$Z'=Z * Y_{range} + Y_{min}$|
|同时归一化X和Y|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$ <br> $Z'=Z * Y_{range}+Y_{min}$|

木头：哦！这样一总结，就可以一目了然啦！我也简单总结一下：

1. X必须归一化，否则无法训练
2. 训练出的结果W和B，在推理时有两种使用方式：
a. 直接使用，此时必须把推理时输入的X也做相同规则的归一化
b. 反归一化为W,B的本来值$W_{Real},B_{Real}$，推理时输入的X不需要改动
3. Y可以归一化，好处是迭代次数少。如果结果收敛，也可以不归一化，如果不收敛（数值过大），就必须归一化
4. 如果Y归一化，先沿袭第2步的做法，对得出来的结果做关于Y的反归一化

铁柱：可以看到对标签值的归一化确实给我们带来了好处，迭代次数减少了一倍多。其实，咱们这个网络还很简单，这样折腾一下是为了让大家有深刻的理解，真正复杂的神经网络有个理论叫做"Batch Normalization"——批标准化，以后咱们遇到时再讲。

木头：好啊好啊！
