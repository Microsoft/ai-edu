Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
 
## 4.3 神经网络法

### 4.3.1 定义神经网络结构

我们是首次尝试建立神经网络，先用一个最简单的单层单点神经元：

<img src=".\Images\4\Setup.png"> 

下面，我们用这个最简的线性回归的例子，来说明神经网络中最重要的反向传播和梯度下降的概念、过程以及代码实现。

- 输入层

它在输入层只接受一个输入，经过参数w,b的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。

严格来说输入层在神经网络中并不能称为一个层。

- 权重w/b

因为是一元线性问题，所以w/b都是一个标量，我们猜想这组数据遵循线性关系：$y = x \cdot w+b$

- 输出层

输出层1个神经元，是上述预测公式的直接输出，但定义上有所变化，应该是$z = x \cdot w + b$，z是模型的预测输出，y是实际的样本标签值。

### 4.3.2 代码实现

- 前向计算

```Python
def ForwardCalculation(w,b,x):
    z = np.dot(w, x) + b
    return z
```

关于Python的函数命名规范，一般是用aa_bb这种形式，而不是AaBb的形式，这是个人习惯而已，大家自己随意。而关于变量命名规范，我个人习惯用大写X表示一个矩阵，用小写x表示一个变量或一个样本。

- 损失函数

我们用传统的均方差函数，其中，z是每一次迭代的预测输出，y是样本标签数据。我们使用所有样本参与计算，因此损失函数实际为：

$$Loss = \frac{1}{2m}\sum_{i=1}^{m}(z_i - y_i) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

**为什么使用所有样本参与计算呢？因为单个样本或少量样本的损失并不能代表广大人民群众的利益。一条直线可能正好穿过一个点，那么对于这个点来说，它的误差为0，但对于其它样本来说，误差就可能很大。**

我们暂时不需要实现这个损失函数，只是用来定义梯度下降时的求导过程。

- 反向传播

下面的代码是通过梯度下降法中的公式推导而得的。

```Python
def BackPropagation(x,y,z):
    dZ = z - y
    dB = dZ
    dW = np.dot(dZ, x)
    return dW, dB
```
dZ是中间变量，避免重复计算。dZ又可以写成delta_Z，是某一层神经网络的反向误差输入。

- 梯度更新

```Python
def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```

- 推理预测

```Python
def Inference(w,b,x):
    z = ForwardCalculation(w,b,x)
    return z
```

推理过程，实际上就是一个前向计算过程，我们把它单独拿出来，方便对外接口的设计。

- 结果显示函数

```Python
def ShowResult(X, Y, w, b, iteration):
    # draw sample data
    plt.plot(X, Y, "b.")
    # draw predication data
    PX = np.linspace(0,1,10)
    PZ = w*PX + b
    plt.plot(PX, PZ, "r")
    plt.title("Air Conditioner Power")
    plt.xlabel("Number of Servers(K)")
    plt.ylabel("Power of Air Conditioner(KW)")
    plt.show()
    print(iteration)
    print(w,b)
```

对于初学神经网络的人来说，可视化的训练过程及结果，可以极大地帮助理解神经网络的原理。

# 主程序
```Python
if __name__ == '__main__':
    # learning rate
    eta = 0.1
    # set w,b=0, you can set to others values to have a try
    #w, b = np.random.random(),np.random.random()
    w, b = 0, 0
    # create mock up data
    X, Y = ReadData()
    # count of samples
    num_example = X.shape[1]

    for i in range(num_example):
        # get x and y value for one sample
        x,y = GetSample(X,Y,i)
        # get z from x,y
        z = ForwardCalculation(w, b, x)
        # calculate gradient of w and b
        dW, dB = BackPropagation(x, y, z)
        # update w,b
        w, b = UpdateWeights(w, b, dW, dB, eta)

    ShowResult(X, Y, w, b, 1)

    result = Inference(w,b,0.346)
    print("result=", result)
```

# 运行结果

<img src="./Images/4/result.png"/>

```
w=1.716290,b=3.196841
result= [3.79067723]
```

最终我们得到了W=1.918，B=3.077，然后根据这两个值画出了上图中的红线：

$$y=1.716x+3.196$$

预测时，已知有346台服务器，先要除以1000，因为横坐标是以K(千台)服务器为单位的，代入前向计算函数，得到的结果是3.74千瓦。


### 代码位置

ch04, Level3