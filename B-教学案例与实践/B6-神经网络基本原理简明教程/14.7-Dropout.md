Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


https://blog.csdn.net/program_developer/article/details/80737724



## DropOut

DropOut的作用是在训练过程中随机舍弃掉一定比例的节点，使用剩余的节点进行训练。采用这种方法可以让不同神经元节点接收到的训练数据不同，可以有效地防止神经元因为接收到过多的同类型参数而陷入过拟合的状态

### 算法思路

   1. 给定舍弃的比例$q$
   2. 按照这个比例选出一定的神经元将其输出置为0，也就相当于从网络中删掉这一些被选出来的神经元
   3. 为了保持各层输出的值大致相等，将剩余的神经元的输出方法$\frac{1}{q}$倍
   4. 继续进行传播和训练
   5. 在测试过程中将dropOut设为无效即可

### 示例代码

```python
class Cdropout(object):
    def __init__(self, inputSize, prob):

        # 记录输出数据尺寸和保持神经元的概率

        self.outputShape = inputSize

        self.prob = prob

    def forward(self, data, train=False):

        self.data = data

        # dropout层只需要在训练过程中发挥作用

        self.train = train

        # 决定每个神经元是否要保留

        self.mask = np.random.rand(
            self.outputShape[0], self.outputShape[1]) > self.prob

        if train:

            return np.multiply(data, self.mask) / (1 - self.prob)

        else:

            return data

    def gradient(self, preError):

        if self.train:

            # 被保留的神经元的梯度会继续传递，被置零的神经元的梯度则会同样的被置成零

            return np.multiply(preError, self.mask)

        else:

            return preError
```