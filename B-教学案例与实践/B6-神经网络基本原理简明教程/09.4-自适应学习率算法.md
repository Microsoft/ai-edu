Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 自适应学习率算法

## AdaGrad

Adagrad是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。Dean 等人发现，Adagrad 法大大提升了 SGD 的鲁棒性，并在谷歌使用它训练大规模的神经网络，其诸多功能包括识别 Youtube 视频中的猫。此外，Pennington 等人使用它训练 GloVe 单词向量映射（Word Embedding），在其中不频繁出现的词语需要比频繁出现的更大的更新值。

在这之前，我们对于所有的参数使用相同的学习率进行更新。但 Adagrad 则不然，对不同的训练迭代次数 t，AdaGrad 对每个参数都有一个不同的学习率。我们首先考察 adagrad 每个参数的的更新过程，然后我们再使之向量化。

优点
学习率将随着梯度的倒数增长，也就是说较大梯度具有较小的学习率，而较小的梯度具有较大的学习率，可以解决普通的sgd方法中学习率一直不变的问题
缺点
还是需要自己手动指定初始学习率，而且由于分母中对历史梯度一直累加，学习率将逐渐下降至0，并且如果初始梯度很大的话，会导致整个训练过程的学习率一直很小，从而导致学习时间变长。


Adagrad 主要优势之一，是它不需要对每个学习率手工地调节。而大多数算法，只是简单地使用一个相同地默认值如 0.1，来避免这样地情况。


Adagrad 地主要劣势，是他在分母上的项中积累了平方梯度和。因为每次加入的项总是一个正值，所以累积的和将会随着训练过程而增大。因而，这会导致学习率不断缩小，并最终变为一个无限小值——此时，这个算法已经不能从数据中学到额外的信息。而下面的算法，则旨在解决这个问题。

### 输入和参数

- $\eta$ - 全局学习率
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-7
  
### 算法

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$
> 
> 累计平方梯度：$r_t = r_{t-1} + g_t \odot g_t$
> 
> 计算梯度更新：$\Delta \theta = {\eta \over \delta + \sqrt{r_t}} \odot g_t$
> 
> 更新参数：$\theta_t=\theta_{t-1} - \Delta \theta$

## AlaDelta

Adadelta 法 [6] 是 Adagrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，Adadelta 法仅计算在一个大小为 的时间区间内梯度值的累积和。
但该方法并不会存储之前 个梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：它被定义为关于过去梯度值的衰减均值（decade average），当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。



## RMSProp - Root Mean Square Prop 

均方根弹性反向传播。

RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。
RMSprop 法和 Adadelta 法几乎同时被发展出来。他们 解决 Adagrad 激进的学习率缩减问题。实际上，RMSprop 和我们推导出的 Adadelta 法第一个更规则相同：

### 输入和参数

- $\eta$ - 全局学习率
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-8
- $\beta$ - 衰减速率，建议缺省取值0.9
- $r$ - 累积变量矩阵，与$\theta$尺寸相同，初始化为0
  
### 算法

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

> 累计平方梯度：$r = \beta \cdot r + (1-\beta)(g_t \odot g_t)$

> 计算梯度更新：$\Delta \theta = {\eta \over \sqrt{r + \delta}} \odot g_t$

> 更新参数：$\theta_{t}=\theta_{t-1} - \Delta \theta$

RMSprop 也将学习率除以了一个指数衰减的衰减均值。Hinton 建议设定 为 0.9，对 而言，0.001 是一个较好的默认值。

为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重 WW 和偏置 bb 的梯度使用了微分平方加权平均数。 
其中，假设在第 tt 轮迭代过程中，各个公式如下所示： 

 
算法的主要思想就用上面的公式表达完毕了。在上面的公式中sdwsdw和sdbsdb分别是损失函数在前t−1t−1轮迭代过程中累积的梯度梯度动量，ββ 是梯度累积的一个指数。所不同的是，RMSProp算法对梯度计算了微分平方加权平均数。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。（比如当 dWdW 或者 dbdb 中有一个值比较大的时候，那么我们在更新权重或者偏置的时候除以它之前累积的梯度的平方根，这样就可以使得更新幅度变小）。为了防止分母为零，使用了一个很小的数值 ϵϵ 来进行平滑，一般取值为10−8。


## Adam - Adaptive Moment Estimation

### 输入和参数

- t - 当前迭代次数
- $\eta$ - 全局学习率，建议缺省值为0.001
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-8
- $\beta_1, \beta_2$ - 矩估计的指数衰减速率，$\in[0,1)$，建议缺省值为0.9和0.999

### 算法

>计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

>计数器加一：$t=t+1$

>更新有偏一阶矩估计：$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$

>更新有偏二阶矩估计：$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2)(g_t \odot g_t)$

>修正一阶矩的偏差：$\hat m_t = m_t / (1-\beta_1^t)$

>修正二阶矩的偏差：$\hat v_t = v_t / (1-\beta_2^t)$

>计算梯度更新：$\Delta \theta = \eta \cdot \hat m_t /(\delta + \sqrt{\hat v_t})$

>更新参数：$\theta_t=\theta_{t-1} - \Delta \theta$

<img src=".\Images\9\1.gif">
a
<img src=".\Images\9\2.gif">