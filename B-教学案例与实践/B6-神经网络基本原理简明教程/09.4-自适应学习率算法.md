Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 自适应学习率算法

## AdaGrad

Adagrad[3] 是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。Dean 等人 [14] 发现，Adagrad 法大大提升了 SGD 的鲁棒性，并在谷歌使用它训练大规模的神经网络，其诸多功能包括识别 Youtube 视频中的猫。此外，Pennington 等人 [5] 使用它训练 GloVe 单词向量映射（Word Embedding），在其中不频繁出现的词语需要比频繁出现的更大的更新值。

在这之前，我们对于所有的参数使用相同的学习率进行更新。但 Adagrad 则不然，对不同的训练迭代次数 t，adagrad 对每个参数都有一个不同的学习率。我们首先考察 adagrad 每个参数的的更新过程，然后我们再使之向量化。

Adagrad 主要优势之一，是它不需要对每个学习率手工地调节。而大多数算法，只是简单地使用一个相同地默认值如 0.1，来避免这样地情况。

Adagrad 地主要劣势，是他在分母上的项中积累了平方梯度和。因为每次加入的项总是一个正值，所以累积的和将会随着训练过程而增大。因而，这会导致学习率不断缩小，并最终变为一个无限小值——此时，这个算法已经不能从数据中学到额外的信息。而下面的算法，则旨在解决这个问题。

$$\delta=1e-7$$
$$r = r + (\nabla\theta)^2 = r + \nabla \theta \odot \nabla \theta$$
$$\alpha = {\eta \over \delta + \sqrt{r}}$$
$$\theta_{new}=\theta - \alpha \cdot \nabla \theta$$

## AlaDelta

Adadelta 法 [6] 是 Adagrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，Adadelta 法仅计算在一个大小为 的时间区间内梯度值的累积和。
但该方法并不会存储之前 个梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：它被定义为关于过去梯度值的衰减均值（decade average），当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。



## RMSProp

RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。
RMSprop 法和 Adadelta 法几乎同时被发展出来。他们 解决 Adagrad 激进的学习率缩减问题。实际上，RMSprop 和我们推导出的 Adadelta 法第一个更规则相同：

$\delta=1e-6$$$
$$r = \rho \cdot r + (1-\rho)(\nabla\theta \odot \nabla\theta)$$
$$\alpha = {\eta \over \sqrt{r+\delta}}$$
$$\theta_{new}=\theta - \alpha \cdot \nabla \theta$$

RMSprop 也将学习率除以了一个指数衰减的衰减均值。Hinton 建议设定 为 0.9，对 而言，0.001 是一个较好的默认值。

## Adam
$$\beta_1 = 0.9, \beta_2=0.999, \delta=1e-8$$
$$t=t+1$$
$$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1)g$$
$$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2)(g \odot g)$$
$$\hat m_t = {m_t \over 1-\beta_1^t}$$
$$\hat v_t = {m_t \over 1-\beta_2^t}$$
$$\alpha = \eta{\hat m_t \over \sqrt{\hat v_t} + \delta}$$
$$\theta_{new}=\theta - \alpha \cdot g$$


<img src=".\Images\9\1.gif">
a
<img src=".\Images\9\2.gif">