Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 常用名词

在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本系列文章中，使用损失函数和Loss Function这两个词汇。


**损失**就是所有样本的**误差**的总和，亦即：$$损失 = \sum^m_{i=1}(误差_i)$$所以有时候损失函数可以和误差函数混用概念。

在黑盒子的例子中，我们如果说“某个样本的损失”是不对的，只能说“某个样本的误差”，如果我们把神经网络的参数调整到完全满足一个样本的输出误差为0，通常会令其它样本的误差变得更大，这样作为误差之和的损失函数值，就会变得更大。所以，我们通常会在根据某个样本的误差调整权重后，计算一下整体样本的损失函数值，来判定网络是不是已经训练到了可接受的状态。

在上面的例子中，我们使用单个样本进行训练。在批量样本训练中，实际写code时，经常会出现的问题是，忘记用样本误差之和除以样本数量，结果得到了比实际误差大很多的数值，造成网络训练结果波动。

# 机器学习常用损失函数

**符号规则：a是预测值，y是样本标签值，Loss是损失函数值**

- Gold Standard Loss，又称0-1误差（蓝色）
$$
Loss=\begin{cases} 0 & a=y \\ 1 & a \ne y \end{cases}
$$

- 绝对值损失函数

$$
L = |y-a|
$$

- Hinge Loss，铰链/折页损失函数或最大边界损失函数（红色），主要用于SVM（支持向量机）中

$$
Loss=max(0,1-y \cdot a), y=\pm 1
$$

- Log Loss，对数损失函数，又叫交叉熵损失函数(cross entropy error)（黄色）

$$
Loss = -\frac{1}{m} \sum_i^m y_i log(a_i) + (1-y_i)log(1-a_i),  y_i \in \{0,1\}
$$

- Squared Loss，均方差损失函数（黑色）
$$
Loss=\frac{1}{2m} \sum_i^m (a_i-y_i)^2
$$

- Exponential Loss，指数损失函数（绿色）
$$
Loss = \frac{1}{m}\sum_i^m e^{-y_i \cdot a_i}
$$



<img src=".\Images\3\error_function.png"/>

以上内容参考了[这篇博客的内容。](http://kubicode.me/2016/04/11/Machine%20Learning/Say-About-Loss-Function/)

# 神经网络中常用的损失函数

## MSE 均方差函数
    
  该函数就是最直观的一个损失函数了，计算预测值和真实值之间的欧式距离。预测值和真实值越接近，两者的均方差就越小。
  
  **均方差函数常用于线性回归(linear regression)，即函数拟合(function fitting)**。
### 公式

$$
Loss=\frac{1}{2m} \sum_i^m (a_i-y_i)^2
$$

### 工作原理

要想得到预测值a与真实值y的差距，最朴素的想法就是用$Loss=a_i-y_i$。

对于单个样本来说，这样做没问题，但是多个样本累计时，$a_i-y_i$有可能有正有负，就会导致相互抵消，从而失去价值。所以有了绝对值差的想法，即$Loss=|a_i-y_i|$。

假设有三个样本的标签值是$y=[1,1,1]$：

|样本标签值|样本预测值|绝对值损失函数|均方差损失函数|
|------|------|------|------|
|$[1,1,1]$|$[1,2,3]$|$(1-1)+(2-1)+(3-1)=3$|$(1-1)^2+(2-1)^2+(3-1)^2=5$|
|$[1,1,1]$|$[1,3,3]$|$(1-1)+(3-1)+(3-1)=4$|$(1-1)^2+(3-1)^2+(3-1)^2=8$|

可以看到5比3已经大了很多，8比4大了一倍，而8比5也放大了某个样本的局部损失对全局带来的影响，用不通俗的语言说，就是“对某些偏离大的样本比较敏感”，从而引起监督训练过程的足够重视，以便差异化回传的误差。

## 交叉熵函数

**交叉熵函数常用于逻辑回归(logistic regression)，也就是分类(classification)。**

### 公式

$$
Loss = -\frac{1}{m} \sum_i^m [y_i log(a_i) + (1-y_i)log(1-a_i)],  y_i \in \{0,1\}
$$
单个样本：
$$
loss = -[y \cdot lna+(1-y) \cdot ln(1-a)]
$$
### 工作原理


当y=1时，即标签值是1，是个正例：

$$
loss = -lna
$$

此时，损失值与预测值的关系是：

<img src="./Images/3/crossentropy1.png"/>

横坐标是预测输出，纵坐标是损失函数值。y=1意味着当前样本标签值是1，当预测输出越接近1时，Loss值越小，训练结果越准确。当预测输出越接近0时，Loss值越大，训练结果越糟糕。

当y=0时，即标签值是0，是个反例：
$$
loss = -ln(1-a)
$$

此时，损失值与预测值的关系是：

<img src="./Images/3/crossentropy2.png"/>

横坐标是预测输出，纵坐标是损失函数值。y=0意味着当前样本标签值是0，当预测输出越接近0时，Loss值越小，训练结果越准确。当预测输出越接近1时，Loss值越大，训练结果越糟糕。

### 推导过程

标签值为1的概率：

$$a=P(y=1|x)$$

很明显，当前样本标签为 0 的概率就可以表达成：

$$1−a=P(y=0|x)$$

从极大似然性的角度出发，把上面两种情况整合到一起：

$$
P(y|x)=a^y⋅(1−a)^{1−y}
$$

当真实样本标签 y = 0 时，上面式子第一项就为 1，概率等式转化为：

$$P(y=0|x)=1−a$$

当真实样本标签 y = 1 时，上面式子第二项就为 1，概率等式转化为：

$$
P(y=1|x)=a
$$

两种情况下概率表达式跟之前的完全一致，只不过我们把两种情况整合在一起了。

我们希望的是概率 P(y|x) 越大越好。我们对 P(y|x) 引入 log 函数，因为 log 运算并不会影响函数本身的单调性。则有：

$$
log P(y|x)=log(a^y⋅(1−a)^{1−y})=y \cdot loga+(1−y) \cdot log(1−a)
$$

我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为：

$$
loss=-[y \cdot loga+(1−y) \cdot log(1−a)]
$$

### 交叉熵函数的使用

分类问题中，包含二分类和多分类两种情况。


#### 二分类

最后输出层使用Sigmoid激活函数，用如下交叉熵损失函数形式：

$$
loss = -[y_i \ln a_i + (1-y_i) \ln (1-a_i)]
$$


#### 多分类（类别大于等于3）

多分类：最后输出层使用Softmax激活函数，用如下交叉熵损失函数形式：
$$
loss = -y_i \ln a_i
$$

假设一共有3个类别，期望输出是y=[1.0, 0.0, 0.0]，即标签值表明当前样本的实际输出应该是第一类。

而实际的训练中，假设测输出是a1=[0.5, 0.2, 0.3]，则损失函数计算如下：

$$Loss1 = -\sum y_i \ln a1_i $$$$
-(1 \times \ln 0.5 + 0 \times \ln 0.2 + 0 \times \ln 0.3)=0.693
$$

假设测输出是a2=[0.8, 0.1, 0.1]，则损失函数计算如下：

$$Loss2 = -\sum y_i \ln a2_i $$$$
-(1 \times \ln 0.8 + 0 \times \ln 0.1 + 0 \times \ln 0.1)=0.223
$$

可以看到a2的损失函数值较小，因为0.8比0.5更接近1.0。

## 均方差和交叉熵的比较

主要是在经过Sigmoid之后的输出值，在求导后，Sigmoid本身的导数是否存在的问题。
在神经网络中，假设在损失函数之前的激活函数是Sigmoid函数：

$$
a=\frac{1}{1+e^{-z}}
$$

<img src="./Images/3/grad_MSE.png"/>

以均方差函数为损失函数，对z求导：

$$
\frac{\partial{Loss}}{\partial{z}}=\frac{\partial{Loss}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{z}}$$$$
=(a-y) \cdot a(1-a)
$$

其中，a(1-a)项是Sigmoid函数的导数，注意上图中红色的导数曲线，其最大值只有0.25，意味着从最后端传回来的误差值，最多只能以0.25倍的比例向前传，这就给反向传播的效果打了折扣。

以交叉熵函数为损失函数，对z求导：

$$
\frac{\partial{Loss}}{\partial{z}}=\frac{\partial{Loss}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{z}}$$$$
= \frac{a-y}{a(1-a)} \cdot a(1-a) = a-y
$$

比较上述两个求导结果可以得出结论：
1. **均方差函数适用于回归（函数拟合）**。当均方差函数前面有Sigmoid激活函数时，当误差很大时，比如大于5时，a(1-a)项的数值会非常小，造成了反向传播的力度很小。所以我们在做线性回归（函数拟合）时，不用Sigmoid函数做激活函数，而是直接用最后一层的线性输出作为Loss函数的输入。

2. **交叉熵函数一般用于分类**。因为交叉熵函数前面连接了激活函数时，激活函数的导数的不良性质不会影响反向传播的力度，从公式看就是只有a-y这样一项。
