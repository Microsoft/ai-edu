<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 20.1 初识LSTM

本小节中，我们将学习长短时记忆（Long Short Term Memory, LSTM）网络的基本原理。本章与前面不同的是，前几节进行原理介绍，最后一节实践。

### 20.1.1 提出问题

循环神经网络（RNN）的提出，使神经网络可以训练和解决带有时序信息的任务，大大拓宽了神经网络的使用范围。但是原始的RNN有明显的缺陷。不管是双向RNN，还是深度RNN，都有一个严重的缺陷：训练过程冲经常会出现梯度爆炸和梯度消失的问题，以至于原始的RNN很难处理长距离的依赖。

#### 从实例角度

例如，在语言生成问题中：

佳佳今天帮助妈妈洗碗，帮助爸爸修理椅子，还帮助爷爷奶奶照顾小狗毛毛，大家都夸奖了 $\underline{\qquad\quad}$。

例句中出现了很多人，空白出要填谁呢？我们知道是“佳佳”，传统RNN无法很好学习这么远距离的依赖关系。

#### 从理论角度

根据循环神经网络的反向传播算法，可以得到任意时刻k, 误差项沿时间反向传播的公式如下（看不懂没关系，后面还会解释）：
$$
\delta^T_k=\delta^T_t \prod_{i=k}^{t-1} diag[f'(z_i)]W
$$

其中 $f$为激活函数，$z_i$为神经网络在第$i$时刻的加权输入， $W$为权重矩阵，$diag$表示一个对角矩阵。

注意，其中有一个连乘项 $\prod_{i=k}^{t-1} diag[f'(z_i)]W$ , 如果激活函数是挤压型，例如 $Tanh$ 或 $sigmoid$ , 他们的导数值在 [0,1] 之间。我们再来看$W$。
1. 如果$W$的值在 (0,1) 的范围内， 则随着$t$的增大，连乘项会越来越趋近于0， 误差无法传播，这就导致了 **梯度消失** 的问题。
2. 如果$W$的值很大，使得$diag[f'(net_i)]W$的值大于$1$， 则随着$t$的增大，连乘项的值会呈指数增长，并趋向于无穷，产生 **梯度爆炸**。

梯度消失使得误差无法传递到较早的时刻，权重无法更新，网络停止学习。梯度爆炸又会使网络不稳定，梯度过大，权重变化太大，无法很好学习，最坏情况还会产生溢出（NaN）错误而无法更新权重。

#### 解决办法

为了解决这个问题，科学家们想了很多办法。

1. 采用半线性激活函数ReLU代替 挤压型激活函数，ReLU函数在定义域大于0的部分，导数恒等于1，来解决梯度消失问题。
2. 合理初始化权重$W$，使$diag[f'(net_i)]W$的值尽量趋近于1，避免梯度消失和梯度爆炸。

上面两种办法都有一定的缺陷，ReLU函数有自身的缺点，而初始化权重的策略也抵不过连乘操作带来的指数增长问题。要想根本解决问题，必须去掉连乘项。

科学家们冥思苦想，终于提出了新的模型 —— 长短时记忆网络（LSTM）。


#### 20.1.2 LSTM网络

#### 20.1.2.1 LSTM的结构

LSTM 的设计思路比较简单，原来的RNN中隐藏层只有一个状态h，对短期输入敏感，现在再增加一个状态c，来保存长期状态。这个新增状态称为 **细胞状态（cell state）**或**单元状态**。
增加细胞状态前后的网络对比图如下：

<img src="../Images/20/rnn_sketch.png" width="400" />

<img src="../Images/20/lstm_sketch.png" width="400" />

那么，如何控制长期状态c呢？在任意时刻$t$，我们需要确定三件事：

1. $t-1$时刻传入的状态$c_{t-1}$，有多少需要保留。
2. 当前时刻的输入信息，有多少需要传递到$t+1$时刻。
3. 当前时刻的隐层输出$h_t$是什么。

LSTM设计了 **门控（gate）** 结构，控制信息的保留和丢弃。LSTM有三个门，分别是：遗忘门（forget gate），输入门（input gate）和输出门（output gate）。

下图是常见的LSTM结构，我们以任意时刻t的一个LSTM单元（LSTM cell）为例，来分析其工作原理。

<img src="../Images/20/lstm_inner_structure.png" />

#### 20.1.2.2 LSTM的前向计算

1. 遗忘门

   由上图可知，遗忘门的输出为$f_t$， 采用sigmoid激活函数，将输出映射到[0,1]区间。上一时刻细胞状态$c_{t-1}$通过遗忘门时，与$ft$结果相乘，显然，乘数为0的信息被全部丢弃，为1的被全部保留。这样就决定了上一细胞状态$c_{t-1}$有多少能进入当前状态$c_t$。

   遗忘门$f_t$的公式如下：
    $$ f_t = \sigma(h_{t-1} \cdot W_f + x_t \cdot U_f + b_f) \tag{1} $$

    其中，$\sigma$为sigmoid激活函数，$h_{t-1}$ 为上一时刻的隐层状态，形状为$(1 \times h)$的行向量。$x_t$为当前时刻的输入，形状为$(1 \times i)$的行向量。参数矩阵$W_f$、$U_f$分别是$(i \times h)$和$(h \times h)$的矩阵，$b_f$为$(1 \times h)$的行向量。

   很多教科书或网络资料将公式写成如下格式：
   $$
   f_t=\sigma(W_f\cdot[h_{t-1}, x_t] + b_f) \tag{1'}
   $$
   或
   $$
   \begin{aligned}
   f_t &= \sigma([W_{fh}\;W_{fx}] \begin{bmatrix}h_{t-1}\\\\x_t \end{bmatrix}+b_f) \\
   &= \sigma(W_{fh}h_{t-1}+W_{fx}x_t+b_f) \tag{1''}
   \end{aligned}
   $$
   后两种形式将权重矩阵放在状态向量前面，在讲解原理时，与公式$(1)$没有区别，但在代码实现时会出现一些问题，所以，在本章中我们采用公式$(1)$的表达方式。


2. 输入门

   输入门$i_t$决定输入信息有哪些被保留，输入信息包含当前时刻输入和上一时刻隐层输出两部分，存入即时细胞状态$\tilde{c}_t$中。输入门依然采用sigmoid激活函数，将输出映射到[0,1]区间。$\tilde{c}_t$通过输入门时进行信息过滤。

   输入门$i_t$的公式如下：
   $$
   i_t = \sigma(h_{t-1} \cdot W_i + x_t \cdot U_i + b_i) \tag{2}
   $$

   即时细胞状态 $\tilde{c}_t$的公式如下:
   $$
   \tilde{c}_t = \tanh(h_{t-1} \cdot W_c + x_t \cdot U_c + b_c) \tag{3}
   $$

   上一时刻保留的信息，加上当前输入保留的信息，构成了当前时刻的细胞状态$c_t$。

   当前细胞状态$c_t$的公式如下：
   $$
   c_t = f_t \circ c_{t-1}+i_t \circ \tilde{c}_t \tag{4}
   $$
   其中，符号 $\cdot$ 表示矩阵乘积， $\circ$ 表示 Hadamard 乘积，即元素乘积。


3. 输出门

   最后，需要确定输出信息。

   输出门$o_t$决定 $h_{t-1}$ 和 $x_t$ 中哪些信息将被输出，公式如下：
   $$
   o_t = \sigma(h_{t-1} \cdot W_o + x_t \cdot U_o + b_o) \tag{5}
   $$

   细胞状态$c_t$通过tanh激活函数压缩到 (-1, 1) 区间，通过输出门，得到当前时刻的隐藏状态$h_t$作为输出，公式如下：
   $$
   h_t=o_t \circ \tanh(c_t) \tag{6}
   $$

最后，时刻t的预测输出为：
$$
a_t = \sigma(h_t \cdot V + b) \tag{7}
$$
其中，
$$
z_t = h_t \cdot V + b \tag{8}
$$

经过上面的步骤，LSTM就完成了当前时刻的前向计算工作。

#### 20.1.2.3 LSTM的反向传播

LSTM使用时序反向传播算法（Backpropagation Through Time, BPTT）进行计算。下图是带有一个输出的LSTM cell。我们使用该图来推导反向传播过程。

<img src="../Images/20/lstm_cell.png" />

假设当前LSTM cell处于第$l$层、$t$时刻。那么，它从两个方向接受反向传播的误差：一个是从$t$时刻$l+1$层的输入传回误差，记为$\delta^{l+1}_{x_t}$；另一个是从$t+1$时刻$l$层传回的误差，记为$\delta^{l}_{h_t}$。（注意，这里的下标不是$h_{t+1}$，而是$h_t$）。

我们先复习几个在推导过程中会使用到的激活函数，以及其导数公式。令sigmoid = $\sigma$，则：
$$
    \sigma(z) = y = \frac{1}{1+e^{-z}} \tag{9}
$$
$$
    \sigma^{'}(z) = y(1-y) \tag{10}
$$
$$
    \tanh(z) = y = \frac{e^z - e^{-z}}{e^z + e^{-z}} \tag{11}
$$
$$
    \tanh^{'}(z) = 1-y^2 \tag{12}
$$


假设某一线性函数 $z_i$ 经过Softmax函数之后的预测输出为 $\hat{y}_i$，该输出的标签值为 $y_i$，则：
$$
    softmax(z_i) = \hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^me^{z_j}} \tag{13}
$$
$$
    \frac{\partial{loss}}{\partial{z_i}} = \hat{y}_i - y_i \tag{14}
$$

从图中可知，从上层传回的误差为输出层$z_t$向$h^l_t$传回的误差，假设输出层的激活函数为softmax函数，输出层标签值为$y$，则：
$$
\delta^{l+1}_{x_t} = \frac{\partial{loss}}{\partial{z_t}} \cdot \frac{\partial{z_t}}{\partial{h^l_t}} = (a - y) \cdot V \tag{15}
$$
从$t+1$时刻传回的误差为$\delta^{l}_{h_t}$，若$t$为时序的最后一个时间点，则$\delta^{l}_{h_t}=0$。

该cell的隐层$h^l_t$的最终误差为两项误差之和，即：
$$
\delta^l_t = \frac{\partial{loss}}{\partial{h_t}} = \delta^{l+1}_{x_t} + \delta^{l}_{h_t} = (a - y) \cdot V \tag{16}
$$

接下来的推导过程仅与本层相关，为了方便推导，我们忽略层次信息，令$\delta^l_t = \delta_t$。

可以求得各个门结构加权输入的误差，如下：

$$
\begin{aligned}
\delta_{z_{ot}} &= \frac{\partial{loss}}{\partial{z_{o_t}}} = \frac{\partial{loss}}{\partial{h_t}} \cdot \frac{\partial{h_t}}{\partial{o_t}} \cdot \frac{\partial{o_t}}{\partial{z_{o_t}}} \\ &= \delta_t \cdot diag[\tanh(c_t)] \cdot diag[o_t \circ (1 - o_t)] \\
&= \delta_t \circ \tanh(c_t) \circ o_t \circ (1 - o_t) 
\end{aligned}
\tag{17}
$$

$$
\delta_{c_t} = \frac{\partial{loss}}{\partial{c_t}} = \frac{\partial{loss}}{\partial{h_t}} \cdot \frac{\partial{h_t}}{\partial{\tanh(c_t)}} \cdot \frac{\partial{\tanh(c_t)}}{\partial{c_t}} = \delta_t \cdot diag[o_t] \cdot diag[1-\tanh^2(c_t)] \\
= \delta_t \circ o_t \circ (1-\tanh^2(c_t)) \tag{18}
$$

$$
\delta_{z_{\tilde{c}t}} = \frac{\partial{loss}}{\partial{z_{\tilde{c}_t}}} = \frac{\partial{loss}}{\partial{c_t}} \cdot \frac{\partial{c_t}}{\partial{\tilde{c}_t}} \cdot \frac{\partial{\tilde{c}_t}}{\partial{z_{\tilde{c}_t}}} = \delta_{c_t} \cdot diag[i_t] \cdot diag[1-\tanh^2(\tilde{c}_t)] \\
= \delta_{c_t} \circ i_t \circ (1-\tanh^2(\tilde{c}_t)) \tag{19}
$$

$$
\delta_{z_{it}} = \frac{\partial{loss}}{\partial{z_{i_t}}} = \frac{\partial{loss}}{\partial{c_t}} \cdot \frac{\partial{c_t}}{\partial{i_t}} \cdot \frac{\partial{i_t}}{\partial{z_{i_t}}} = \delta_{c_t} \cdot diag[\tilde{c}_t] \cdot diag[i_t \circ (1 - i_t)] \\
= \delta_{c_t} \circ \tilde{c}_t \circ i_t \circ (1 - i_t) \tag{20}
$$

$$
\delta_{z_{ft}} = \frac{\partial{loss}}{\partial{z_{f_t}}} = \frac{\partial{loss}}{\partial{c_t}} \cdot \frac{\partial{c_t}}{\partial{f_t}} \cdot \frac{\partial{f_t}}{\partial{z_{f_t}}} = \delta_{c_t} \cdot diag[c_{t-1}] \cdot diag[f_t \circ (1 - f_t)] \\
= \delta_{c_t} \circ c_{t-1} \circ f_t \circ (1 - f_t) \tag{21}
$$

于是，在$t$时刻，输出层参数的各项误差为：
$$
d_{W_{o,t}} = \frac{\partial{loss}}{\partial{W_{o,t}}} = \frac{\partial{loss}}{\partial{z_{o_t}}} \cdot \frac{\partial{z_{o_t}}}{\partial{W_o}} = h^T_{t-1} \cdot \delta_{z_{ot}}
$$
$$
d_{U_{o,t}} = \frac{\partial{loss}}{\partial{U_{o,t}}} = \frac{\partial{loss}}{\partial{z_{o_t}}} \cdot \frac{\partial{z_{o_t}}}{\partial{U_o}} = x^T_t \cdot \delta_{z_{ot}}
$$
$$
d_{b_{o,t}} = \frac{\partial{loss}}{\partial{b_{o,t}}} = \frac{\partial{loss}}{\partial{z_{o_t}}} \cdot \frac{\partial{z_{o_t}}}{\partial{b_o}} = \delta_{z_{ot}}
$$

最终误差为各时刻误差之和，则：
$$
d_{W_o} = \sum^\tau_{t=1}d_{W_{o,t}} = \sum^\tau_{t=1}h^T_{t-1} \cdot \delta_{z_{ot}}
$$
$$
d_{U_o} = \sum^\tau_{t=1}d_{U_{o,t}} = \sum^\tau_{t=1}x^T_t \cdot \delta_{z_{ot}}
$$
$$
d_{b_o} = \sum^\tau_{t=1}d_{b_{o,t}} = \sum^\tau_{t=1}\delta_{z_{ot}}
$$


同理可得：
$$
d_{W_{c}} = \sum^\tau_{t=1}d_{W_{c,t}} = \sum^\tau_{t=1}h^T_{t-1} \cdot \delta_{z_{\tilde{c}t}}
$$
$$
d_{U_{c}} = \sum^\tau_{t=1}d_{U_{c,t}} = \sum^\tau_{t=1}x^T_t \cdot \delta_{z_{\tilde{c}t}}
$$
$$
d_{b_{c}} = \sum^\tau_{t=1}d_{b_{c,t}} = \sum^\tau_{t=1}\delta_{z_{\tilde{c}t}}
$$

$$
d_{W_{i}} = \sum^\tau_{t=1}d_{W_{i,t}} = \sum^\tau_{t=1}h^T_{t-1} \cdot \delta_{z_{it}}
$$
$$
d_{U_{i}} = \sum^\tau_{t=1}d_{U_{i,t}} = \sum^\tau_{t=1}x^T_t \cdot \delta_{z_{it}}
$$
$$
d_{b_{i}} = \sum^\tau_{t=1}d_{b_{i,t}} =\sum^\tau_{t=1}\delta_{z_{it}}
$$

$$
d_{W_{f}} = \sum^\tau_{t=1}d_{W_{f,t}} = \sum^\tau_{t=1}h^T_{t-1} \cdot \delta_{z_{ft}}
$$
$$
d_{U_{f}} = \sum^\tau_{t=1}d_{U_{f,t}} = \sum^\tau_{t=1}x^T_t \cdot \delta_{z_{ft}}
$$
$$
d_{b_{f}} = \sum^\tau_{t=1}d_{b_{f,t}} = \sum^\tau_{t=1}\delta_{z_{ft}}
$$

当前LSTM cell分别向前一时刻（$t-1$）和下一层（$l-1$）传递误差，公式如下：

沿时间向前传递：

$$
\delta_{h_{t-1}} = \frac{\partial{loss}}{\partial{h_{t-1}}} = \frac{\partial{loss}}{\partial{z_{ft}}} \cdot \frac{\partial{z_{ft}}}{\partial{h_{t-1}}} + \frac{\partial{loss}}{\partial{z_{it}}} \cdot \frac{\partial{z_{it}}}{\partial{h_{t-1}}} + \frac{\partial{loss}}{\partial{z_{\tilde{c}t}}} \cdot \frac{\partial{z_{\tilde{c}t}}}{\partial{h_{t-1}}} + \frac{\partial{loss}}{\partial{z_{ot}}} \cdot \frac{\partial{z_{ot}}}{\partial{h_{t-1}}} \\
= \delta_{z_{ft}} \cdot W_f^T + \delta_{z_{it}} \cdot W_i^T + \delta_{z_{\tilde{c}t}} \cdot W_c^T + \delta_{z_{ot}} \cdot W_o^T
$$

沿层次向下传递：

$$
\delta_{x_t} = \frac{\partial{loss}}{\partial{x_t}} = \frac{\partial{loss}}{\partial{z_{ft}}} \cdot \frac{\partial{z_{ft}}}{\partial{x_t}} + \frac{\partial{loss}}{\partial{z_{it}}} \cdot \frac{\partial{z_{it}}}{\partial{x_t}} + \frac{\partial{loss}}{\partial{z_{\tilde{c}t}}} \cdot \frac{\partial{z_{\tilde{c}t}}}{\partial{x_t}} + \frac{\partial{loss}}{\partial{z_{ot}}} \cdot \frac{\partial{z_{ot}}}{\partial{x_t}} \\
= \delta_{z_{ft}} \cdot U_f^T + \delta_{z_{it}} \cdot U_i^T + \delta_{z_{\tilde{c}t}} \cdot U_c^T + \delta_{z_{ot}} \cdot U_o^T
$$

以上，LSTM反向传播公式推导完毕。

### 20.1.3 LSTM单元的代码实现

下面是单个LSTM Cell实现的代码。

#### 初始化

初始化时需要告知LSTM Cell 输入向量的维度和隐层状态向量的维度，分别为$input\_size$和$hidden\_size$。

```Python
def __init__(self, input_size, hidden_size, bias=True):
            self.input_size = input_size
            self.hidden_size = hidden_size
            self.bias = bias
```

#### 前向计算

```Python
def forward(self, x, h_p, c_p, W, U, b=None):
    self.get_params(W, U, b)
    self.x = x

    # caclulate each gate
    # use g instead of \tilde{c}
    self.f = self.get_gate(x, h_p, self.wf, self.uf, self.bf, Sigmoid())
    self.i = self.get_gate(x, h_p, self.wi, self.ui, self.bi, Sigmoid())
    self.g = self.get_gate(x, h_p, self.wg, self.ug, self.bg, Tanh())
    self.o = self.get_gate(x, h_p, self.wo, self.uo, self.bo, Sigmoid())
    # calculate the states
    self.c = np.multiply(self.f, c_p) + np.multiply(self.i, self.g)
    self.h = np.multiply(self.o, Tanh().forward(self.c))
```

其中，$get\_params$将传入参数拆分，每个门使用一个独立参数。$get\_gate$实现每个门的前向计算公式。
```Python
def get_params(self, W, U, b=None):
            self.wf, self.wi, self.wg, self.wo = self.split_params(W, self.hidden_size)
            self.uf, self.ui, self.ug, self.uo = self.split_params(U, self.input_size)
            self.bf, self.bi, self.bg, self.bo = self.split_params((b if self.bias else np.zeros((4, self.hidden_size))) , 1)
```
```Python
def get_gate(self, x, h, W, U, b, activator):
    if self.bias:
        z = np.dot(h, W) + np.dot(x, U) + b
    else:
        z = np.dot(h, W) + np.dot(x, U)
    a = activator.forward(z)
    return a
```

#### 反向传播

反向传播过程分为沿时间传播和沿层次传播两部分。$dh$将误差传递给前一个时刻，$dx$将误差传向下一层。

```Python
def backward(self, h_p, c_p, in_grad):
        tanh = lambda x : Tanh().forward(x)

        self.dzo = in_grad * tanh(self.c) * self.o * (1 - self.o)
        self.dc = in_grad * self.o * (1 - tanh(self.c) * tanh(self.c))
        self.dzg = self.dc * self.i * (1- tanh(self.g) * tanh(self.g))
        self.dzi = self.dc * self.g * self.i * (1 - self.i)
        self.dzf = self.dc * c_p * self.f * (1 - self.f)

        self.dwo = np.dot(h_p.T, self.dzo)
        self.dwg = np.dot(h_p.T, self.dzg)
        self.dwi = np.dot(h_p.T, self.dzi)
        self.dwf = np.dot(h_p.T, self.dzf)

        self.duo = np.dot(self.x.T, self.dzo)
        self.dug = np.dot(self.x.T, self.dzg)
        self.dui = np.dot(self.x.T, self.dzi)
        self.duf = np.dot(self.x.T, self.dzf)

        if self.bias:
            self.dbo = np.sum(self.dzo,axis=0, keepdims=True)
            self.dbg = np.sum(self.dzg,axis=0, keepdims=True)
            self.dbi = np.sum(self.dzi,axis=0, keepdims=True)
            self.dbf = np.sum(self.dzf,axis=0, keepdims=True)

        # pass to previous time step
        self.dh = np.dot(self.dzf, self.wf.T) + np.dot(self.dzi, self.wi.T) + np.dot(self.dzg, self.wg.T) + np.dot(self.dzo, self.wo.T)
        # pass to previous layer
        self.dx = np.dot(self.dzf, self.uf.T) + np.dot(self.dzi, self.ui.T) + np.dot(self.dzg, self.ug.T) + np.dot(self.dzo, self.uo.T)
```

最后，我们将所有拆分的参数merge到一起，便于更新梯度。
```Python
def merge_params(self):
        self.dW = np.concatenate((self.dwf, self.dwi, self.dwg, self.dwo), axis=0)
        self.dU = np.concatenate((self.duf, self.dui, self.dug, self.duo), axis=0)
        if self.bias:
            self.db = np.concatenate((self.dbf, self.dbi, self.dbg, self.dbo), axis=0)
```
以上，完成了LSTM Cell的代码实现。

通常，LSTM的输出会接一个线性层，得到最终预测输出，即公式$(7)$和$(8)$的内容。

下面是线性单元的实现代码：

```Python
class LinearCell_1_2(object):
    def __init__(self, input_size, output_size, activator=None, bias=True):
        self.input_size = input_size
        self.output_size = output_size
        self.bias = bias
        self.activator = activator

    def forward(self, x, V, b=None):
        self.x = x
        self.batch_size = self.x.shape[0]
        self.V = V
        self.b = b if self.bias else np.zeros((self.output_size))
        self.z = np.dot(x, V) + b
        if self.activator:
            self.a = self.activator.forward(self.z)

    def backward(self, in_grad):
        self.dz = in_grad
        self.dV = np.dot(self.x.T, self.dz)
        if self.bias:
            # in the sake of backward in batch
            self.db = np.sum(self.dz, axis=0, keepdims=True)
        self.dx = np.dot(self.dz, self.V.T)
```

### 20.1.4 训练网络的代码实现

我们以前面讲过的4位二进制减法为例，验证LSTM网络的正确性。

该实例需要4个时间步（time steps），我们搭建一个含有4个LSTM单元的单层网络，连接一个线性层，提供最终预测输出。网络结构如图所示：

<img src="../Images/20/lstm_binraryminus_structure.png" />

网络初始化，前向计算，反向传播的代码如下：
```Python
class net(object):
    def __init__(self, dr, input_size, hidden_size, output_size, bias=True):
        self.dr = dr
        self.loss_fun = LossFunction_1_1(NetType.BinaryClassifier)
        self.loss_trace = TrainingHistory_3_0()
        self.times = 4
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.bias = bias
        self.lstmcell = []
        self.linearcell = []
        #self.a = []
        for i in range(self.times):
            self.lstmcell.append(LSTMCell_1_2(input_size, hidden_size, bias=bias))
            self.linearcell.append(LinearCell_1_2(hidden_size, output_size, Logistic(), bias=bias))
            #self.a.append((1, self.output_size))

    def forward(self, X):
        hp = np.zeros((1, self.hidden_size))
        cp = np.zeros((1, self.hidden_size))
        for i in range(self.times):
            self.lstmcell[i].forward(X[:,i], hp, cp, self.W, self.U, self.bh)
            hp = self.lstmcell[i].h
            cp = self.lstmcell[i].c
            self.linearcell[i].forward(hp, self.V, self.b)
            #self.a[i] = Logistic().forward(self.linearcell[i].z)
```
在反向传播的过程中，不同时间步，误差的来源不同。最后的时间步，传入误差只来自输出层的误差$dx$。其他时间步的误差来自于两个方向$dh$和$dx$（时间和层次）。第一个时间步，传入的状态$h0$，$c0$皆为0。

```Python
    def backward(self, Y):
        hp = []
        cp = []
        # The last time step:
        tl = self.times-1
        dz = self.linearcell[tl].a - Y[:,tl:tl+1]
        self.linearcell[tl].backward(dz)
        hp = self.lstmcell[tl-1].h
        cp = self.lstmcell[tl-1].c
        self.lstmcell[tl].backward(hp, cp, self.linearcell[tl].dx)
        # Middle time steps:
        dh = []
        for i in range(tl-1, 0, -1):
            dz = self.linearcell[i].a - Y[:,i:i+1]
            self.linearcell[i].backward(dz)
            hp = self.lstmcell[i-1].h
            cp = self.lstmcell[i-1].c
            dh = self.linearcell[i].dx + self.lstmcell[i+1].dh
            self.lstmcell[i].backward(hp, cp, dh)
        # The first time step:
        dz = self.linearcell[0].a - Y[:,0:1]
        self.linearcell[0].backward(dz)
        dh = self.linearcell[0].dx + self.lstmcell[1].dh
        self.lstmcell[0].backward(np.zeros((self.batch_size, self.hidden_size)), np.zeros((self.batch_size, self.hidden_size)), dh)
```

下面就可以开始训练了，训练部分主要分为：初始化参数，训练网络，更新参数，计算误差几个部分。主要代码如下：

```Python
def train(self, batch_size, checkpoint=0.1):
    self.batch_size = batch_size
    max_epoch = 100
    eta = 0.1
    # Try different initialize method
    #self.U = np.random.random((4 * self.input_size, self.hidden_size))
    #self.W = np.random.random((4 * self.hidden_size, self.hidden_size))
    self.U = self.init_params_uniform((4 * self.input_size, self.hidden_size))
    self.W = self.init_params_uniform((4 * self.hidden_size, self.hidden_size))
    self.V = np.random.random((self.hidden_size, self.output_size))
    self.bh = np.zeros((4, self.hidden_size))
    self.b = np.zeros((self.output_size))

    max_iteration = math.ceil(self.dr.num_train/batch_size)
    checkpoint_iteration = (int)(math.ceil(max_iteration * checkpoint))

    for epoch in range(max_epoch):
        self.dr.Shuffle()
        for iteration in range(max_iteration):
            # get data
            batch_x, batch_y = self.dr.GetBatchTrainSamples(batch_size, iteration)
            # forward
            self.forward(batch_x)
            self.backward(batch_y)
            # update
            for i in range(self.times):
                self.lstmcell[i].merge_params()
                self.U = self.U - self.lstmcell[i].dU * eta /self.batch_size
                self.W = self.W - self.lstmcell[i].dW * eta /self.batch_size
                self.V = self.V - self.linearcell[i].dV * eta /self.batch_size
                if self.bias:
                    self.bh = self.bh - self.lstmcell[i].db * eta /self.batch_size
                    self.b = self.b - self.linearcell[i].db * eta /self.batch_size
            # check loss
            total_iteration = epoch * max_iteration + iteration
            if (total_iteration+1) % checkpoint_iteration == 0:
                X,Y = self.dr.GetValidationSet()
                loss,acc,_ = self.check_loss(X,Y)
                self.loss_trace.Add(epoch, total_iteration, None, None, loss, acc, None)
                print(epoch, total_iteration)
                print(str.format("loss={0:6f}, acc={1:6f}", loss, acc))
            #end if
        #enf for
        if (acc == 1.0):
            break
    #end for
    self.loss_trace.ShowLossHistory("Loss and Accuracy", XCoordinate.Iteration)
```

### 20.1.5 最终结果

下图展示了训练过程，以及loss和accuracy的曲线变化。
<img src="../Images/20/loss_acc_binaryminus_using_lstm.png">

该模型在验证集上可得100%的正确率。随机测试样例预测值与真实值完全一致。网络正确性得到验证。
```
  x1: [1, 1, 0, 1]
- x2: [1, 0, 0, 1]
------------------
true: [0, 1, 0, 0]
pred: [0, 1, 0, 0]
13 - 9 = 4
====================
  x1: [1, 0, 0, 0]
- x2: [0, 1, 0, 1]
------------------
true: [0, 0, 1, 1]
pred: [0, 0, 1, 1]
8 - 5 = 3
====================
  x1: [1, 1, 0, 0]
- x2: [1, 0, 0, 1]
------------------
true: [0, 0, 1, 1]
pred: [0, 0, 1, 1]
12 - 9 = 3
```

### 代码位置

ch20, Level1

### 思考和练习

1. 本例中都使用了bias参数，如果去掉bias（即bh，b皆为0）效果如何。
2. 用不同的方式初始化权重W，U，比较不同的结果。
3. 使用不同batch_size，比较不同结果。




