<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.7 双向循环神经网络

### 19.7.1 深度循环神经网络的结构图

前面学习的内容，都是因为“过去”的时间步的状态对“未来”的时间步的状态有影响，才产生了不同种类的循环神经网络结构，在本节中，我们将学习一种双向影响的结构，即双向循环神经网络。

比如在一个语音识别的模型中，可能前面的一个词听上去比较模糊，会产生多个猜测，但是后面的词都很清晰，于是可以用后面的词来为前面的词提供一个最有把握（概率最大）的猜测。

<img src="../Images/19/bi_rnn_net.png"/>

图一：双向循环神经网络结构图

在图中，用h1、s1表示正向计算的状态，h2、s2表示反向计算的状态。而U、V、W三个参数矩阵也有就对应的变化，从1个变成了1对，并不共享参数。

另外请注意，图 任然是一个有向无环图，仔细沿着每一个箭头搜索，会发现并不会产生首尾相连的循环状态，这里的“循环”与循环神经网络中的“循环”的含义不同。

所以在本节中会出现两组相似的词：前向、反向、正向、逆向。区别如下：

- 前向：是指神经网络中通常所说的前向计算。
- 反向：是指神经网络中通常所说的反向传播。
- 正向：是指双向循环神经网络中的从左到右时间步。在正向过程中，会存在前向计算和反向传播。
- 逆向：是指双向循环神经网络中的从右到左时间步。在逆向过程中，也会存在前向计算和反向传播。

### 19.7.2 前向计算

我们先假设应用场景只需要在最后一个时间步有输出（比如19.4节和19.5节中的应用就是如此），所以t2所代表的所有中间步都没有a、loss、y三个节点（用空心的圆表示），只有最后一个时间步有输出。

与前面的单向循环网络不同的是，由于有逆向网络的存在，在逆向过程中，t3是第一个时间步，t1是最后一个时间步，所以t1也应该有输出。

#### 公式推导

$$
h1 = x \cdot U1 + s1_{t-1} \cdot W1 \tag{1}
$$

注意公式1在t1时，$s1_{t-1}$是空，所以加法的第二项不存在。

$$
s1 = Tanh(h1) \tag{2}
$$

$$
h2 = x \cdot U2 + s2_{t+1} \cdot W2 \tag{3}
$$

注意公式3在t3时，$s2_{t+1}$是空，所以加法的第二项不存在。

$$
s2 = Tanh(h2) \tag{4}
$$

$$
z = s1 \cdot V1 + s2 \cdot V2 \tag{5}
$$

$$
a = Softmax(z) \tag{6}
$$

公式6只对t1、t3有效，对于所有其他中间的时间步来说，不需要计算。

#### 代码实现

由于是双向的，所以在主过程中，存在一正一反两个计算链。

```Python
class timestep(object):
    def forward_f2e(self, x, U1, U2, V1, V2, W1, W2, prev_s1, isFirst):
        ......
        # 公式1
        if (isFirst):
            self.h1 = np.dot(x, U1)
        else:
            self.h1 = np.dot(x, U1) + np.dot(prev_s1, W1) 
        # 公式2
        self.s1 = Tanh().forward(self.h1)

    def forward_e2f(self, next_s2, isFirst, isLast):
        # 公式3
        if (isLast):
            self.h2 = np.dot(self.x, self.U2)
        else:
            self.h2 = np.dot(self.x, self.U2) + np.dot(next_s2, self.W2)
        # 公式4
        self.s2 = Tanh().forward(self.h2)

        if (isFirst or isLast):
            # 公式5
            self.z = np.dot(self.s1, self.V1) + np.dot(self.s2, self.V2)
            # 公式6
            self.a = Softmax().forward(self.z)
```
正向的前向计算函数叫做forward_f2e，意为front to end，即从前向后的方向；逆向的前向计算函数叫做forward_e2f，意为end to front，即从后向前的方向。

注意对输出部分的计算，即节点z和a，只有在第一个和最后一个节点才发生（本应用场景的需要），而且只能在forward_e2f函数里才能出现，因为此时s1和s2的值都计算好了，而在forward_f2e函数中，s2的值并没有计算好。

### 19.7.3 反向传播

#### 正向计算链的反向传播

先推导正向计算链的反向传播公式，即关于h1、s1节点的计算。

对于最后一个时间步（t3）：

$$
\frac{\partial loss}{\partial z}=a-y \rightarrow dz \tag{7}
$$

$$
\frac{\partial loss}{\partial h1}=\frac{\partial loss}{\partial z}\frac{\partial z}{\partial s1}\frac{\partial s1}{\partial h1}=dz\cdot V1^T \odot \sigma'(s1) \rightarrow dh1 \tag{8}
$$

其中$\sigma(s1)'$表示激活函数的导数，$s1$是激活函数的数值。下同$。

对于中间的所有时间步：

$$
\begin{aligned}
\frac{\partial J}{\partial h1_t} &= \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial s1_t}\frac{\partial s1_t}{\partial h1_t} + \frac{\partial loss_{t+1}}{\partial h1_{t+1}}\frac{\partial h1_{t+1}}{\partial s1_{t}}\frac{\partial s1_t}{\partial h1_t}
\\
&=dz_t \cdot V1^T \odot \sigma'(s1_t) + \frac{\partial loss_{t+1}}{\partial h1_{t+1}} \cdot W1^T \odot \sigma'(s1_t)
\\
&=(dz_t \cdot V1^T + dh1_{t+1} \cdot W1^T) \odot \sigma'(s1_t) \rightarrow dh1_t
\end{aligned} \tag{9}
$$

为什么

$$
\frac{\partial loss}{\partial V1} = \frac{\partial loss}{\partial z}\frac{\partial z}{\partial V1}= s1^T \cdot dz \rightarrow dV1 \tag{10}
$$

$$
\frac{\partial loss}{\partial U1} = \frac{\partial loss}{\partial h1}\frac{\partial h1}{\partial U1}= x^T \cdot dh1 \rightarrow dU1 \tag{11}
$$

$$
dW1 = 0 \tag{16}
$$

$$
\frac{\partial loss}{\partial W1} = \frac{\partial loss}{\partial h1}\frac{\partial h1}{\partial W1}= s1_{t-1}^T \cdot dh1 \rightarrow dW1 \tag{17}
$$


公式9中的$dh1_{t+1}$，就是上一步中计算得到的$dh1_t$，如果追溯到最开始，即公式8中的$dh1$。因此，先有最后一个时间步的$dh1$，然后依次向前推，就可以得到所有时间步的$dh1_t$。

#### 逆向计算链的反向传播

下面推导反向计算链的反向传播公式，即关于h2、s2节点的计算。

对于第一个时间步（t1，实际上是逆向计算的最后一个时间步）：

$$
\frac{\partial loss}{\partial h2}=\frac{\partial loss}{\partial z}\frac{\partial z}{\partial s2}\frac{\partial s2}{\partial h2}=dz\cdot (V2)^T \odot \sigma'(s2) \rightarrow dh2 \tag{10}
$$

对于中间的所有时间步：

$$
\begin{aligned}
\frac{\partial J}{\partial h2_t} &= \frac{\partial loss_t}{\partial z_t}\frac{\partial z_t}{\partial s2_t}\frac{\partial s2_t}{\partial h2_t} + \frac{\partial loss_{t-1}}{\partial h2_{t-1}}\frac{\partial h2_{t-1}}{\partial s2_{t}}\frac{\partial s2_t}{\partial h2_t}
\\
&=dz_t \cdot V2^T \odot \sigma'(s2_t) + \frac{\partial loss_{t-1}}{\partial h2_{t-1}} \cdot W2^T \odot \sigma'(s2_t)
\\
&=(dz_t \cdot V2^T + dh2_{t-1} \cdot W2^T) \odot \sigma'(s2_t) \rightarrow dh2_t
\end{aligned} \tag{11}
$$


$$
\frac{\partial loss}{\partial V2} = \frac{\partial loss}{\partial z}\frac{\partial z}{\partial V2}= s2^T \cdot dz \rightarrow dV2 \tag{13}
$$



$$
\frac{\partial loss}{\partial U2} = \frac{\partial loss}{\partial h2}\frac{\partial h2}{\partial U2}= x^T \cdot dh2 \rightarrow dU2 \tag{15}
$$

对于第一个时间步：


$$
\frac{\partial loss}{\partial W2} = \frac{\partial loss}{\partial h2}\frac{\partial h2}{\partial W2}= s2_{t+1}^T \cdot dh2 \rightarrow dW2 \tag{18}
$$

对于最后一个时间步：

$$
dW2 = 0 \tag{19}
$$



<img src="../Images/19/birnn_loss.png"/>
<img src="../Images/19/birnn_result.png"/>

```
......
9998:11058894:0.001 loss=0.969669, acc=0.685000
9999:11060000:0.001 loss=0.946195, acc=0.702000
save last parameters...
correctness=3114/4400=0.7077272727272728
```