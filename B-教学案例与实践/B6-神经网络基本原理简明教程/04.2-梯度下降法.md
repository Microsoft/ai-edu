Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 4.2 梯度下降法

有了上一节的最小二乘法做基准，我们这次用梯度下降法求解w和b，从而可以比较二者的结果。

### 4.2.1 数学原理

- 预设函数（Hypothesis Function）为：

$$z_i = x_i \cdot w + b \tag{1}$$

- 损失函数（Loss Function）为：

$$J(w,b) = \frac{1}{2} (z_i-y_i)^2 \tag{2}$$

z是预测值，y是样本标签值。

- 求w的梯度

我们用J的值作为基准，去求w对它的影响，也就是J对w的偏导数（链式求导）：

$$
\frac{\partial{J}}{\partial{w}} = \frac{\partial{J}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{w}}=(z_i-y_i)x_i \tag{3}
$$

- 求b的梯度

$$
\frac{\partial{J}}{\partial{b}} = \frac{\partial{J}}{\partial{z_i}}\frac{\partial{z_i}}{\partial{b}}=z_i-y_i \tag{4}
$$

### 4.2.2 代码实现

```Python
if __name__ == '__main__':

    reader = SimpleDataReader()
    reader.ReadData()
    X,Y = reader.GetWholeTrainSamples()

    eta = 0.1
    w, b = 0.0, 0.0
    #w,b = np.random.random(),np.random.random()
    # count of samples
    num_example = X.shape[0]
    for i in range(num_example):
        # get x and y value for one sample
        x = X[i]
        y = Y[i]
        # get z from x,y
        z = x * w + b
        # calculate gradient of w and b
        db = z - y
        dw = (z - y) * x
        # update w,b
        w = w - eta * dw
        b = b - eta * db

    print("w=", w)    
    print("b=", b)
```

大家可以看到，在代码中，我们完全按照公式推导实现了代码，所以，大名鼎鼎的梯度下降，其实就是把推导的结果转化为数学公式和代码，直接放在迭代过程里！另外，我们并没有直接计算损失函数值，而只是把它融入在公式推导中。

### 4.2.3 运行结果

```
w= [1.71629006]
b= [3.19684087]
```
读者可能会注意到，上面的结果和最小二乘法的结果（w1=2.056827, b1=2.965434）相差比较多，这个问题我们留在本章稍后的地方解决。

### 代码位置

ch04, Level2