Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 双层神经网络的实现

先来观察一下样本：

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|x|0.606|0.129|0.643|...|0.199|
|y|-0.113|-0.269|-0.217|...|-0.281|

首先观察一下样本数据的范围，x是在[0,1]，y是[-1,1]，这样我们就不用做数据归一化了。这条线看起来像一条处于攻击状态的眼镜蛇！

<img src=".\Images\8\Sample.png">

## 定义神经网络结构

我们定义一个两层的神经网络，输入层不算，一个隐藏层，含128个神经元，一个输出层。

<img src=".\Images\8\nn.png">

## 输入层

输入层就是一个标量x值。

## 权重矩阵W1/B1

它是连接两层之间的纽带，有的人理解它应该属于输入层，有的人理解应该属于隐藏层，各有各的道理，我个人倾向于把它归到隐藏层，理由是$Z1=W1 \cdot X+B1$，在X固定的前提下，W1决定了Z1的值。另外一个理由是B1的存在位置，在本例中B1是一个128x1的矩阵，它是隐藏层128个神经元的偏移，所以它应该属于隐藏层。

$$
W1=
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3} \\
w_{4} \\
\end{pmatrix}
$$

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-128的权重线B1-1...B1-128应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-128的矩阵/向量。
$$
B1=
\begin{pmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
b_{4} \\
\end{pmatrix}
$$


## 隐藏层

我们用一个4个神经元的网络来模拟函数，每个神经元的输入$Z1 = W1 \cdot X + B1$，我们在这里使用双曲sigmoid函数，所以输出是$A1 = Sigmoid(Z1)$。

## 权重矩阵W2/B2

与W1/B1类似，我个人认为它属于输出层。W2的尺寸是1x4，B2的尺寸是1x1。
$$
W2=
\begin{pmatrix}w_{1} & w_{2} & w_{3} & w_{4} \end{pmatrix}
$$

$$
B2=
\begin{pmatrix}
b
\end{pmatrix}
$$

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元，$Z2=W2 \cdot A1+B2$。


## 前向计算图

<img src=".\Images\8\fc.png">

至此，我们得到了以下一串公式：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$


## 模块化设计

Python代码，初学者容易写一大堆东西放在一个文件里。但Python本身是支持面向对象的，所以我们用模块化的思想来设计神经网络。

<img src=".\Images\8\NNModule.png">

### Setting and Parameters

这部分可以放在主程序里，手动设置好各种参数，传入神经网络来控制其运行过程。

### Import and Data Operator

主要负责读取训练数据，归一化等等。

### Two Layer Network Trainer

两层神经网络的框架代码，包括前向计算，反向传播，更新梯度，循环控制。

### Activations

激活函数库。

### Loss Functions

损失函数库，损失值的历史记录，便于分析训练过程，以便修改参数，得到更好的训练结果。

# 运行结果

按照上述代码的“标准”设置，我们可以得到以下结果：

<img src=".\Images\8\r128-1000.png"> 

代码位置：ch08, Level2
