Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 权重矩阵初始化

## Xavier初始化方法

$$
W \sim U [-\sqrt{{6 \over n_{input} + n_{output}}}, \sqrt{{6 \over n_{input} + n_{output}}}]
$$

即权重矩阵参数应该满足在该区间内的均匀分布。其中的W是权重矩阵，U是Uniform分布，即均匀分布。

[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

by Xavier Glorot, Yoshua Bengio in AISTATS 2010.

Abstract:

Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We find that the logistics Sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.

简译：神经网络在2006年之前不能很理想地工作，很大原因在于权重矩阵初始化方法上。Sigmoid函数不太适合于深度学习，因为会导致梯度饱和。基于以上原因，我们提出了一种可以快速收敛的参数初始化方法。

[这是中译版](https://blog.csdn.net/victoriaw/article/details/73000632)，感谢译者。

||各层的激活值|各层的反向传播梯度|
|---|---|---|
|Normal|<img src=".\Images\10\normal_activation.jpg">|<img src=".\Images\10\normal_bp.jpg">|
|Xavier|<img src=".\Images\10\xavier_activation.png">|<img src=".\Images\10\xavier_bp.png">|

## MSRA初始化方法

[原文](https://arxiv.org/pdf/1502.01852.pdf)，何凯明，Microsoft Research Asia，2015。

Abstract:

Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [29]). To our knowledge,our result is the ﬁrst to surpass human-level performance(5.1%, [22])on this visual recognition challenge.

主要是想解决使用Relu激活函数后，方差会发生变化，因此初始化权重的方法也应该变化。

只考虑输入个数时，MSRA初始化是一个均值为0，方差为2/n的高斯分布：

$$
W \sim G [-\sqrt{{6 \over n_{input} + n_{output}}}, \sqrt{{6 \over n_{input} + n_{output}}}]
$$

其中的W为权重矩阵，G表示高斯分布，Gaussian Distribution，也叫做正态分布，Normal Distribution。

##  三种初始化方法的比较

1. 零值初始化
2. [0,1]正态分布随机值初始化（均值为0，方差为1）
3. [0,1]Xavier均匀分布随机值初始化

|初始化方法|损失函数值|训练结果|
|---|---|---|
|零值初始化|<img src=".\Images\10\zero_loss.png">|<img src=".\Images\10\zero_result.png">|
|正态分布初始化|<img src=".\Images\10\norm_loss.png">|<img src=".\Images\10\norm_result.png">|
|Xavier初始化|<img src=".\Images\10\xavier_loss.png">|<img src=".\Images\10\xavier_result.png">|

木头：为什么零值初始化不能得到正确的结果呢？

铁柱：我们只训练了50000个epoch，如果训练更多的轮数，也不会得到正确的结果。看下面的零值初始化的权重矩阵值打印输出：
```
[[-10.29778156]
 [-10.29778156]
 [-10.29778156]
 [-10.29778156]]
[[9.92847462]
 [9.92847462]
 [9.92847462]
 [9.92847462]]
[[-0.29563053 -0.29563053 -0.29563053 -0.29563053]]
[[0.93438753]]
```
可以看到W1和W2的值内部4个单元都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有w的值都同步更新，没有差别。这样的话，无论多少论，最终的结果也不会准确。

正态分布初始化的结果：

```
[[ -1.0711346 ]
 [-19.75850005]
 [ -8.84552296]
 [  5.08410339]]
[[ 1.81361968]
 [16.69671287]
 [ 4.25394174]
 [-3.49831165]]
[[ 2.47519669 -2.01738001 -2.73550422 -4.55522827]]
[[2.50646588]]
```

Xavier初始化的结果：

```
[[ -5.86643409]
 [-12.80232309]
 [ -0.94429957]
 [ 10.36952416]]
[[ 4.13811338]
 [10.71442528]
 [-0.27816434]
 [-4.92664038]]
[[ 5.65247181 -3.34531482  0.9183063   2.53985718]]
[[-2.84968849]]
```

后两者最后都得到了准确的结果，但是Xavier的epoch次数是13166，比第二种方法的29096少了一倍多。所以我们推荐使用Xavier初始化方法。

