Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 梯度下降

在绝大多数文章中，都以“一个人被困在山上，需要迅速下到谷底”来举例，这个人会“寻找当前所处位置最陡峭的地方向下走”。这个例子中忽略了安全因素，这个人不可能沿着最陡峭的方向走，要考虑坡度。

在自然界中，梯度下降的最好例子，就是泉水下山的过程：

<img src=".\Images\2\gd_water.png"/>

1. 水受重力影响，会在当前位置，沿着最陡峭的方向流动，有时会形成瀑布（梯度下降）
2. 水流下山的路径不是唯一的，在同一个地点，有可能有多个位置具有同样的陡峭程度，而造成了分流（可以得到多个解）
3. 遇到坑洼地区，有可能形成湖泊，而终止下山过程（不能得到全局最优解，而是局部最优解）


# 梯度下降的数学理解

先抛开神经网络，损失函数，反向传播等内容，用数学概念理解一下梯度下降。

梯度下降的数学公式：

$$x_n = x_{n-1} - \eta \cdot \nabla J(x) \tag{1}$$

其中：
- $x_n$：下一个值
- $x_{n-1}$：当前值
- $-$：梯度的反向
- $\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长
- $\nabla$：梯度，函数当前位置的最快上升点
- $J(x)$：函数

## 单变量函数的梯度下降

假设一个单变量函数：

$$J(x) = x ^2$$

我们的目的是找到该函数的最小值，于是计算其微分：

$$J'(x) = 2x$$

在假设初始位置为：

$$x_0=1.2$$

学习率：

$$\eta = 0.3$$

根据公式1的迭代过程是：
$$
x_0 = 1.2 $$$$
x_1 = x_0 - \eta \cdot J'(x_0) = 1.2 - 0.3 \cdot 2 \cdot 1.2 = 0.48
$$$$
x_2 = x_1 - \eta \cdot J'(x_1) = 0.48 - 0.3 \cdot 2 \cdot 0.48 = 0.192
$$$$
x_3 = 0.0768$$$$
x_4 = 0.03072$$$$
x_5=0.0912288$$$$
x_6=0.0049152
$$

上面的过程如下图所示：

<img src=".\Images\2\gd_single_variable.png"> 


## 双变量的梯度下降


上面的损失函数的二维图像如下图所示：
<img src=".\Images\2\grad1.png" width="600"> 
其中，横坐标是预测值，纵坐标是损失值。假设真实值为1，那么预测值越接近1，损失函数的值越小。

### 为什么说是“梯度下降”？

“梯度下降”，刚接触这个词时，我总是往“降低难度”或“降低维度”方面去理解，因为有个“下降”的动词在里面。而实际上，“下降”在这里面的含义是“与导数相反的方向”的意思。

我们假设上面这个图形的函数是$y = (x-1)^2+0.01$，则$y'_x = 2(x-1)$。

- 在点B上，这个函数的切线（绿色）是指向下方的（Y轴方向），所以是个负数：
假设$X_B$ = 0.1，则$y'_{0.1} = 2*(0.1-1) = -1.8$
亦即点B的梯度是个**负数**。此时我们要把预测值向**正的方向**移动，亦即**梯度的反方向**。
- 在F点上，切线（绿色）向上：
假设$X_F$ = 1.5, 则$y'_{1.5} = 2*(1.5-1) = 1$
是个**正数**，此时我们要把预测值向左侧**负的方向**移动，也是**梯度的反方向**。

所以总体上看，无论预测值在极值的左侧还是右侧，只要是沿着梯度的反方向移动，损失函数值都会向中间（坡底）靠拢。

另外一种理解方式如下图所示：
<img src=".\Images\2\gd.png">  
不同的参数组合形成的损失函数值，可以形成一个不规则椭圆，不规则椭圆的圆心位置，是损失值为0的位置，也是我们要逼近的目标。

这个椭圆，类似地图等高线来表示的一个洼地，中心位置比边缘位置要低，通过对损失函数的计算，对损失函数的求导，对网络参数的求导，会带领我们沿着等高线形成的梯子一步步下降，无限逼近中心点。


如何使用损失函数呢？具体步骤：
1. 用随机值初始化前向计算公式
2. 代入样本，计算输出的预测值
3. 用损失函数计算预测值和标签值（真实值）的误差
4. 将误差回传，修订前向计算公式
5. goto 2, 直到损失函数值达到一个满意的值就停止迭代

如下图所示：
<img src=".\Images\2\grad2.png" width="600">  

1. 我们先知道了A点的切线的方向，亦即黄色的线，但是不知道长度
2. 我们有步长值η，以及梯度下降公式$X_1 = X_0 – η * dx$
3. 因为$y'_x的导数dx = 2(X-1), η = 0.1, X_0 = 0.2, 于是有X_1 = X_0–0.1*2(X_0-1) = 0.36$，这就等同于我们知道了切线的长度，亦即绿色的线的长度和方向都确定了
4. 然后我们可以画出红色的线（亦即弦线）

所以，弦线在这里面没啥用途，只是表示一个迭代跳跃的动作而已。实际的变化值已经由绿色的线定义好了。

## 学习率η的选择

在code里，我们把学习率定义为learning_rate，或者eta，或者叫 $\alpha$。
针对上面的例子，当初始值为-0.8，学习率为1时，迭代的情况很尴尬：不断在一条水平线上跳来跳去，永远也不能下降。
<img src=".\Images\2\gd100.png"> 
当学习率=0.8时，会有这种左右跳跃的情况发生，这不利于神经网络的训练。
<img src=".\Images\2\gd080.png"> 
当学习率=0.6时，也会有跳跃，幅度偏小。
<img src=".\Images\2\gd060.png"> 
当学习率=0.4时，损失值会从单侧下降，4步以后基本接近了理想值。
<img src=".\Images\2\gd040.png"> 
当学习率=0.2时，损失值会从单侧下降，但下降速度较慢，8步左右接近极值。
<img src=".\Images\2\gd020.png"> 
当学习率=0.1时，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。
<img src=".\Images\2\gd010.png"> 

**练习：用Python代码实现以上过程**


