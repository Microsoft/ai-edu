<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 18.4 解决MNIST分类问题

### 18.4.1 模型搭建

在12.1中，我们用一个三层的神经网络解决MNIST问题，并得到了97.49%的准确率。当时使用的模型是这样的：

<img src='../Images/12/nn3.png'/>

这一节中，我们将学习如何使用卷积网络来解决MNIST问题。首先搭建模型如下：

<img src="../Images/17/mnist_net.png">

|Layer|参数|输入|输出|参数个数|
|---|---|---|---|---|
|卷积层|8x5x5,s=1|1x28x28|8x24x24|200+8|
|激活层|2x2,s=2, max|8x24x24|8x24x24||
|池化层|Relu|8x24x24|8x12x12||
|卷积层|16x5x5,s=1|8x12x12|16x8x8|400+16|
|激活层|Relu|16x8x8|16x8x8||
|池化层|2x2, s=2, max|16x8x8|16x4x4||
|全连接层|256x32|256|32|8192+32|
|批归一化层||32|32||
|激活层|Relu|32|32||
|全连接层|32x10|32|10|320+10|
|分类层|softmax,10|10|10|

卷积核的大小如何选取呢？大部分卷积神经网络都会用1、3、5、7的方式递增，还要注意在做池化时，应该尽量让输入的矩阵尺寸是偶数，如果不是的话，应该在上一层卷积层加padding，使得卷积的输出结果矩阵的宽和高为偶数。

### 18.4.2 代码实现

```Python
def model():
    num_output = 10
    dataReader = LoadData(num_output)

    max_epoch = 5
    batch_size = 128
    learning_rate = 0.1
    params = HyperParameters_4_2(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.Xavier,
        optimizer_name=OptimizerName.Momentum)

    net = NeuralNet_4_2(params, "mnist_conv_test")
    
    c1 = ConvLayer((1,28,28), (8,5,5), (1,0), params)
    net.add_layer(c1, "c1")
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "relu1")
    p1 = PoolingLayer(c1.output_shape, (2,2), 2, PoolingTypes.MAX)
    net.add_layer(p1, "p1") 
  
    c2 = ConvLayer(p1.output_shape, (16,5,5), (1,0), params)
    net.add_layer(c2, "23")
    r2 = ActivationLayer(Relu())
    net.add_layer(r2, "relu2")
    p2 = PoolingLayer(c2.output_shape, (2,2), 2, PoolingTypes.MAX)
    net.add_layer(p2, "p2")  

    f3 = FcLayer_2_0(p2.output_size, 32, params)
    net.add_layer(f3, "f3")
    bn3 = BnLayer(f3.output_size)
    net.add_layer(bn3, "bn3")
    r3 = ActivationLayer(Relu())
    net.add_layer(r3, "relu3")

    f4 = FcLayer_2_0(f3.output_size, 10, params)
    net.add_layer(f4, "f2")
    s4 = ClassificationLayer(Softmax())
    net.add_layer(s4, "s4")

    net.train(dataReader, checkpoint=0.05, need_test=True)
    net.ShowLossHistory(XCoordinate.Iteration)
```

### 18.4.3 运行结果

训练5个epoch后的损失函数值和准确率的历史记录曲线如下：

<img src="../Images/17/mnist_loss.png">

打印输出结果如下：

```
...
epoch=4, total_iteration=2089
loss_train=0.105681, accuracy_train=0.976562
loss_valid=0.090841, accuracy_valid=0.974600
epoch=4, total_iteration=2111
loss_train=0.030109, accuracy_train=0.992188
loss_valid=0.064819, accuracy_valid=0.981000
epoch=4, total_iteration=2133
loss_train=0.054449, accuracy_train=0.984375
loss_valid=0.060550, accuracy_valid=0.982000
save parameters
time used: 513.3446323871613
testing...
0.9865
```

最后可以得到98.44%的的准确率，比全连接网络要高1个百分点。如果想进一步提高准确率，可以尝试增加卷积层的能力，比如使用更多的卷积核来提取更多的特征。

### 18.4.4 可视化

#### 第一层卷积核

<img src="../Images/17/mnist_kernal.png">

||1|2|3|4|
|--|:--:|:--:|:--:|:--:|
|1|边框|局部密集区域|左上侧轮廓|右下侧轮廓|
|2|骨干偏上|上侧轮廓|骨干偏下|左斜45度|

#### 第一层卷积结果

|||
|--|--|
|<img src="../Images/17/mnist_0.png">|<img src="../Images/17/mnist_1.png">|
|<img src="../Images/17/mnist_2.png">|<img src="../Images/17/mnist_3.png">|
|<img src="../Images/17/mnist_4.png">|<img src="../Images/17/mnist_5.png">|
|<img src="../Images/17/mnist_6.png">|<img src="../Images/17/mnist_7.png">|
|<img src="../Images/17/mnist_8.png">|<img src="../Images/17/mnist_9.png">|


下图是第一梯队的卷积、激活、池化层的输出结果：

<img src="../Images/17/mnist_layer_123.png">
- Conv1：是卷积的结果
- Relu1：是卷积后激活的结果，一些负值会被去掉
- Pool1：是激活后的池化的结果

从池化后的结果可以大致分析出：
- 第2、5、7个卷积核的效果并不好，图像模糊一团，看不出什么典型特征
- 第1、3个卷积核，找到的是左上边缘的信息
- 第4个卷积核，找到的是右下边缘的信息
- 第6个卷积核，找到上边缘的信息
- 第8个卷积核找到了左斜45度的骨干信息

下图是第二梯队的卷积、激活、池化层的输出结果：

<img src="../Images/17/mnist_layer_456.png">

- Conv2：由于是在第一层的特征图上卷积后叠加的结果，所以基本不能理解
- Relu2：能看出的是如果黑色区域多的话，说明基本没有激活值，此卷积核效果就没用
- Pool2：池化后分化明显的特征图是比较有用的的特征，比如3、6、12、15、16，信息太多或者太少的特征图，都用途偏小

### 参考资料

- http://scs.ryerson.ca/~aharley/vis/conv/

读者可以在上面这个网站看到MNIST的可视化结果，用鼠标可以改变三维视图的视角。
