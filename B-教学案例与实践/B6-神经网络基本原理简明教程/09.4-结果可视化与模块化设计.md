Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 结果可视化

对于分线性问题，我们不能用简单的直线来分割区域了，因为边界并不是直线。其实对于神经网络来说，它分类的原理是这样的：

<img src='./Images/9/inference.png'/>

我们假设在一个三分类问题中，上图中有三个点，对于每个点，都计算一个softmax值，分别得到了三组值：

- [0.8, 0.1, 0.1] - 红点
- [0.4, 0.5, 0.1] - 绿点
- [0.2, 0.2, 0.6] - 蓝点

每组值内三个值相加都是1，表明该点属于三个分类的概率。红点的0.8为最大值，判定为第一类；绿点的0.5为最大值，判定为第二类；蓝点的0.6为最大值，判定为第三类。

假设这个区域内有无数个点，都分别被分到三类中，那么会形成三种颜色区域，绘出这些区域，就可以视觉上判断我们的训练结果是否准确。

根据这个原理，我们可以以任意精度来给不同的区域着色，从而在视觉上理解神经网络的工作结果。

```Python
    def ShowAreaResult(self, X, dict_weights):
        count = 50
        x1 = np.linspace(0,1,count)
        x2 = np.linspace(0,1,count)
        for i in range(count):
            for j in range(count):
                x = np.array([x1[i],x2[j]]).reshape(2,1)
                dict_cache = self.ForwardCalculationBatch(x, dict_weights)
                output = dict_cache["Output"]
                r = np.argmax(output, axis=0)
                if r == 0:
                    plt.plot(x[0,0], x[1,0], 's', c='m')
                elif r == 1:
                    plt.plot(x[0,0], x[1,0], 's', c='y')
                # end if
            # end for
        # end for
    #end def
```

然后在着色区域上绘出原始点：

```Python
    def ShowData(self, X, Y):
        for i in range(X.shape[1]):
            if Y[0,i] == 1:
                plt.plot(X[0,i], X[1,i], '^', c='g')
            elif Y[0,i] == 2:
                plt.plot(X[0,i], X[1,i], 'x', c='r')
            elif Y[0,i] == 3:
                plt.plot(X[0,i], X[1,i], '.', c='b')
            # end if
        # end for
        plt.xlabel("x1")
        plt.ylabel("x2")
        plt.show()
```

ShowAreaResult()函数的工作原理：
1. 首先把显示区域分成50x50的点阵，如果需要更高的精度，可以用100x100，但是绘图所需时间要多4倍。
2. 然后依次访问这2500个点，用其坐标值代入前向计算公式进行推理，得到上面描述的[p1, p2, p3]
3. 然后取其最大值，会得到[0, 1, 2]这三个值中的一个，表示对应到三个区域。
4. 再根据返回值着色，于是得到了下面这张图

<img src='./Images/9/sgd_result_9.png'/>

ShowData()函数，是再前面的着色区基础上，把原始数据叠加上去，以便让大家直观地看到工作结果。


# 模块化设计

Python代码，初学者容易写一大堆东西放在一个文件里。但Python本身是支持面向对象的，所以我们用模块化的思想来设计神经网络。

<img src=".\Images\9\netlayers.png">

## Data Reader

封装了对训练数据的各种操作：
- 读取
- 归一化样本值和预测值
- 处理标签值
- 获取批量数据
- 打乱数据顺序

## Optimizer

优化器，包括：

- SGD
- Adam
- Momentum
- NAG
- AdaGrad
- RMSProp

## Activations

激活函数库，目前包括（待扩充）：

- Sigmoid
- Softmax

## Loss Function

损失函数帮助函数，包括：

- 损失函数定义（均方差和交叉熵）
- 记录损失值
- 显示损失值历史记录
- 获得最小损失值的epoch:iteration时刻的权重值

## WeightsBias

对权重值和偏移值的帮助函数：

- 初始化方法
- 初始化
- 永久存储
- 附加优化器
- 应用优化器更新参数

## Parameters

定义神经网络运行的必要参数：

- 输入特征值数量
- 输出数量
- 隐层神经元数量
- 全局学习率
- 停止条件（最大epoch数和最小loss门限）
- 样本批数
- 损失函数类型
- 初始化参数方法
- 优化器迭代方法

## TwoLayerNet

- 反向传播
- 训练方法

## TwoLayerFittingNet

- 正向计算（Sigmoid + Identity）

## TwoLayerClassificationNet

- 正向计算（Sigmoid + Softmax）

代码位置：ch09, Level0, Level1, Level2