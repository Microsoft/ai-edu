Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。这个一层的神经网络没有中间层，只有输入项和输出层（输入项不算做一层），而且只有一个神经元，并且神经元有一个线性输出，不经过激活函数处理。亦即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

<img src=".\Images\5\setup.jpg">

矩阵运算过程：

$$W^{1 \times 3} \cdot X^{3 \times 1} + B^{1 \times 1} => Z^{1 \times 1}$$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(1,3)表示W是一个1行3列的矩阵。

## 输入层

单独看一个样本是这样的：

$$
X_1 =
\begin{pmatrix}
x_{1,1} \\
\\
x_{1,2} \\
\\
x_{1,3}
\end{pmatrix} = 
\begin{pmatrix}
3 \\
\\
1 \\
\\
96
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 434 \end{pmatrix}
$$

假设一共有m个样本，每个样本n个特征值，X就是一个$n \times m$的矩阵，模样是这样紫的（n=3，m=1000，亦即3行1000列）：

$$
X = \\
\begin{pmatrix} 
X_1 & X_2 \dots X_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{2,1} & \dots & x_{1000,1} \\
x_{1,2} & x_{2,2} & \dots & x_{1000,2} \\
x_{1,3} & x_{2,3} & \dots & x_{1000,3}
\end{pmatrix} = 
\begin{pmatrix}
3 & 2 & \dots & 3 \\
1 & 4 & \dots & 2 \\
96 & 100 & \dots & 54
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y_1 & y_2 & \dots & y_m \\
\end{pmatrix}=
\begin{pmatrix}
434 & 500 & \dots & 410 \\
\end{pmatrix}
$$


$X_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

## 权重W和B

有人问了，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？那样好像更符合思维习惯？

确实是！但是在实际的矩阵运算时，由于是$Z=W \cdot X+B$，W在前面，X在后面，所以必须是这个样子的：

$$
\begin{pmatrix}
w_1 & w_2 & w_3
\end{pmatrix}
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix}=
w_1 \cdot x_1+w_2 \cdot x_2+w_3 \cdot x_3
$$

假设每个样本x有n个特征向量，上式中的W就是一个$1 \times n$的向量，让每个w都对应一个x：
$$
\begin{pmatrix}w_1 & w_2 \dots w_n\end{pmatrix}
$$

B是个单值，因为只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

## 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。可以把拟合任务想象成用一支笔沿着一堆样本画一条中线，所以我们用一个神经元代表一支笔，用两个神经元的话，就会画出两条线来，不符合我们的任务假设。

由于是线性的，所以没有用激活函数。


# 代码

首先，我们把示例代码ch04中的level4-BatchGradientDescent.py的代码全部拷贝过来。

然后稍微修改一下主程序部分：

```Python
if __name__ == '__main__':
    # hyper parameters
    # SGD, MiniBatch, FullBatch
    method = "SGD"

    eta, max_epoch,batch_size = InitializeHyperParameters(method)
    
    # W size is 3x1, B is 1x1
    W, B = InitialWeights(3,1,0)
    # calculate loss to decide the stop condition
    loss = 5
    dict_loss = {}
    # read data
    X, Y = ReadData()
    # count of samples
    num_example = X.shape[1]
    num_feature = X.shape[0]


    # if num_example=200, batch_size=10, then iteration=200/10=20
    max_iteration = (int)(num_example / batch_size)
    for epoch in range(max_epoch):
        print("epoch=%d" %epoch)
        for iteration in range(max_iteration):
            # get x and y value for one sample
            batch_x, batch_y = GetBatchSamples(X,Y,batch_size,iteration)
            # get z from x,y
            batch_z = ForwardCalculationBatch(W, B, batch_x)
            # calculate gradient of w and b
            dW, dB = BackPropagationBatch(batch_x, batch_y, batch_z)
            # update w,b
            W, B = UpdateWeights(W, B, dW, dB, eta)
            
            # calculate loss for this batch
            loss = CheckLoss(W,B,X,Y)
            print(epoch,iteration,loss,W,B)
            prev_loss = loss

            dict_loss[loss] = CData(loss, W, B, epoch, iteration)            

    ShowLossHistory(dict_loss, method)
    w,b,cdata = GetMinimalLossData(dict_loss)
    print(cdata.w, cdata.b)
    print("epoch=%d, iteration=%d, loss=%f" %(cdata.epoch, cdata.iteration, cdata.loss))
```
上述代码中：
1. 权重W的尺寸是3x1，所以InitialWeights(3,1,0)
2. 们用最简单的SGD先测试一下。
3. 然后我们把max_epoch手工设置为1，看看运行情况如何。

木头同学怀着期待的心情用颤抖的右手按下了运行键......but......what happened?

```
epoch=0
0 0 48904435754.68428 [[  93.8  187.6 3705.1]] [[46.9]]
0 1 1.8166533368107932e+16 [[  -57809.94  -115619.88 -2254540.76]] [[-28904.97]]
0 2 3.303753527006361e+22 [[5.24153113e+07 1.57303744e+08 3.04118649e+09]] [[26207655.65900001]]
0 3 4.666606120613639e+28 [[-9.95909922e+10 -9.94861038e+10 -3.61733595e+12]] [[-3.31882615e+10]]
0 4 3.1950576277511314e+33 [[7.40656556e+13 9.26070721e+13 9.41989558e+14]] [[1.85081234e+13]]
......
0 51 2.5265780510604876e+290 [[-1.30516140e+142 -2.17739960e+142 -2.65321628e+143]] [[-4.35320286e+141]]
0 52 9.833147574107431e+295 [[6.30118777e+144 4.18771893e+144 1.66009649e+146]] [[2.10039326e+144]]
0 53 2.2330906427718674e+301 [[-1.14168064e+147 -4.58773957e+147 -7.90447361e+148]] [[-1.14588143e+147]]
D:\ProgramLanguage\Python3664\lib\site-packages\numpy\core\_methods.py:36: RuntimeWarning: overflow encountered in reduce
  return umr_sum(a, axis, dtype, out, keepdims, initial)
0 54 inf [[1.15910260e+150 3.47614511e+150 4.22698716e+151]] [[5.7897626e+149]]
E:\GitHub\AI-EDU\B-教学案例与实践\B6-神经网络基本原理简明教程\微软-方案1\NeuralNetwork\ch05\level2-NeuralNetworkl.py:52: RuntimeWarning: overflow encountered in square
  LOSS = (Z - Y)**2
0 55 inf [[-9.05391078e+152 -1.35634913e+153 -1.19695200e+154]] [[-2.26058569e+152]]
0 56 inf [[3.83049444e+155 7.66553321e+155 1.35544346e+157]] [[1.27758886e+155]]
0 57 inf [[-2.71982869e+158 -1.35416406e+158 -3.39101954e+159]] [[-6.79637207e+157]]
0 58 inf [[1.31978600e+161 1.32115166e+161 3.20368561e+162]] [[3.29946819e+160]]
0 59 inf [[-3.55246981e+163 -2.13807945e+164 -3.95468743e+165]] [[-3.5623682e+163]]
......
0 107 inf [[-3.66925180e+295 -1.46769384e+296 -2.56685527e+297]] [[-3.66923305e+295]]
0 108 inf [[8.54324107e+298 6.39550581e+298 1.77091704e+300]] [[2.13305835e+298]]
0 109 inf [[-3.99010488e+301 -3.99225262e+301 -9.97891114e+302]] [[-1.33074965e+301]]
0 110 inf [[5.46180544e+303 1.09634904e+304 3.01595966e+305]] [[5.48839899e+303]]
0 111 inf [[-9.42076161e+306 -9.41525992e+306             -inf]] [[-3.13658607e+306]]
E:\GitHub\AI-EDU\B-教学案例与实践\B6-神经网络基本原理简明教程\微软-方案1\NeuralNetwork\ch05\level2-NeuralNetworkl.py:44: RuntimeWarning: invalid value encountered in subtract
  W = W - eta*dW
0 112 nan [[inf inf nan]] [[inf]]
0 113 nan [[nan nan nan]] [[nan]]
0 114 nan [[nan nan nan]] [[nan]]
0 115 nan [[nan nan nan]] [[nan]]
......
```
怎么会overflow呢？于是右手的颤抖没有停止，左手也开始颤抖了。

再看看损失函数值的变化：

<img src=".\Images\5\wrong_loss.png" width="600">

损失值随着迭代次数越来越大，说明训练根本没有成功。
