Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
# 知识点

- 多元线性回归
- 样本数据归一化

# 提出问题

大哥，北京的房价了解一下！

房价预测问题，成为了机器学习的一个入门话题。我们也不能免俗，但是，不要用美国的什么多少平方英尺，多少个房间的例子来说事儿了，中国人不能理解为啥多出来一个room还买得贵，羊毛不是出在羊身上吗？我们来个北京的例子！

影响北京房价的因素有很多，几个最重要的因子和它们的取值范围是：
- 朝向（在北方地区，窗户面向阳光的房子要抢手一些）：北=1，西=2，东=3，南=4
- 地理位置：二环，三环，四环，五环，六环，分别取值为2，3，4，5，6，二环的房子单价最贵
- 面积：40~120平米，连续值


影响房价的因素有很多，居住面积，地理位置，朝向，是其中三个比较重要的因素。当然还有学区房什么比较特殊的情况。关于房价，我们有1000个样本，每个样本有三个特征值，一个标签值，示例如下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|整套价格（万元）|434|500|321|482|...|410|

- 特征值1 - 窗户朝向
一共有”东”“南”“西”“北“四个值，用数字化表示是：
东：3
南：4
西：2
北：1
因为朝南的房子比较贵，其次为东，西，北。为啥东比西贵？难度是因为我们常说“东西”，东在前，西在后？瞎扯！是因为夏天时朝西的窗户西晒时间长，比较热。

- 特征值2 - 地理位置
二环：2 - 单价最贵
三环：3
四环：4
五环：5
六环：6 - 单价最便宜

- 特征值3 - 房屋面积
统计所有样本数据得到房屋的面积范围是[40,120]


### 问题：在北京东五环的一套窗户朝西的93平米的房子，大概是多少钱？

由于表中可能没有恰好符合上述条件的数据，因此我们必须根据1000个样本值来建立一个模型，来解决我们的问题。

先假设这个问题是个线性回归问题，而且是典型的多元线性回归。函数模型如下：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

为了方便大家理解，咱们具体化一下上面的公式，按照本系列文章的符号约定就是：

$$ 
Z = w_1x_1+w_2x_2+w_3x_3+b = W*X + B
$$

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。这个一层的神经网络没有中间层，只有输入项和输出层（输入项不算做一层），而且只有一个神经元，并且神经元有一个线性输出，不经过激活函数处理。亦即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

<img src=".\Images\5\setup.jpg" width="600">

矩阵运算过程：$W(1,3) * X(3,1) + B(1,1) => Z(1,1)$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(1,3)表示W是一个1行3列的矩阵。

## 输入层

假设一共有m个样本，每个样本n个特征值，X就是一个$n \times m$的矩阵，模样是这样紫的（n=3，m=1000，亦即3行1000列）：

$$
X = \\
\begin{pmatrix} 
X_1 & X_2 \dots X_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{2,1} & \dots & x_{1000,1} \\
\\
x_{1,2} & x_{2,2} & \dots & x_{1000,2} \\
\\
x_{1,3} & x_{2,3} & \dots & x_{1000,3}
\end{pmatrix} = 
\begin{pmatrix}
3 & 2 & \dots & 3 \\
\\
1 & 4 & \dots & 2 \\
\\
96 & 100 & \dots & 54
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y_1 & y_2 & \dots & y_m \\
\end{pmatrix}=
\begin{pmatrix}
434 & 500 & \dots & 410 \\
\end{pmatrix}
$$

单独看一个样本是这样的：

$$
x_1 =
\begin{pmatrix}
x_{1,1} \\
\\
x_{1,2} \\
\\
x_{1,3}
\end{pmatrix} = 
\begin{pmatrix}
3 \\
\\
1 \\
\\
96
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 434 \end{pmatrix}
$$

$X_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

## 权重W和B

有人问了，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？那样好像更符合思维习惯？

确实是！但是在实际的矩阵运算时，由于是$Z=W \cdot X+B$，W在前面，X在后面，所以必须是这个样子的：

$$
\begin{pmatrix}
w_1 & w_2 & w_3
\end{pmatrix}
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix}=
w_1 \cdot x_1+w_2 \cdot x_2+w_3 \cdot x_3
$$

假设每个样本x有n个特征向量，上式中的W就是一个$1 \times n$的向量，让每个w都对应一个x：
$$
\begin{pmatrix}w_1 & w_2 \dots w_n\end{pmatrix}
$$

B是个单值，因为只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

## 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。由于是线性的，所以没有用激活函数。

# 下载训练数据

[点击下载训练数据](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HousePriceXData.dat)

[点击下载标签数据](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/HousePriceYData.dat)

# 读取文件数据
```Python
def LoadData():
    Xfile = Path("HouseXData.dat")
    Yfile = Path("HouseYData.dat")
    if Xfile.exists() & Yfile.exists():
        XData = np.load(Xfile)
        YData = np.load(Yfile)
        return XData,YData
    
    return None,None
```

# 前向计算

```Python
def ForwardCalculation(Xm,W,b):
    z = np.dot(W, Xm) + b
    return z
```
# 损失函数

我们依然用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。

```Python
def CheckLoss(w, b, X, Y, count, prev_loss):
    Z = ForwardCalculation(X, w, b)
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/count/2
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

# 反向传播

```Python
def BackPropagation(Xm,Y,Z):
    dZ = Z - Y
    dB = dZ
    dW = np.dot(dZ, Xm.T)
    return dW, dB

def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```
上面第一个求梯度函数，第二个函数用于每次迭代时更新w，b的值。
求解W和B的梯度方法与我们前面的文章“单入单出的一层神经网络”完全一样，所以求db和dw的逻辑也是相同的，重要区别在于：

第四章是：
```Python
dW = dZ * x
```
而本章是：
```Python
dW = np.dot(dZ, Xm.T)
```

原因是在第四章中，我们的样本值只有一维特征，所以用个标量计算就可以了。在本章中，样本值有三个特征值，所以Xm是个向量，需要用到矩阵运算。Xm.T表示样本值的转置矩阵，这个的公式推导在《基本数学导数公式》中。

# 预测房价
以下代码使用问题中的参数（朝西，五环，93平米）来预测房价：
```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(w, b):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, w, b)
    print(z1)
```

# 主程序
```Python
X, Y = ReadData()

m = X.shape[1]  # count of examples
n = X.shape[0]  # feature count
eta = 0.01   # learning rate
loss, diff_loss, prev_loss = 10, 10, 5
eps = 1e-10
max_iteration = 100 # 最多100次循环
# 初始化w,b
B = np.zeros((1,1))
W = np.zeros((1,n))

for iteration in range(max_iteration):
    for i in range(m):
        Xm = X[0:n,i].reshape(n,1)
        Ym = Y[0,i].reshape(1,1)
        Z = ForwardCalculation(Xm, W, B)
        dw, db = BackPropagation(Xm, Ym, Z)
        W, B = UpdateWeights(W, B, dw, db, eta)
        
        loss, diff_loss = CheckLoss(W,B,X,Y,m,prev_loss)
        if i%10==0:
            print(iteration, i, diff_loss)
        if diff_loss < eps:
            print(i)
            break
        prev_loss = loss
    print(iteration, W, B, diff_loss)
    if diff_loss < eps:
        break

print("W=", W)
print("B=", B)

PredicateTest(W, B)
```
怀着期待的心情用颤抖的右手按下了运行键......but......what happened?

```
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:61: RuntimeWarning: overflow encountered in square
  LOSS = (Z - Y)**2
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:63: RuntimeWarning: invalid value encountered in double_scalars
  diff_loss = abs(loss - prev_loss)
C:\Users\Python\LinearRegression\MultipleInputSingleOutput.py:55: RuntimeWarning: invalid value encountered in subtract
  w = w - eta*dw
0 [[nan nan nan]] [[nan]] nan
1 [[nan nan nan]] [[nan]] nan
2 [[nan nan nan]] [[nan]] nan
3 [[nan nan nan]] [[nan]] nan
```
怎么会overflow呢？于是右手的颤抖没有停止，左手也开始颤抖了。

难度我们遇到了传说中的梯度爆炸！数值太大，导致计算溢出了。第一次遇到这个情况，但相信不会是最后一次，因为这种情况在神经网络中太常见了。别慌，擦干净头上的冷汗，让我们debug一下。

# 解决梯度爆炸

## 检查迭代中的数值变化情况

先把迭代中的关键值打印出来：

```
0 -----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[  93.8  187.6 3705.1]]
B: [[46.9]]
1 -----------
Z: [[289982.7]]
Y: [[464]]
dLoss/dZ: [[289518.7]]
dw: [[  579037.4         1158074.8        22582458.60000001]]
db: [[289518.7]]
W: [[  -57809.94  -115619.88 -2254540.76]]
B: [[-28904.97]]
2 -----------
Z: [[-2.62364972e+08]]
Y: [[634]]
dLoss/dZ: [[-2.62365606e+08]]
dw: [[-5.24731213e+08 -1.57419364e+09 -3.04344103e+10]]
db: [[-2.62365606e+08]]
W: [[5.24153113e+07 1.57303744e+08 3.04118649e+09]]
B: [[26207655.65900001]]
......
```
最开始的W,B的值都是0，三次迭代后，W,B的值已经大的超乎想象了。可以停止运行程序了，想一想为什么。

难道是因为学习率太大吗？目前是0.1，设置成0.01试试看：
```
0 ----------
Z: [[0.]]
Y: [[469]]
dLoss/dZ: [[-469.]]
dw: [[  -938.  -1876. -37051.]]
db: [[-469.]]
W: [[ 0.938  1.876 37.051]]
B: [[0.469]]
1 -----------
Z: [[2899.827]]
Y: [[464]]
dLoss/dZ: [[2435.827]]
dw: [[  4871.654   9743.308 189994.506]]
db: [[2435.827]]
W: [[  -3.933654   -7.867308 -152.943506]]
B: [[-1.966827]]
2 ----------
Z: [[-17798.484679]]
Y: [[634]]
dLoss/dZ: [[-18432.484679]]
dw: [[  -36864.969358  -110594.908074 -2138168.222764]]
db: [[-18432.484679]]
W: [[  32.93131536  102.72760007 1985.22471676]]
B: [[16.46565768]]
```
没啥改进。

回想一个问题：为什么在“单入单出的一层神经网络”一文的代码中，我们没有遇到这种情况？把样本拿来看一看：

|样本序号|1|2|3|...|200|
|---|---|---|---|---|---|
|服务器数量(千)|0.928|0.0469|0.855|...|0.373|
|空调功率(千瓦)|4.824|2.950|4.643|...|3.594|

因为所有的X值（服务器数量）都是在[0,1]之间的，而本次的数据有三个特征值，全都是不是在[0,1]之间的，并且取值范围还不相同。我们不妨把本次样本数据也做一下这样的处理，亦即“归一化”。

## 数据归一化
### min-max标准化
也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：

$$
x_{new} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$

其中max为样本数据的最大值，min为样本数据的最小值。
如果想要将数据映射到[-1,1]，则将公式换成：
$$
x_{new} = \frac{x-x_{mean}}{x_{max}-x_{min}}
$$
mean表示数据的均值。

### 样本分析

再把这个表拿出来分析一下：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|1|4|2|4|...|2|
|地理位置（几环）|3|2|6|3|...|3|
|居住面积（平米）|96|100|54|72|...|69|
|价格（万元）|434|500|321|482|...|410|

我们用min-max标准化来归一以上的样本特征数据，得到下表：

|样本序号|1|2|3|4|...|1000|
|---|---|----|---|--|--|--|
|朝向（东南西北）|0|0.667|0.333|1|...|0.33|
|地理位置（几环）|0.25|0|1|0.25|...|0.75|
|居住面积（平米）|0.7|0.75|0.175|0.4|...|0.36|
|价格(万元)|434|500|321|482|...|410|

注意，我们并没有归一化样本的标签数据，所以最后一行的价格还是保持不变。

### 归一化的实现

我们把归一化的函数写好：
```Python
def NormalizeByData(X):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    x_range = np.zeros((1,n))
    x_min = np.zeros((1,n))
    for i in range(n):
        x = X[i,:]
        max_value = np.max(x)
        min_value = np.min(x)
        x_min[0,i] = min_value
        x_range[0,i] = max_value - min_value
        x_new = (x - x_min[0,i])/(x_range[0,i])
        X_new[i,:] = x_new
    return X_new, x_range, x_min

def NormalizeXY(XData, YData, flag):
    if flag=='x_only':
        X, X_range, X_min = NormalizeByData(XData)
        return X, X_range, X_min, YData, -1, -1
    elif flag=='x_and_y':
        X, X_range, X_min = NormalizeByData(XData)
        Y, Y_range, Y_min = NormalizeByData(YData)
        return X, X_range, X_min, Y, Y_range, Y_min
```
- x_range：表示样本特征值的取值范围
- x_min：表示样本特征值的取值范围的下限
- y_range：表示样本标签值的取值范围
- y_min：表示样本标签值的取值范围的下限

然后再改一下主程序的头两行，加上归一化的调用，'x_only'表示只对特征值归一化，不改变标签值：
```
flag = 'x_only' # only normalize X value
XData, YData = ReadData()
X, X_range, X_min, Y, Y_range, Y_min = NormalizeXY(XData, YData, flag)
```
用颤抖的双手同时按下Ctrl+F5，运行开始，结束，一眨眼！仔细看打印结果：
```
......
35 600 [[  5.99999997 -40.00000004 394.9999999 ]] [[292.00000009]] 344.8930674898838
35 700 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 284.6144928234677
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
[[36838.99999298]]
```
# 房屋价格预测

W，B的值都算出来了，说明网络收敛了，这是成功的第一步。再看最后的预测值，93平米的房子需要36839万元！难道是传说中的黄金屋？这不科学！

## 还原真实的W值

这次错在哪里了呢？我们唯一修改的地方就是样本数据特征值的归一化。假设在归一化之前，真实的样本值是X，真实的权重值是W，在归一化之后，样本值变成了X'，训练出来的权重值是W'，不考虑B值的情况下：
$$
Y = W \cdot X \tag{标签值}
$$
$$
Z = W' \cdot X' \tag{预测值}
$$

由于训练时标签值（房价）并没有做归一化，意味着我们是用真实的房价做的训练，所以预测值和标签值应该相等：
$$
因为：Y == Z \\
所以：W \cdot X = W' \cdot X'
$$

假设X'是X的1/N大小：
$$
N*X' = X
$$

把X代入等式左侧:
$$
W \cdot N \cdot X'=W' \cdot X' => W \cdot N = W' => 即：W = W'/N
$$

也就是说我们应该把训练的结果W'除以N倍后，就可以得到真实的W值。N是什么呢？就是X_range。对于本例来说，X_range是一个数组，有三个元素，分别代表三个特征向量值归一化前的取值范围。

## 还原真实的B值

既然W的值不是真实值，需要还原，那么B值一定也是如此。如果我们得到了W的真实值，那么B的真实值也很容易得到，我们还是利用样本数据来做这件事。

因为样本数据标签值$Y = W \cdot X+B$，我们已经有了还原后的W的值，归一化前的样本X值也存在，标签值Y也存在，则：
$$B=Y - W*X$$

## 变成代码
```Python
# get real weights
def DeNormalizeWeights(X_range, XData, n):
    W_real = np.zeros((1,n))
    for i in range(n):
        W_real[0,i] = W[0,i] / X_range[0,i]
    print("W_real=", W_real)

    B_real = 0
    num_sample = XData.shape[1]
    for i in range(num_sample):
        xm = XData[0:n,i].reshape(n,1)
        zm = ForwardCalculation(xm, W_real, 0)
        ym = Y[0,i].reshape(1,1)
        B_real = B_real + (ym - zm)
    B_real = B_real / num_sample
    print("B_real=", B_real)
    return W_real, B_real
```
X_range是我们在做归一化时保留下来的样本的三个特征向量的取值范围，存放在一个三个元素的数组中。XData是原始值，与W_real相乘以后，再用标签值Y去减，就会得到B值。为了避免样本噪音，用了所有的样本来计算差值的总和，最后除以样本总数，就是B_real。

在主程序最后两行加一下这个逻辑，再次运行：

```Python
W_real, B_real = DeNormalizeWeights(X_range, XData, n)
PredicateTest(W_real, B_real)
```
运行结果如下：
```
W= [[  5.9996169  -40.00073494 394.99808248]]
B= [[292.00163197]]
W_real= [[  1.9998723  -10.00018374   4.99997573]]
B_real= [[110.00302237]]
result=[[528.99999999]]
```
W的真实值W_real=[1.99, -10, 4.99]，B的真实值B_real=110，那间93平米的房子的预测值=529万元。相当make sense！

至此，我们完美地解决了北京地区的房价预测问题！但是还没有解决自己可以有能力买一套北京的房子的问题......

Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

“铁柱”是一名老师，在神经网络中穿梭多年，挂了满身的蜘蛛网。“木头”是一名刚入门者，木头木脑的，有问题经常向铁柱请教。

# 高(中)数(学)回忆

### 均值mean：

$$\bar{x}=\frac{1}{n}\sum_i^nx_i$$

### 标准差stdandard deviation：

$$
std=\sqrt{\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}}
$$

### 方差variance

$$
var=\frac{1}{n-1} \sum_i^n{(x_i-\bar{x})^2}=s^2
$$


### 协方差covariance
$$
cov(X,Y)=\frac{1}{n-1} \sum_i^n{[(x_i-\bar{x})(y_i-\bar{y})]}
$$
结果为正，表示X,Y是正相关。

# 数据预处理

木头：老师，归一化原来这么重要，在理论上是怎么解释呢？

铁柱：理论层面上，神经网络是以样本在事件中的统计分布概率为基础进行训练和预测的，也就是说：
1. 样本的各个特征的取值要符合概率分布，即[0,1]
2. 样本的度量单位要相同。我们并没有办法去比较1米和1公斤的区别，但是，如果我们知道了1米在整个样本中的大小比例，以及1公斤在整个样本中的大小比例，比如一个处于0.2的比例位置，另一个处于0.3的比例位置，就可以说这个样本的1米比1公斤要小！

木头：是不是说，数据中地理位置的取值范围是[2,6]，而房屋面积的取值范围为[40,120]，二者相差太远，根本不可以放在一起计算了？

铁柱：是的，你看$W1*X1+W2*X2$这个式子中，如果X1的取值是[2,6]，X2的取值是[40,120]，一是相差太远，而是都不在[0,1]之间，所以对于神经网络来说很难理解。下图展示了归一化前后的情况Loss值的等高图，意思是地理位置和房屋面积取不同的值时，作为组合来计算损失函数值时，形成的类似地图的等高图。左侧为归一化前，右侧为归一化后：

<img src=".\Images\5\normalize.jpg" width="800">

房屋面积的取值范围是[40,120]，而地理位置的取值范围是[2,6]，二者会形成一个很扁的椭圆，如左侧。这样在寻找最优解的时候，过程会非常曲折。运气不好的话，如同我们上面的代码，根本就没法训练。

木头：我还听说有一些类似的词汇，什么标准化，这个和归一化有什么区别呢？

铁柱：有三个类似的概念，归一化，标准化，中心化。

## 归一化

把数据线性地变成[0,1]或[-1,1]之间的小数，把带单位的数据（比如米，公斤）变成无量纲的数据，区间缩放。


归一化有三种方法：
- Min-Max归一化：
$$x_{new}=(x-MinValue)/(MaxValue - MinValue) \tag{5.1}$$
- 平均值归一化： 
$$x_{new} = (x - \bar{x}) / (MaxValue - MinValue) \tag{5.2}$$

- 非线性归一化：
$$对数转换y=log(x)，反余切转换y=atan(x) \cdot 2/π  \tag{5.4}$$

## 标准化
把每个特征值中的所有数据，变成平均值为0，标准差为1的数据，最后为正态分布。

Z-score规范化（标准差标准化 / 零均值标准化，其中std是标准差）：
$$x_{new} = (x - \bar{x})／std \tag{5.5}$$

## 中心化

平均值为0，无标准差要求：
$$x_{new} = x - \bar{x} \tag{5.7}$$

## 神经网络中的数据归一化的意义

木头：老师，那我们在数据归一化之前，根本无法进行训练，是怎么回事儿呢？

铁柱：神经网络中，数据归一化在理论上有重要意义。

1. 数值问题

    归一化/标准化可以避免一些不必要的数值问题。输入变量的数量级未致于会引起数值问题吧，但其实要引起也并不是那么困难。因为tanh的非线性区间大约在[-1.7，1.7]。意味着要使神经元有效，tanh( w1x1 + w2x2 +b) 里的 w1x1 +w2x2 +b 数量级应该在 1 （1.7所在的数量级）左右。这时输入较大，就意味着权值必须较小，一个较大，一个较小，两者相乘，就引起数值问题了。
    假如你的输入是421，你也许认为，这并不是一个太大的数，但因为有效权值大概会在1/421左右，例如0.00243，那么，在matlab里输入421x0.00243 == 0.421x2.43，会发现不相等，这就是一个数值问题。

2. 求解需要
    
    a. 初始化：
    
    在初始化时我们希望每个神经元初始化成有效的状态，tansig函数在[-1.7, 1.7]范围内有较好的非线性，所以我们希望函数的输入和神经元的初始化都能在合理的范围内使得每个神经元在初始时是有效的。（如果权值初始化在[-1,1]且输入没有归一化且过大，会使得神经元饱和）
    
    b. 梯度：
    
    以输入-隐层-输出这样的三层BP为例，我们知道对于输入-隐层权值的梯度有2ew(1-a^2)*x的形式（e是误差，w是隐层到输出层的权重，a是隐层神经元的值，x是输入），若果输出层的数量级很大，会引起e的数量级很大，同理，w为了将隐层（数量级为1）映身到输出层，w也会很大，再加上x也很大的话，从梯度公式可以看出，三者相乘，梯度就非常大了。这时会给梯度的更新带来数值问题。
    
    c. 学习率：
    
    由（2）中，知道梯度非常大，学习率就必须非常小，因此，学习率（学习率初始值）的选择需要参考输入的范围，不如直接将数据归一化，这样学习率就不必再根据数据范围作调整。 隐层到输出层的权值梯度可以写成 2ea，而输入层到隐层的权值梯度为 2ew(1-a^2)x ，受 x 和 w 的影响，各个梯度的数量级不相同，因此，它们需要的学习率数量级也就不相同。对w1适合的学习率，可能相对于w2来说会太小，若果使用适合w1的学习率，会导致在w2方向上步进非常慢，会消耗非常多的时间，而使用适合w2的学习率，对w1来说又太大，搜索不到适合w1的解。如果使用固定学习率，而数据没归一化，则后果可想而知。
    
    d.搜索轨迹：前面的图已解释

你可以回去看一下这个链接：https://www.jianshu.com/p/95a8f035c86c

# 正确的预测房屋价格的方法

木头：在正文中，咱们用训练出来的模型预测房屋价格，还需要先还原W和B的值，我觉得有些有点儿麻烦，有没有别的更好的办法？

铁柱：在线性问题中，我们还能够还原W和B的值，在非线性问题中，我们可能根本做不到这一点。所以，咱们可以把需要预测的数据，也先做归一化，送进模型中做推理计算。

木头：哦！对啊！因为我们训练时并没有把样本标签值做改变，所以预测的值应该还是有效的，不会脱离样本标签值的范围。

铁柱：那你来看看应该怎么做？

木头：（劈里啪啦敲键盘）...（铁柱听着机械键盘的声音皱了皱眉）...（10分钟后）......我写了段代码，您给看看：
```Python
# normalize data by specified range and min_value
def NormalizeByRange(X, x_range, x_min):
    X_new = np.zeros(X.shape)
    n = X.shape[0]
    for i in range(n):
        x = X[i,:]
        x_new = (x-x_min[0,i])/x_range[0,i]
        X_new[i,:] = x_new
    return X_new

# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)
    print("z1=", z1)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)
    print("z2=", z2)
```
我先增加了一个函数，NormalizeByRange，根据样本训练时的样本归一化方法，同样地处理一下预测样本xt，得到归一化后的xt_new，然后用训练出来的W/B的值来计算，得到z2，结果是这样的：
```
35 800 [[  5.99999997 -40.00000003 394.99999991]] [[292.00000008]] 19.37525512595306
35 900 [[  5.99999997 -40.00000003 394.99999992]] [[292.00000007]] 21.662740071130102
36 0 [[  5.99999998 -40.00000003 394.99999992]] [[292.00000007]] 172.2871446916127
36 100 [[  5.99999998 -40.00000003 394.99999993]] [[292.00000006]] 337.24697163103997
W= [[  5.99999998 -40.00000003 394.99999993]]
B= [[292.00000006]]
W_real= [[  1.99999999 -10.00000001   5.        ]]
B_real= [[110.00000012]]
z1= [[528.99999999]]
z2= [[528.99999999]]
```
可以看到z1和z2的预测值是相同的，两种方法都有效，但第二种方法看起来比较容易理解。

# 对标签值归一化

铁柱：木头不错哦！

木头：一般一般，年级第三！

铁柱：吼吼，还挺押韵。再给你提个问题，你有没有注意，在计算Loss值时，会达到172.287，337.246这样大的数值，是什么原因呢？我们第四章的Loss值可都是小于1的数。

木头：哦！啊！这个......（喝了一口82年的雪碧压压惊）......我想想啊......

铁柱：心动不如行动，想什么想，动手！

木头：吼的！......（劈里啪啦敲键盘）.......（铁柱听着那机械键盘的声音觉得闹得慌）......（20分钟后）......老师，我把PredicateTest函数改了一下：

```Python
# try to give the answer for the price of 朝西(2)，五环(5)，93平米的房子
def PredicateTest(W_real, B_real, W, B, flag):
    xt = np.array([2,5,93]).reshape(3,1)
    z1 = ForwardCalculation(xt, W_real, B_real)

    xt_new = NormalizeByRange(xt, X_range, X_min)
    z2 = ForwardCalculation(xt_new, W, B)

    if flag == 'x_only':
        print("xt,W_real,B_real:",z1)
        print("xt_new,W,B:",z2)
    elif flag == 'x_and_y':
        print("xt,W_real,B_real:", z1*Y_range+Y_min)
        print("xt_new,W,B:", z2*Y_range+Y_min)
```
铁柱：哦？当flag=='x_and_y'时，为什么z1,z2要做这样的转换呢？

木头：flag='x_and_y'，意思是在训练时我们对Y值也做了归一化，所以，我想试一下反归一化，因为：

$$y_{new} = \frac{y-y_{min}}{y_{max}-y_{min}} = \frac{y-y_{min}}{y_{range}}$$

所以：

$$y = y_{new}*y_{range}+y_{min}$$

然后，把主程序第一行的flag值改成：
```Python
flag = 'x_and_y'
```
运行......结果如下：
```
15 900 [[ 0.0137157  -0.09154976  0.90384324]] [[0.08242129]] 0.00011344288067411854
16 0 [[ 0.01371643 -0.09154922  0.90384595]] [[0.08241911]] 0.0009021195541084537
16 100 [[ 0.01371701 -0.0915483   0.90384837]] [[0.08241596]] 0.0017657981568301397
W= [[ 0.01371698 -0.09154826  0.90384839]]
B= [[0.08241599]]
W_real= [[ 0.00457233 -0.02288706  0.01144112]]
B_real= [[-0.33402734]]
xt,W_real,B_real: [[528.99654603]]
xt_new,W,B: [[528.99670957]]
```
哈哈！82年的雪碧果然厉害！我们只迭代了16轮就结束了，而且得到了正确的预测值！

铁柱：不错不错！我们来总结一下正确的归一化和反归一化的关系：

|归一化|Weight|Bias|预测值Xt|预测方法|
|---|---|---|---|---|
|只归一化X|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$|
|只归一化X|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$|
|同时归一化X和Y|反归一化为$W_{Real}$|反归一化为$B_{Real}$|不归一化$Xt$|$Z=W_{Real} * Xt + B_{Real}$<br>$Z'=Z * Y_{range} + Y_{min}$|
|同时归一化X和Y|直接使用训练结果W|直接使用训练结果B|归一化为$Xt_{new}$|$Z=W * Xt_{new} + B$ <br> $Z'=Z * Y_{range}+Y_{min}$|

木头：哦！这样一总结，就可以一目了然啦！我也简单总结一下：

1. X必须归一化，否则无法训练
2. 训练出的结果W和B，在推理时有两种使用方式：
a. 直接使用，此时必须把推理时输入的X也做相同规则的归一化
b. 反归一化为W,B的本来值$W_{Real},B_{Real}$，推理时输入的X不需要改动
3. Y可以归一化，好处是迭代次数少。如果结果收敛，也可以不归一化，如果不收敛（数值过大），就必须归一化
4. 如果Y归一化，先沿袭第2步的做法，对得出来的结果做关于Y的反归一化

铁柱：可以看到对标签值的归一化确实给我们带来了好处，迭代次数减少了一倍多。其实，咱们这个网络还很简单，这样折腾一下是为了让大家有深刻的理解，真正复杂的神经网络有个理论叫做"Batch Normalization"——批标准化，以后咱们遇到时再讲。

木头：好啊好啊！
