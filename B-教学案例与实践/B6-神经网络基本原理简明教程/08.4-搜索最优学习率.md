Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 搜索最优学习率

## 挑战

前面章节学习过，普通梯度下降法，包含三种形式：

1. 单样本
2. 全批量样本
3. 小批量样本

我们通常把1和3统称为SGD(Stochastic Gradient Descent)。当批量不是很大时，全批量也可以纳入此范围。大的含义是：十万级以上的数据量。

使用梯度下降的这些形式时，我们通常面临以下挑战：

1. 很难选择出合适的学习率
   
   太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
   
2. 相同的学习率并不适用于所有的参数更新
   
   如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
   
3. 避免陷于多个局部最小值中。
   
   实际上，问题并非源于局部最小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。

<img src=".\Images\13\saddle_point.png" width="400">

比如我们在本章前面的实例结果，Loss值随迭代次数而下降的历史记录图是这样的：

<img src=".\Images\13\eta_01_loss.png" width="600">

为什么在3000至6000个epoch之间，有很大一段平坦地段，Loss值并没有显著下降？这其实也体现了这个问题的实际损失函数的形状，在这一区域上梯度比较平缓，以至于梯度下降算法并不能找到合适的突破方向寻找最优解，而是在原地转腰子（徘徊）。这一平缓地区就是损失函数的驻点或者鞍点。

## 学习率的选择

我们前面一直使用固定的学习率，比如0.1或者0.05。这是因为在接近极小点时，损失函数的梯度也会变小，因此不会担心越过极小点。保证SGD收敛的充分条件是：

$$\sum_{k=1}^\infty \eta_k = \infty，且： \sum_{k=1}^\infty \eta^2_k < \infty$$ 

   
实践中，有必要随之迭代次数的增加而逐渐降低学习率。

下图是不同的学习率的选择对损失函数与迭代次数的比值：

<img src=".\Images\13\learning_rate.png">

- 黄色：学习率太大，loss值增高，网络发散
- 绿色：学习率可以使网络收敛，但值较大，loss值徘徊不降
- 蓝色：学习率值太小，loss值下降速度慢，训练次数长，收敛慢
- 红色：正确的学习率设置

## 如何找到最佳学习率

木头：在第三节里，我们已经知道了学习率对的影响，结论是学习率在0.7左右效果最好，只是基于遍历的方法搜索到的，很费时费力。有什么通用的办法可以帮助我们吗？

铁柱：好问题！学习率是“灰常灰常”重要的一个参数，所以很多人研究了它。Leslie N. Smith 在2015年的一篇论文[Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)中的3.3节描述了一个非常棒的方法来找初始学习率，同时推荐大家去看看这篇论文，有一些非常启发性的学习率设置想法。

这个方法在论文中是用来估计网络允许的最小学习率和最大学习率，我们也可以用来找我们的最优初始学习率，方法非常简单。首先我们设置一个非常小的初始学习率，比如1e-5，然后在每个batch之后都更新网络，同时增加学习率，统计每个batch计算出的loss。最后我们可以描绘出学习的变化曲线和loss的变化曲线，从中就能够发现最好的学习率。

下面就是随着迭代次数的增加，学习率不断增加的曲线，以及不同的学习率对应的loss的曲线（理想中的曲线）。

|随着迭代次数增加学习率|观察Loss值与学习率的关系|
|---|---|
|<img src=".\Images\13\lr-select-1.jpg">|<img src=".\Images\13\lr-select-2.jpg">|

从有图可以看到，学习率在0.3左右表现最好，再大就有可能发散了。我们把这个方法用于到我们的代码中试一下是否有效。

首先，设计一个数据结构，做出如下这张表：

|学习率段|0.0001~0.0009|0.001~0.009|0.01~0.09|0.1~0.9|1.0~1.1|
|----|----|----|----|---|---|
|步长|0.0001|0.001|0.01|0.1|0.01|
|迭代|10|10|10|10|10|

对于每个学习率段，在每个点上迭代10次，然后：

$$当前学习率+步长=>下一个学习率$$

以第一段为例，会在0.0001迭代10次，在0.0002上迭代50次，......，在0.0009上迭代50次，然后进入第二段。步长和迭代次数可以分段设置。代码如下：

```Python
def try_1():
    # try 1    
    lr_Searcher = LearningRateSearcher()
    looper = CLooper(0.0001,0.0001,50)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.001,0.001,50)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.01,0.01,50)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.1,0.1,50)
    lr_Searcher.addLooper(looper) 
    looper = CLooper(1,0.01,50,1.2)
    lr_Searcher.addLooper(looper) 
    return lr_Searcher
```

使用时，首先继承TwoLayerNet，生成一个新类：LrSeekingNet，修改其中的train方法和UpdateWeights方法，加入控制修改learning rate的逻辑代码，在主程序中调用try_1()方法得到返回的lr_Searcher实例，传入train()方法中，得到的图示如下：

<img src=".\Images\13\LR_try_1.png">

横坐标用了np.log10()函数来显示对数值。

好像并不理想，前面一大段都是在下降，说明0.0001(对应坐标值-4)，0.001(对应坐标值-3)，0.01(对应坐标值-2)，都太小了。那我们就继续探查0.5以后的段：

```Python
def try_2():
    # try 2
    lr_Searcher = LrLooper()
    # 从0.5开始，步长0.01，每个步长上迭代50次，到1.0为止
    looper = Looper(0.5,0.01,50,1.0)
    lr_Searcher.addLooper(looper)
    return lr_Searcher
```

<img src=".\Images\13\LR_try_2.png">

到-0.1时（对应学习率0.79）开始，损失值上升，所以合理的学习率应该是0.7左右，于是我们可以用0.6，0.7，0.8去做试验了。

代码位置：ch10, Level3
