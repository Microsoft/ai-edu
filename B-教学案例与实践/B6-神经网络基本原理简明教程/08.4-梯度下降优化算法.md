Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 梯度下降优化算法

前面章节学习过，普通梯度下降法，包含三种形式：

1. 单样本
2. 批量样本
3. 小批量样本

我们通常把1和3统称为SGD(Stochastic Gradient Descent)。

使用梯度下降的这些形式时，我们通常面临以下挑战：

1. 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
2. 此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
3. 在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。

<img src=".\Images\8\saddle_point.png" width="400">

## 学习率的选择


关键参数有两个：

1. 批大小
   
   通过抽取m个独立分布的小批量样本训练，通过计算他们的梯度均值，可以得到梯度的无偏差估计。4.4章中说明了选择批量大小的一些考虑。

2. 学习率
   
    我们前面一直使用固定的学习率，比如0.1或者0.05。这是因为在接近极小点时，损失函数的梯度也会变小，因此不会担心越过极小点。保证SGD收敛的充分条件是：

$$\sum_{k=1}^\infty \eta_k = \infty，且： \sum_{k=1}^\infty \eta^2_k < \infty$$ 

   
实践中，有必要随之迭代次数的增加而逐渐降低学习率。

下图是不同的学习率的选择对损失函数与迭代次数的比值：

<img src=".\Images\8\learning_rate.jpg">

- 黄色：学习率太大，loss值增高，网络发散
- 绿色：学习率可以使网络收敛，但值较大，loss值徘徊不降
- 蓝色：学习率值太小，loss值下降速度慢，训练次数长，收敛慢
- 红色：正确的学习率设置


## 动量 Momentum

SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：

Momentum算法会观察历史梯度，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度。若当前梯度与历史梯方向不一致，则梯度会衰减。

一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。

$$v_{t0} = 0$$
$$v_1 = \gamma \cdot v_{0} - \eta \cdot \nabla J_{\theta}(\theta)$$
$$\theta = \theta + v_1$$
$$v_t = \gamma \cdot v_{t-1} - \eta \cdot \nabla J_{\theta}(\theta)$$
$$\theta = \theta + v_t$$
$$ = \theta - \eta \cdot \nabla J_{\theta}(\theta)+ \gamma \cdot v_{t-1}$$

|SGD|Momentum|
|---|---|
|<img src=".\Images\8\SGD.png">|<img src=".\Images\8\Momentum.png">|

从上图的比较可以看到，使用同等的条件：
- 终止条件：loss = 0.001
- batch_size = 10
- eta = 0.1
- 隐层神经元数量ne=4

左侧的普通梯度下降法，epoch=25280次停止，而右侧的动量法经过2243次迭代结束。原因是在左侧的图中，中间有一大段比较平坦的区域，梯度值很小，或者是随机梯度下降算法找不到合适的方向前进，只能慢慢搜索。而右侧的动量法，利用惯性，判断当前梯度与上次梯度的关系，如果方向相同，则会加速前进；如果不同，则会减速，并趋向平衡。

## NAG - Nesterov Accelerated Gradient

在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好。

$$v_t = \gamma v_{t-1} - \eta \nabla J(\theta - \gamma v_{t-1})$$
$$\theta = \theta + v_t$$

其核心思想是：注意到 momentum 方法，如果只看 γ * v 项，那么当前的θ经过momentum的作用会变成 θ-γ * v。因此可以把 θ-γ * v这个位置看做是当前优化的一个”展望”位置。所以，可以在 θ-γ * v求导, 而不是原始的θ。

同带动量的SGD相比，梯度不是根据当前位置θ计算出来的，而是在移动之后的位置$\theta - \gamma v_{t-1}$计算梯度。理由是，既然已经确定会移动$\theta - \gamma v_{t-1}$，那不如之前去看移动后的梯度。

这个改进的目的就是为了提前看到前方的梯度。如果前方的梯度和当前梯度目标一致，那我直接大步迈过去； 如果前方梯度同当前梯度不一致，那我就小心点更新。

|Momentum|NAG|
|---|---|
|<img src=".\Images\8\Momentum.png">|<img src=".\Images\8\NAG.png">|

从上图的比较可以看出，NAG比动量法还要快一些，只迭代了1959次就到达loss=0.001的停止条件。

下图是二者的前进方向比较：

<img src=".\Images\8\NesterovMomentum.jpg">

蓝色线表示正常的带动量SGD的两次移动； 棕色线是计划移动的量$\gamma v_{t-1}$; 红色线是在移动后位置计算的移动量； 棕色线和红色线的合并效果就是绿色线NAG。

<img src=".\Images\8\sgd_m_nag.png">
