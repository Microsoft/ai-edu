Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 用数学概念理解反向传播

## 线性的例子

我们再用一个纯数学的例子来说明反向传播的概念。

假设我们有一个函数 $z = x * y，其中: x = w * 2 + b, y = b + 1，即: z = (w * 2 + b) * (b + 1)$

关系如下图：

<img src=".\Images\2\flow1.png" width="800">

注意这里x, y, z不是变量，w, b是才变量，因为在神经网络中，我们要最终求解的是w和b的值，x,y,z只是样本值。

当w = 3, b = 4时，会得到如下结果

<img src=".\Images\2\flow2.png" width="800">

最终的z值，受到了前面很多因素的影响：变量w，变量b，计算式x，计算式y。常数是个定值，不考虑。目前的z=50，如果我们想让z变大一些，w和b应该如何变化呢？

我们从z开始一层一层向回看，图中各节点关于变量b的偏导计算结果如下图：

<img src=".\Images\2\flow3.png" width="800">

因为z = x * y，其中x = w * 2 + b，y = b + 1
所以：

$$\frac{\partial{z}}{\partial{b}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}=5 \times 1+10 \times 1=15$$

其中：

$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=y=5$$
$$\frac{\partial{z}}{\partial{y}}=\frac{\partial{}}{\partial{y}}(x \cdot y)=x=10$$
$$\frac{\partial{x}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(w \cdot 2+b)=1$$
$$\frac{\partial{y}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(b+1)=1$$

有一个很有趣的问题是：z = x * y = 10 * 5 = 50，表面看起来x=10，y=5，似乎x对z的贡献较大。那么x的微小变化和y的微小变化对z来说，哪一个贡献大呢？

我们假设只有x变化时，△x = 0.1, 则z = (x + △x) * y = 10.1 * 5 = 50.5

我们再假设只有y变化时，△y = 0.1, 则z = x * (y +△y) = 10 * 5.1 = 51

50.5 < 51，说明y的微小变化对z的贡献比较大，这个从

$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=5 < \frac{\partial{z}}{\partial{y}}=\frac{\partial{}}{\partial{y}}(x \cdot y)=10$$

和这两个值的比较来看也可以证明。而△x和△y就可以理解为梯度值。

同理，我们也可以得到图中各变量对w的偏导值：

<img src=".\Images\2\flow4.png" width="800">

从以上两图可以看出，反向微分保留了所有变量（包括中间变量）对结果z的影响。若z为误差函数，则对图进行一次计算，可以得到所有节点对z的影响，即梯度值，下一步就可以利用这些梯度值来更新w和b的权重。

w的变化和b的变化，哪一个对z的变化贡献大？从图中还可以注意到：

$$\frac{\partial{z}}{\partial{b}}=15$$
$$\frac{\partial{z}}{\partial{w}}=10$$

所以每次w和b的变化值是不相同的，b的变化会比w大一些，也就是每一步的跨度大一些，这个是与z = x*y = (w*2+b)*(b+1)这个算式相关的，并不代表神经网络中实际情况。

# 反向传播的实际计算过程（单变量）

还是用上面的例子，目前：

- $w = 3$
- $b=4$
- $x = w*2+b = 10$
- $y = b+1 = 5$
- $z = x*y=50$

假设我们最终的目的想让z = 60，只改变b的值，如何实现？
答案就是偏导数：

$$\frac{\partial{z}}{\partial{b}}=\frac{\Delta{z}}{\Delta{b}}=15$$

目前z=50, 距离60相差10，所以我们令$\Delta{z}=60-50=10$（**这里的减法实际就是损失函数的概念**），则：

$$
\frac{\Delta{z}}{\Delta{b}}=15=\frac{10}{\Delta{b}} \\
$$

所以:

$$\Delta{b} = 0.66667$$

再带入式子中（顺便说一句，下面这个计算过程就叫做**前向计算**）

- $w = 3$
- $b=4+0.66667=4.66667$
- $x = w*2+b = 10.66667$
- $y = b+1 = 5.66667$
- $z = x*y=10.66667*5.66667=60.4445$

一下子超过60了，咋办？再来一次（下面的过程就叫做**反向传播**）：

我们令$\Delta{z}=60-60.4445=-0.4445$，则：

$$
\frac{\Delta{z}}{\Delta{b}}=15=\frac{-0.4445}{\Delta{b}} \\
$$

所以:

$$\Delta{b} = -0.02963$$

再带入式子中：

- $w = 3$
- $b=4.666667-0.02963=4.63704$
- $x = w*2+b = 10.63704$
- $y = b+1 = 5.63704$
- $z = x*y =10.63704*5.63704=59.96$


咦哈！十分接近59.96了！再迭代几次，应该可以近似等于60了，直到误差不大于0.00001时，我们就可以结束迭代了，对于计算机来说，这些运算的执行速度很快。

有的同学会说了：这个问题不是用数学公式倒推求解一个二次方程，就能直接得到准确的b值吗？是的！但是我们是要说明机器学习的方法，机器并不会解二次方程，而且很多时候不是用二次方程就能解决实际问题的。而上例所示，是用**机器所擅长的迭代计算的方法**来不断逼近真实解，这就是机器学习的真谛！而且这种方法是普遍适用的。



# 非线性的例子
- x是输入
- 第一层：$a=x^2$
- 第二层：$b=\ln (a)$
- 第三层：$c=\sqrt{b}$
- 假设我们想最后得到c=2的值，问：x应该是多少？

$$c=\sqrt{b}=\sqrt{\ln(a)}=\sqrt{\ln(x^2)}=\sqrt{2\ln(x)}$$

我们给定一个初始值x=2，依次计算得到：
- a = 4
- b = 1.386
- c = 1.177



# 参考资料
http://colah.github.io/posts/2015-08-Backprop/
