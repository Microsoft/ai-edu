Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 用数学概念理解反向传播

## 线性的例子

我们再用一个纯数学的例子来说明反向传播的概念。

假设我们有一个函数：
$$z = x * y$$
其中: 
$$x = 2w + 3b$$
$$y = 2b + 1$$
即: 
$$z = (2w + 3b) * (2b + 1)$$

计算图如下：

<img src=".\Images\2\flow1.png">

注意这里x, y, z不是变量，w, b是才变量，因为在神经网络中，我们要最终求解的是w和b的值，x,y,z只是样本值。

当w = 3, b = 4时，会得到如下结果

<img src=".\Images\2\flow2.png">

最终的z值，受到了前面很多因素的影响：变量w，变量b，计算式x，计算式y。常数是个定值，不考虑。目前的z=162，如果我们想让z变小一些，比如150，w和b应该如何变化呢？

我们从z开始一层一层向回看，图中各节点关于变量b的偏导计算结果如下：

$$因为z = x * y，其中x = 2w + 3b，y = 2b + 1$$
所以：

$$\frac{\partial{z}}{\partial{b}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}=y \cdot 3+x \cdot 2=9 \cdot 3+18 \cdot 2=63$$

其中：

$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=y=9$$
$$\frac{\partial{z}}{\partial{y}}=\frac{\partial{}}{\partial{y}}(x \cdot y)=x=18$$
$$\frac{\partial{x}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(2w+3b)=3$$
$$\frac{\partial{y}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(2b+1)=2$$

<img src=".\Images\2\flow3.png">

同样可以计算出z对w偏导数：
$$\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}=y \cdot 2=18$$

从上图可以看出，反向微分保留了所有变量（包括中间变量）对结果z的影响。若z为误差函数，则对图进行一次计算，可以得到所有节点对z的影响，即梯度值，下一步就可以利用这些梯度值来更新w和b的权重。

w的变化和b的变化，哪一个对z的变化贡献大？从图中还可以注意到：

$$\frac{\partial{z}}{\partial{b}}=63$$
$$\frac{\partial{z}}{\partial{w}}=18$$

所以每次w和b的变化值是不相同的，b的变化会比w大很多。

### 反向传播的实际计算过程（单变量）

还是用上面的例子，目前：

- $w = 3$
- $b=4$
- $x = 2w+3b = 18$
- $y = 2b+1 = 9$
- $z = x*y=162$

假设我们最终的目的想让z = 150，**只改变b的值**，如何实现？答案就是偏导数：

$$\frac{\partial{z}}{\partial{b}}=\frac{\Delta{z}}{\Delta{b}}=63$$

所以我们令$\Delta{z}=162-150=12$（**这里的减法实际就是损失函数的概念**），则：

$$
\frac{\Delta{z}}{\Delta{b}}=63=\frac{12}{\Delta{b}} \\
$$

所以:

$$\Delta{b} = 12/63=0.19$$

再带入式子中（顺便说一句，下面这个计算过程就叫做**前向计算**）

- $w = 3$
- $b=4-0.19=3.81$
- $x = 2w+3b = 2*3+3*3.81=17.43$
- $y = 2b+1 = 2*3.81+1=8.62$
- $z = x*y=17.43*8.62=150.246$

咦哈！十分接近150了！再迭代几次，应该可以近似等于150了，直到误差不大于0.00001时，我们就可以结束迭代了，对于计算机来说，这些运算的执行速度很快。

这个问题用数学公式倒推求解一个二次方程，就能直接得到准确的b值吗？是的！但是我们是要说明机器学习的方法，机器并不会解二次方程，而且很多时候不是用二次方程就能解决实际问题的。而上例所示，是用**机器所擅长的迭代计算的方法**来不断逼近真实解，这就是机器学习的真谛！而且这种方法是普遍适用的。

### 反向传播的实际计算过程（双变量）

这次我们要同时改变w和b，到达最终结果为150的目的。
已知$\Delta z=12$，我们不妨把这个误差的一半算在w账上，另外一半算在b的账上：
$$
\Delta b=\frac{\Delta z / 2}{63} = \frac{12/2}{63}=0.095
$$$$
\Delta w=\frac{\Delta z / 2}{18} = \frac{12/2}{18}=0.333
$$

- $w = w-\Delta w=3-0.333=2.667$
- $b = b - \Delta b=4-0.095=3.905$
- $x=2w+3b=2*2.667+3*3.905=17.049$
- $y=2b+1=2*3.905+1=8.81$
- $z=x*y=17.049*8.81=150.2$


## 非线性的例子

在上面的例子中，我们可以发现，误差一次性地传递给了初始值w和b，因为无论中间计算过程有多么复杂，它都是线性的，所以可以一次传到底。缺点是这种线性的组合最多只能解决线性问题，不能解决更复杂的问题。这个我们在神经网络基本原理中已经阐述过了，需要有激活函数连接两个线性单元。

下面我们看一个非线性的例子。

##游戏

5个人，分别代表x,a,b,c,y，其中$1<x<=10，0<y<2.15$


1. 第1个人，输入层
正向：随机输入第一个x值，x取值范围(1,10]，假设第一个数是2
反向，第2个人传回$\Delta x，更新x：x = x - \Delta x$，再次正向
2. 第2个人，第一层网络计算
正向，第1个人传入x的值，计算：$a=x^2$
反向，第3个人传回$\Delta a，计算\Delta x：\Delta x = \Delta a / 2x$
3. 第3个人，第二层网络计算
正向，第2个人传入a的值，计算b：$b=\ln (a)$
反向，第4个人传回$\Delta b，计算\Delta a：\Delta a = \Delta b \cdot a$
4. 第4个人，第三层网络计算
正向，第3个人传入b的值，计算c：$c=\sqrt{b}$
反向，第5个人传回$\Delta c，计算\Delta b：\Delta b = \Delta c \cdot 2\sqrt{b}$
5. 第5个人，输出层
正向，第4个人传入c的值
反向，计算y与c的差值：$\Delta c = y - c$，传回给第4个人

假设我们想最后得到c=2.13的值，问：x应该是多少？（误差小于0.001即可）

<img src=".\Images\2\game.png">


## 数学解析解
$$c=\sqrt{b}=\sqrt{\ln(a)}=\sqrt{\ln(x^2)}=\sqrt{2\ln(x)}=2.13 \\
2*\ln{x}=2.13*2.13=4.5369 \\
\ln{x}=4.5369/2=2.26854 \\
两侧取e的次方：e^{\ln{x}} = e^{2.26854} \\
x = 9.6653
$$

## 梯度迭代解

$$
\frac{da}{dx}=\frac{d(x^2)}{dx}=2x=\frac{\Delta a}{\Delta x} \tag{1}
$$$$
\frac{db}{da} =\frac{d(\ln{a})}{da} =\frac{1}{a} = \frac{\Delta b}{\Delta a} \tag{2}
$$$$
\frac{dc}{db}=\frac{d(\sqrt{b})}{db}=\frac{1}{2\sqrt{b}}=\frac{\Delta c}{\Delta b} \tag{3}
$$
因此得到如下一组公式，可以把最后一层$\Delta c$的误差一直反向传播给最前面的$\Delta x$，从而更新x值：
$$
\Delta c = y - c \tag{4}
$$$$
\Delta b = \Delta c \cdot 2\sqrt{b}  \tag{根据式3}
$$
$$
\Delta a = \Delta b \cdot a  \tag{根据式2}
$$
$$
\Delta x = \Delta a / 2x \tag{根据式1}
$$


我们给定一个初始值x=2，依次计算结果如下表：

|迭代|正向$x=x-\Delta x$|正向$a=x^2$|正向$b=\ln(a)$|正向$c=\sqrt{b}$|标签值y|反向$\Delta c = y - c$|反向$\Delta b = \Delta c \cdot 2\sqrt{b}$|反向$\Delta a = \Delta b \cdot a$|反向$\Delta x = \Delta a / 2x$|
|--|--|--|--|--|--|--|--|--|--|
|1|2|4|1.386|1.177|2.13|-0.953|-2.243|-8.973|-2.243|
|2|4.243|18.005|2.891|1.700|2.13|-0.430|-1.462|-26.314|-3.101|
|3|7.344|53.934|3.988|1.997|2.13|-0.133|-0.531|-28.662|-1.951|
|4|9.295|86.404|4.459|2.112|2.13|-0.018|-0.078|-6.698|-0.360|
|5|9.655|93.233|4.535|2.129|2.13

Python代码：
```Python
import numpy as np
import matplotlib.pyplot as plt

if __name__ == '__main__':
    x = 2
    y = 2.13
    while(True):
        print("forward...")
        # forward
        a = x*x
        b = np.log(a)
        c = np.sqrt(b)
        print(x,a,b,c)
        print("backward...")
        # backward
        loss = c - y
        if np.abs(loss) < 0.001:
            break
        delta_c = loss
        delta_b = delta_c * 2 * np.sqrt(b)
        delta_a = delta_b * a
        delta_x = delta_a / 2 / x
        x = x - delta_x
        print(delta_c, delta_b, delta_a, delta_x)

    print(x,a,b,c,loss)
    x = np.linspace(1,10)
    a = x*x
    b = np.log(a)
    c = np.sqrt(b)
    plt.plot(x,c)
    plt.show()
```
该组合函数图像：
<img src=".\Images\2\figure_5.png">

# 参考资料
http://colah.github.io/posts/2015-08-Backprop/
