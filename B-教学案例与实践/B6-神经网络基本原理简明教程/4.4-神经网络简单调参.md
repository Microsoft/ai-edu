Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

# 提高准确度

木头：老师，我还有一个疑问，列在下表中：
|方法|w|b|
|----|----|----|
|最小二乘法|1.9962|3.0054|
|梯度下降法|1.9182|3.0778|
|神经网络法|1.9182|3.0778|

因为我猜这个问题的原始值是可能是$w=2，b=3$。貌似神经网络的准确率不够啊？从下图来的神经网络的训练结果来看，红色线是斜着穿过蓝色点区域的，并没有在正中央。

<img src="./Images/4/SGD.png"/>

铁柱：咱们初次使用神经网络，一定有水土不服的地方。最小二乘法是数学方法，所以它的结果是可信的。梯度下降法和神经网络法，实际是一回事儿，只是梯度下降没有使用神经元模型而已。

所以，咱们一起看看如何调整这个神经网络的训练过程，也就是“调参”。一般来说，神经网络的训练，只跑一轮是不够的，会欠拟合。

# 梯度下降的三种方法

## 随机梯度下降 SDG(Stochastic Grident Descent)

每次随机使用一个样本进行训练，立刻更新一次梯度，然后随机抽取下一个样本，再次更新梯度，直到所有样本都使用一遍，叫做一个Epoch。可以进行多个Epoch。

- 优点：在开始时收敛速度快，梯度更新速度快。有可能获得全局最优解。
- 缺点：前后两个样本的左右可能会相互抵消，受单个样本的噪声影响大，每次迭代并不能一定向着最优方向发展。训练一段时间后收敛速度变慢。

<img src="./Images/4/SGD-Loss.png"/>
上图，迭代时，Loss值毛刺波动非常大，受样本差异影响。

<img src="./Images/4/SGD-Trace.png"/>
上图，梯度下降时，开始收敛较快，到后期波动较大，找不到准确的方向。


## 全批量梯度下降 Full-Batch Gradient Descent

每次使用全部数据集进行训练，更新一次梯度，叫做一个Epoch。可以进行多个Epoch。

- 优点：受单个样本的影响最小，一次计算全体样本速度快。
- 缺点：数据量较大时不能实现（内存限制），训练过程变慢。初始值不同，可能导致获得局部最优解，并非全局最优解。
<img src="./Images/4/FBGD-Loss.png"/>
上图，迭代时，Loss值没有毛刺波动。

<img src="./Images/4/FBGD-Trace.png"/>
上图，梯度下降时，直接了当达到最佳点。

## 小批量梯度下降 Mini-Batch Gradient Descent

当数据量较大，不能一次加载入内存进行计算时，可以选择一小部分样本进行训练，更新一次梯度，然后再选取另外一小部分样本进行训练，再更新一次梯度，直到所有样本都使用了一次。

- 优点：不受单样本噪声影响，训练速度较快。
- 缺点：batch size的数值选择很关键，会影响训练结果。

Batch Size：批大小，一次训练的样本数量。
Iteration：迭代，一次正向+一次反向。
Epoch：所有样本被使用了一次，叫做一个Epoch。

假设一共有样本1000个，batch size=20，则一个Epoch中，需要1000/20=50次Iteration才能训练完所有样本。
<img src="./Images/4/MBGD-Loss.png"/>
上图，迭代时，Loss值毛刺波动较小。

<img src="./Images/4/MBGD-Trace.png"/>
上图，梯度下降时，在接近中心时有小波动。


对比图如下：
|Name|Loss|W,B Trace|
|---|---|---|
|随机|<img src="./Images/4/SGD-Loss.png"/>|<img src="./Images/4/SGD-Trace.png"/>|
|全量|<img src="./Images/4/FBGD-Loss.png"/>|<img src="./Images/4/FBGD-Trace.png"/>|
|批量|<img src="./Images/4/MBGD-Loss.png"/>|<img src="./Images/4/MBGD-Trace.png"/>|


