Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 学习率的确定

前面章节学习过，普通梯度下降法，包含三种形式：

1. 单样本
2. 全批量样本
3. 小批量样本

我们通常把1和3统称为SGD(Stochastic Gradient Descent)。当批量不是很大时，全批量也可以纳入此范围。大的含义是：十万级以上的数据量。

使用梯度下降的这些形式时，我们通常面临以下挑战：

1. 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
2. 此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
3. 在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。

比如我们在本章前面的实例结果，Loss值随迭代次数而下降的历史记录图是这样的：

<img src=".\Images\8\SGD.png" width="600">

为什么在5000至20000个epoch之间，有很大一段平坦地段，Loss值并没有显著下降？这其实也体现了这个问题的实际损失函数的形状，在这一区域上梯度比较平缓，以至于梯度下降算法并不能找到合适的突破方向寻找最优解，而是在原地转腰子（徘徊）。这一平缓地区就是损失函数的驻点或者鞍点。

<img src=".\Images\8\saddle_point.png" width="400">

## 学习率的选择

关键参数有两个：

1. 批大小
   
   通过抽取m个独立分布的小批量样本训练，通过计算他们的梯度均值，可以得到梯度的无偏差估计。4.4章中说明了选择批量大小的一些考虑。

2. 学习率
   
    我们前面一直使用固定的学习率，比如0.1或者0.05。这是因为在接近极小点时，损失函数的梯度也会变小，因此不会担心越过极小点。保证SGD收敛的充分条件是：

$$\sum_{k=1}^\infty \eta_k = \infty，且： \sum_{k=1}^\infty \eta^2_k < \infty$$ 

   
实践中，有必要随之迭代次数的增加而逐渐降低学习率。

下图是不同的学习率的选择对损失函数与迭代次数的比值：

<img src=".\Images\8\learning_rate.jpg">

- 黄色：学习率太大，loss值增高，网络发散
- 绿色：学习率可以使网络收敛，但值较大，loss值徘徊不降
- 蓝色：学习率值太小，loss值下降速度慢，训练次数长，收敛慢
- 红色：正确的学习率设置

## 学习率的影响

学习率=0.05,0.1，设置learning_rate=0.05。

|learning_rate=0.05,iteraion=6513|learning_rate=0.1,iteration=983|
|---|---|
|<img src=".\Images\8\r128-1000-005.png">|<img src=".\Images\8\r128-1000.png">|

结论：学习率太小，需要更多的迭代次数才能到达近似的效果。但右上角的拟合效果也不错。

铁柱：这个试例里能调的参数也就这么多了，更多的参数调整方法咱们可以在以后的章节中来学习体会。

木头：老师，那两个能拐弯儿的拟合是怎么回事儿？是否是过拟合了呢？

铁柱：哦！这种情况不好说，因为是末端了，如果训练数据能再向右扩充一些，比如到1.2，那么这个拐弯儿是正确的。实际上对于我们的标准结果（example=1000, neuron=128, learning_rate=0.1, iteraion=983），如果迭代次数再多些，也可能出现拐弯儿的情况。你可以下去试一下。

木头：好呀好呀！其实在上面的例子中，学习率是0.05时，迭代了6513次，最上端已经拐弯儿了。中间层神经元是96时，也有拐弯儿的迹象了。


## 如何找到最佳学习率

Leslie N. Smith 在2015年的一篇论文[Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)中的3.3节描述了一个非常棒的方法来找初始学习率，同时推荐大家去看看这篇论文，有一些非常启发性的学习率设置想法。
这个方法在论文中是用来估计网络允许的最小学习率和最大学习率，我们也可以用来找我们的最优初始学习率，方法非常简单。首先我们设置一个非常小的初始学习率，比如1e-5，然后在每个batch之后都更新网络，同时增加学习率，统计每个batch计算出的loss。最后我们可以描绘出学习的变化曲线和loss的变化曲线，从中就能够发现最好的学习率。
下面就是随着迭代次数的增加，学习率不断增加的曲线，以及不同的学习率对应的loss的曲线。

|随着迭代次数增加学习率|观察Loss值与学习率的关系|
|---|---|
|<img src=".\Images\8\lr-select-1.jpg">|<img src=".\Images\8\lr-select-2.jpg">|

