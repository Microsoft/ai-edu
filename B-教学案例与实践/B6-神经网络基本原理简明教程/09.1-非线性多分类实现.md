Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 非线性多分类实现

## 定义神经网络结构

<img src='./Images/9/nn.png'/>

矩阵运算过程：
$$W_1^{8 \times 2} \cdot X^{2 \times 1} + B_1^{8 \times 1} => Z_1^{8 \times 1}$$
$$Sigmoid(Z1) => A_1^{10 \times 1}$$
$$W_2^{3 \times 8} \times A_1^{8 \times 1} + B_2^{3 \times 1} => Z_2^{3 \times 1}$$
$$Softmax(Z2) => A_2^{3 \times 1}$$

## 输入层

$$
X = \begin{pmatrix}
X_1 & X_2 \dots X_{1000} 
\end{pmatrix}
= \begin{pmatrix}
x_{1,1} & x_{2,1} \dots x_{1000,1} \\
x_{1,2} & x_{2,2} \dots x_{1000,2}
\end{pmatrix}
$$

我们的约定是每行是一个样本，每列一个样本的所有特征，这里是2个特征，第一个是x1，第二个是x2。比如$x_{2,1}$，表示第2个样本的第1个特征值。

### 样本标签数据

一般来说，在标记样本时，我们会用1，2，3这样的标记，来指明是哪一类。所以样本数据中是这个样子的：
$$
Y = 
\begin{pmatrix}
Y_1 & Y_2 \dots Y_{200}
\end{pmatrix}=
\begin{pmatrix}1 & 2 & 3 & \dots & 2\end{pmatrix}
$$

在有Softmax的多分类计算时，我们用下面这种等价的方式，俗称One-Hot，就是在一个向量中只有一个数据是1，其它都是0。

$$
Y = 
\begin{pmatrix}
Y_1 & Y_2 & \dots & Y_m
\end{pmatrix}=
\begin{pmatrix}
y_{1,1} & y_{2,1} \dots y_{132,1} \\
y_{1,2} & y_{2,2} \dots y_{132,2} \\
y_{1,3} & y_{2,3} \dots y_{132,3}
\end{pmatrix}=
\begin{pmatrix}
1 & 0 & 0 & \dots & 0 \\
0 & 1 & 0 & \dots & 1 \\
0 & 0 & 1 & \dots & 0
\end{pmatrix}
$$

## 权重矩阵W1/B1
$$
W1=\begin{pmatrix}
w_{1,1} & w_{1,2} \\
w_{2,1} & w_{2,2} \\
\dots \\
w_{8,1} & w_{8,2}
\end{pmatrix}
$$

B1的尺寸是8x1，行数永远和W一样，列数永远是1。

$$
B1=\begin{pmatrix}
b_{1,1} \\
b_{2,1} \\
\dots \\
b_{8,1}
\end{pmatrix}
$$


## 中间层（隐层）

定义8个神经元，当然在代码里要把这个数字写成参数可调的。

$$
Z1 = \begin{pmatrix}
z_{1,1} \\ 
z_{2,1} \\ 
\dots \\
z_{3,1} \end{pmatrix},
A1 = \begin{pmatrix}
a_{1,1} \\ 
a_{2,1} \\ 
\dots \\
a_{8,1} \end{pmatrix}
$$

其中，$Z1=W1 \cdot X+B1，A1=Sigmoid(Z1)$，大1表示第一层神经网络。

## 权重矩阵W2/B2

$$
W2=\begin{pmatrix}
w_{1,1} & w_{1,2} \dots w_{1,8}\\
w_{2,1} & w_{2,2} \dots w_{2,8} \\
w_{3,1} & w_{3,2} \dots w_{3,8}
\end{pmatrix}
$$

B2的尺寸是3x1，行数永远和W一样，列数永远是1。

$$
B2=\begin{pmatrix}
b_{1,1} \\
b_{2,1} \\
b_{3,1}
\end{pmatrix}
$$

## 输出层

输出层3个神经元，负责3个分类的求和输出，再加上一个Softmax分类函数：

$$Z2 = \begin{pmatrix}z_{1,1} \\ z_{2,1} \\ z_{3,1} \end{pmatrix}$$

$$A2 = Softmax(Z2) = \begin{pmatrix} a_{1,1} \\ a_{1,2} \\ a_{1,3} \end{pmatrix}$$

其中，$Z2=W2 \cdot A1+B2，A2=Softmax(Z2)$，大2表示第二层神经网络。

## 损失函数

对于多分类，我们使用交叉熵损失函数的多分类形式：

$$J(w,b) = -{1 \over m} \sum^m_{i=1}y_iln(a_i)$$


## 反向传播

在第7章中的多分类原理中，已经讲过了如何推导反向传播公式，咱们把结论拿过来复习一下。

计算图与反向传播过程如下图：

<img src='./Images/9/backward.png'/>

先求输出层的梯度，根据第7.4节中的公式8：

$$dZ2 = \frac{\partial{J}}{\partial{A2}} \frac{\partial{A2}}{\partial{Z2}} = A2-Y \tag{1}$$

后续的梯度求解与8.1节一样，只拷贝结论在这里：

$$dW2=(A2-Y)A1^T \tag{2}$$

$$dB2=A2-Y \tag{3}$$

$$dZ1=W2^T \cdot dZ2 \cdot A1 \cdot (1-A1) \tag{4}$$

$$dW1= dZ1 \cdot X^T \tag{5}$$

$$dB1= dZ1 \tag{6}$$

代码位置：ch09
