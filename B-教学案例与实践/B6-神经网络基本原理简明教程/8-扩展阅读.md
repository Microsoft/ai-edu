Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 参数调整

木头：老师，经常听人说起“调参”，很神秘的样子。

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

咱们的基准代码中的参数设置如下：
1. 隐藏层神经元数=128
2. 输入训练数据量=1000
3. 学习率=0.1
4. 损失函数值<=0.002，也就是能基本拟合住目标曲线的前提下

## 中间层神经元数量的影响

神经元数量=96,128，设置代码中的n_hidden=96或者128。

|neuron=96,iteraion=3966|neuron=128,iteration=983|
|---|---|
|<img src=".\Images\8\r96-1000.png">|<img src=".\Images\8\r128-1000.png">|

结论：基准神经元数为128，在96时，需要迭代3966次，才能达到和128个神经元近似的效果。但是请注意左图中右上角的拟合效果——红色拟合线拐弯了！

## 样本数量的影响

样本数据量=500,1000，设置loop=int(num_samples/2)。

|example=500,iteraion=1562|example=1000,iteration=983|
|---|---|
|<img src=".\Images\8\r128-500.png">|<img src=".\Images\8\r128-1000.png">|<img src=".\Images\8\128-500-1000-010.png" width="600">

结论：样本数据量小，需要更多次的迭代。

## 学习率的影响

学习率=0.05,0.1，设置learning_rate=0.05。

|learning_rate=0.05,iteraion=6513|learning_rate=0.1,iteration=983|
|---|---|
|<img src=".\Images\8\r128-1000-005.png">|<img src=".\Images\8\r128-1000.png">|

结论：学习率太小，需要更多的迭代次数才能到达近似的效果。但右上角的拟合效果也不错。

铁柱：这个试例里能调的参数也就这么多了，更多的参数调整方法咱们可以在以后的章节中来学习体会。

木头：老师，那两个能拐弯儿的拟合是怎么回事儿？是否是过拟合了呢？

铁柱：哦！这种情况不好说，因为是末端了，如果训练数据能再向右扩充一些，比如到1.2，那么这个拐弯儿是正确的。实际上对于我们的标准结果（example=1000, neuron=128, learning_rate=0.1, iteraion=983），如果迭代次数再多些，也可能出现拐弯儿的情况。你可以下去试一下。

木头：好呀好呀！其实在上面的例子中，学习率是0.05时，迭代了6513次，最上端已经拐弯儿了。中间层神经元是96时，也有拐弯儿的迹象了。
