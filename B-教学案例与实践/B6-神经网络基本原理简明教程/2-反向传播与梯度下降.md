Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

反向传播和梯度下降这两个词，第一眼看上去似懂非懂，不明觉厉。这两个概念是整个神经网络中的重要组成部分，是和误差函数/损失函数的概念分不开的。

神经网络训练的最基本的思想就是：先“蒙”一个结果，我们叫预测结果a，看看这个预测结果和事先标记好的训练集中的真实结果y之间的差距，然后调整策略，再试一次，这一次就不是“蒙”了，而是有依据地向正确的方向靠近。如此反复多次，一直到预测结果和真实结果之间相差无几，亦即|a-y|->0，就结束训练。

在神经网络训练中，我们把“蒙”叫做初始化，可以随机，也可以根据以前的经验给定初始值。即使是“蒙”，也是有技术含量的。

# 通俗地理解反向传播

举个通俗的例子，Bob拿了一支步枪，射击100米外的靶子。这支步枪没有准星，或者是准星有bug，或者是Bob眼神儿不好看不清靶子，或者是雾很大，或者风很大，或者由于木星的影响而侧向引力场异常......反正就是Bob很倒霉。

第一次试枪后，拉回靶子一看，弹着点偏左了，于是在第二次试枪时，Bob就会有意识地向右侧偏几毫米，再看靶子上的弹着点，如此反复几次，Bob就会掌握这支步枪的脾气了。下图显示了Bob的5次试枪过程：

<img src=".\Images\2\target1.png" width="500">

在有监督的学习中，需要衡量神经网络输出和所预期的输出之间的差异大小。这种误差函数需要能够反映出当前网络输出和实际结果之间一种量化之后的不一致程度，也就是说函数值越大，反映出模型预测的结果越不准确。

这个例子中，Bob预期的目标是全部命中靶子的中心，最外圈是1分，之后越向靶子中心分数是2，3，4分，正中靶心可以得10分。

- 每次试枪弹着点和靶心之间的差距就叫做误差，可以用一个误差函数来表示，比如差距的绝对值，如图中的红色线。
- 一共试枪5次，就是迭代/训练了5次的过程 。
- 每次试枪后，把靶子拉回来看弹着点，然后调整下一次的射击角度的过程，叫做反向传播。注意，把靶子拉回来看和跑到靶子前面去看有本质的区别，后者容易有生命危险，因为还有别的射击者。一个不恰当的比喻是，在数学概念中，人跑到靶子前面去看，叫做正向微分；把靶子拉回来看，叫做反向微分。
- 每次调整角度的数值和方向，叫做梯度。比如向右侧调整1毫米，或者向左下方调整2毫米。如图中的绿色矢量线。

上图是每次单发点射，所以每次训练样本的个数是1。在实际的神经网络训练中，通常需要多个样本，做批量训练，以避免单个样本本身采样时带来的误差。在本例中，多个样本可以描述为连发射击，假设一次可以连打3发子弹，每次的离散程度都类似，如下图所示：

<img src=".\Images\2\target2.png" width="500">

-  如果每次3发子弹连发，这3发子弹的弹着点和靶心之间的差距之和再除以3，叫做损失，可以用损失函数来表示。

那Bob每次射击结果和目标之间的差距是多少呢？在这个例子里面，用得分来衡量的话，就是说Bob得到的反馈结果从差9分，到差8分，到差2分，到差1分，到差0分，这就是用一种量化的结果来表示Bob的射击结果和目标之间差距的方式。也就是误差函数的作用。因为是一次只有一个样本，所以这里采用的是误差函数的称呼。如果一次有多个样本，就要叫做损失函数了。

其实射击还不这么简单，如果是远距离狙击，还要考虑空气阻力和风速，在神经网络里，空气阻力和风速可以对应到隐藏层的概念上。


# 通俗地理解损失函数

在各种材料中经常看到的中英文词汇有：误差，偏差，Error，Cost，Loss，损失，代价......意思都差不多，在本系列文章中，使用损失函数和Loss Function这两个词汇。

假设有一个黑盒子，这个样子：
<img src=".\Images\2\blackbox.png">

我们只能看到输入和输出的数值，看不到里面的样子，当输入1时，输出3.212，然后黑盒子有个大喇叭喊话：我需要输出值是4。然后我们试了试输入2，结果输出5.238，一下子比4大了很多。那么我们第一次的损失值是$3.212-4=-0.788$，而二次的损失值是$5.238-4=1.238$。这里，我们的损失函数就是一个简单的减法，用实际值减去目标值，但是它可以告诉你两个信息：1）方向，是大了还是小了；2）差值，是0.1还是1.1。这样就给了我们下一次猜的依据。

**损失**就是所有样本的**误差**的总和，亦即：$$损失 = \sum_i(误差_i)$$所以有时候损失函数可以和误差函数混用概念。

在黑盒子的例子中，我们如果说“某个样本的损失”是不对的，只能说“某个样本的误差”，如果我们把神经网络的参数调整到完全满足一个样本的输出误差为0，通常会令其它样本的误差变得更大，这样作为误差之和的损失函数值，就会变得更大。所以，我们通常会在根据某个样本的误差调整权重后，计算一下整体样本的损失函数值，来判定网络是不是已经训练到了可接受的状态。

在上面的例子中，我们使用单个样本进行训练。在批量样本训练中，实际写code时，经常会出现的问题是，忘记用样本误差之和除以样本数量，结果得到了比实际误差大很多的数值，造成网络训练结果波动。

最初的损失函数的概念，应该是从经济学引用过来的。比如一套新的营销策略，消费者是不是真的买账，还要拿到市场上去检验。市场就是个黑盒子，我们只能从最后的收支情况来判别营销策略的好坏：如果你挣钱了，就是好的策略；如果你赔钱了，有损失了，就是不合适的策略，损失越大，说明策略背离实际情况越多。


# 通俗地理解梯度下降

必须先有损失函数的概念，然后才能理解梯度下降。
比如两个人玩儿猜数的游戏，数字的范围是[1,50]：
甲：我猜5
乙：太小了
甲：50
乙：有点儿大
甲：30
乙：小了
......
甲：不玩儿了！

这个游戏里的损失函数是什么呢？就是“太小了”，“有点儿大”......感觉很不靠谱！因为这个所谓的损失函数给了两个信息：
1. 方向：大了或小了
2. 程度：“太”，“有点儿”，但是很模糊

如果用Bob射击的例子来说，过程是这样的：
1. 1环，偏左上45度
2. 6环，偏左上15度
3. 7环，偏左
4. 8环，偏左下15度
5. 10环

这里的损失函数也有两个信息：
1. 距离
2. 方向

**所以，梯度，是个矢量！** 它应该即告诉我们方向，又告诉我们数值。

以上两个例子比较简单，容易理解，我们把黑盒子再请出来：
<img src=".\Images\2\blackbox.png">

黑盒子这件事真正的意义并不是猜测当输入是多少时输出会是4。它的实际意义是：我们要破解这个黑盒子！于是，我们会有如下破解流程：
1. 记录下所有输入值和输出值：(1, 3.334), (1.1, 2.605), (1.2, 2.881), (2, 5.332),...其中，括号里前面的数叫输入值，后面的值叫标签值。可以列个表：

|输入|输出(标签)|
|--|--|
|1|2.334|
|1.1|2.605|
|1.2|2.881|
|2|5.332|

2. 搭建一个神经网络，给出初始权重值，我们先假设这个黑盒子的逻辑是：$z=x + x^2$
3. 输入1，假设得到输出为2，而实际的输出值是2.334，则误差值为$2.3-2.334=-0.034$，小了
4. 调整权重值，比如$z=1.5x+x^2$，再输入1.1，假设得到的输出为2.86，则误差值为$2.86-2.605=0.255$，大了
5. 调整权重值，比如$z=1.2x+x^2$再输入1.2......
6. 调整权重值，再输入2......
7. 所有样本遍历一遍，计算损失函数值
8. 依此类推，重复3，4，5，6过程，直到损失函数值小于一个指标，比如0.001，我们就可以认为网络训练完毕，黑盒子“破解”(实际是被复制)了

从上面的过程可以看出，如果误差值是正数，我们就把权重降低一些；如果误差值为负数，则升高权重。


# 总结

- 在正向预测完成后，损失函数为我们提供了计算损失的方法
- 梯度下降法是在损失函数曲线上向着损失最小的点靠近而指引了网络权重调整的方向
- 反向传播把损失值反向传给神经网络的每一层，让每一层都根据损失值反向调整权重，依然是梯度下降的概念
