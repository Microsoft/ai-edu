Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 线性二分类原理

此原理对线性和非线性二分类都适用。

## 分类函数

对率函数Logistic Function，本身是激活函数，又可以当作二分类的分类函数。

### 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

### 导数

$$a^{'}(z) = a(z)(1 - a(z))$$

### 输入值域

$$[-\infty, \infty]$$

### 输出值域

$$[0,1]$$

### 函数图像

<img src=".\Images\7\sigmoid.png">

### 使用方式

此函数实际上是一个概率计算，它把$[-\infty, \infty]$之间的任何数字都压缩到[0,1]之间，返回一个概率值。这就是它的工作原理。

训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Sigmoid后，输出一个[0,1]之间的预测值。我们假设这个样本的标签值为0（属于负类，另外一类是第1类属于正类），如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。

推理时，我们预先设定一个阈值，比如上图中的红线，我们设置阈值=0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反。

## 正向计算

下面的公式中，我们使用单样本来举例（下标有个小i），便于说明偏导数的推导。

### 神经网络计算

$$
z_i = wx_i+b  \tag{1}
$$

### 分类计算

$$
a_i={1 \over 1 + e^{-z_i}} \tag{2}
$$

### 损失函数计算

$$
J(w,b) = -{1 \over m} \sum^m_{i=1}y_i \ln (a_i)+(1-y_i) \ln (1-a_i) \tag{3}
$$

## 反向传播


对公式3求导：
$$
{\partial{J} \over \partial{a_i}}= -{1 \over m} \sum^m_{i=1} ({y_i \over a_i} - {1-y_i \over 1-a_i})=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \tag{4}
$$

对公式2求导：
$$
{\partial{a_i} \over \partial{z_i}}=a_i(1-a_i) \tag{5}
$$

用链式法则结合公式4,5：

$$
{\partial{J} \over \partial{z_i}}={\partial{J} \over \partial{a_i}}{\partial{a_i} \over \partial{z_i}}
$$

$$
=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \cdot a_i(1-a_i)
$$

$$
=-{1 \over m} \sum^m_{i=1}(y_i-a_i)
$$

$$
={1 \over m} \sum^m_{i=1}(a_i-y_i) \tag{6}
$$

至此，我们得到了z的误差，进一步向后传播给w和b，对公式1求导可知：

$$
{\partial{z_i} \over \partial{w}} = x_i \tag{7}
$$

$$
{\partial{z_i} \over \partial{b}} = 1 \tag{8}
$$

用链式法则结合公式6，7，8：

$$
{\partial{J} \over \partial{w}}={\partial{J} \over \partial{z_i}}{\partial{z_i} \over \partial{w}}={1 \over m} \sum^m_{i=1}(a_i-y_i)x_i \tag{9}
$$

$$
{\partial{J} \over \partial{b}}={\partial{J} \over \partial{z_i}}{\partial{z_i} \over \partial{b}}={1 \over m} \sum^m_{i=1}(a_i-y_i) \tag{10}
$$

## 神经网络的线性二分类工作原理

木头：老师，我有个问题！如下图所示，我们假设绿色方块为正类：标签值$y=1$，红色三角形为负类：标签值$y=0$。

<img src=".\Images\6\linear_binary_analysis.png">

从直观上理解，如果我们有一条直线，其公式为：$z = w \cdot x1+b$，如图中的虚线所示，则所有正类的样本的x2都大于z，而所有的负类样本的x2都小于z，那么这条直线就是我们需要的解决方案。用正例的样本来表示：
$$
x2 > z，即：x2 > w \cdot x1 + b \tag{11}
$$

那么神经网络用矩阵运算+分类函数+损失函数这么复杂的流程，其工作原理是什么呢？

铁柱：好问题！经典机器学习中的SVM确实就是用这种思路来解决这个问题的，即一个类别的所有样本在分割线的一侧，而负类样本都在线的另一侧。神经网络的正向公式是这样：

$$Z = w1 \cdot x1 + w2 \cdot x2 + b$$

$$A = sigmoid(Z) = {1 \over 1 + e^{-Z}}$$

<img src=".\Images\6\sigmoid_binary.png">

当A>0.5时，为正类。当A<0.5时，为负类。我们用正类来举例子：

$$A = sigmoid(Z) = {1 \over 1 + e^{-Z}} > 0.5$$

做公式变形，再两边取自然对数，可以得到：

$$Z > 0$$

即：
$$
Z = w1 \cdot x1 + w2 \cdot x2 + b > 0
$$
对上式做一下变形，把x2放在左侧，其他项放在右侧：
$$
x2 > - {w1 \over w2}x1 - {b \over w2} \tag{12}
$$
简化一下两个系数，令w'=-w1/w2，b'=-b/w2：
$$
x2 > w'x1 + b' \tag{13}
$$
比较一下公式11和13，
$$
x2 > w \cdot x1 + b \tag{11}
$$

一模一样！这就说明神经网络的数学原理和我们在二维平面上的直观感觉是相同的。由此，我们还得到了一个额外的收获，即：

$$w = w' = - w1 / w2 \tag{14}$$

$$b = b' = -b/w2 \tag{15}$$

我们可以使用神经网络计算出w1,w2,B以后，换算成w,b来在二维平面上画那根直线，来直观地判断神经网络的正确性。

## 对数几率回归 Logistic Regression

木头：哦！明白了，经过数学推导后可以知道，神经网络实际也是在做这样一件事：经过调整w和b的值，把所有正例的样本都归纳到大于0.5的范围内，所有负例都小于0.5。

铁柱：对，只说大于或者小于，无法做准确的量化计算，所以用一个对率函数来模拟。

木头：啊，说到对率函数，还有一个问题，它为啥叫做“对数几率”函数呢？从哪里看出是“对数”了？“几率”是什么意思呢？

铁柱：我们来看一下神经网络的前向计算公式......

$$z = wx+b  \tag{1}$$
$$a={1 \over 1 + e^{-z}} \tag{2}$$
组合二者：
$$a={1 \over 1 + e^{-(wx+b)}}$$
做公式变形：
$$e^{-(wx+b)}={1-a \over a}$$
两侧取对数，再乘以-1：
$$\ln {a \over 1-a} = wx+b \tag{12}$$

上式中，如果a是把样本x的预测为正例的可能性，那么1-a就是其负例的可能性，a/(1-a)就是正负例的比值，称为几率(odds)，反映了x作为正例的相对可能性，而对几率取对数就叫做对数几率(log odds, logit)。

木头：欸！公式12不就是我们前面学过的线性回归模型吗？原来线性分类也是用了类似的原理。

铁柱：是的，公式12就是用线性回归模型的预测结果来逼近样本分类的对数几率。这就是为什么它叫做逻辑回归(logistic regression)，但其实是分类学习的方法。这种方法的优点如下：

- 直接对分类可能性建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题
- 不仅预测出类别，而是得到了近似的概率，这对许多需要利用概率辅助决策的任务很有用
- 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解

参考：周志华老师的西瓜书


