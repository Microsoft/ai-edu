Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 多分类函数

此原理对线性和非线性多分类都适用。

## 分类函数 - Softmax

为什么叫做Softmax？

假设输入值是：[3,1,-3]，如果取max会变成：[1,0,0]，这符合我们的分类需要。但是max操作本身不可导，无法用在反向传播中，所以加了个"soft"来模拟max的行为。

### 公式

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}}
$$

上式中:
- $z_j$是对第 j 项的分类原始值（即矩阵运算的结果，假设j=1，上图中$z_1=w_{11}x_1+w_{12}x_2+b1$）
- $z_i$是参与分类计算的每个类别的原始值（即矩阵运算的结果，上图中$z_i=w_{i1}x_1+w_{i2}x_2+bi$）
- m 是总的分类数
- $a_j$是对第 j 项的计算结果

假设j=1，m=3，上式为：
  
$$a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}$$

<img src=".\Images\7\Loss-A-Z.jpg">

也就是说把接收到的输入归一化成一个每个分量都在$(0,1)$之间并且总和为一的一个概率函数。在上图中，$A1+A2+A3=1$。

分类的方式是：看A1/A2/A3三者谁的比重最大，就算是那一类。比如A1=0.6, A2=0.1, A3=0.3，则这个样本就算是第一类了。

用一张图来形象说明这个过程：

<img src=".\Images\7\softmax.png">

当输入的数据$[z_1,z_2,z_3]$是$[3,1,-3]$时，按照图示过程进行计算，可以得出输出的概率分布是$[0.88,0.12,0]$。

试想如果我们并没有这样一个softmax的过程而是直接根据[3，1，-3]这样的输出，而我们期望得结果是[1，0，0]这样的概率分布结果，那传递给网络的信息是什么呢？

其实[3,1,-3]这一组数字和[1,0,0]是不可比的，就好像用1公里的“1”和1毫米的“1”去比，虽然它们都是长度单位。它先归一化所有输入数据到[0,1]之间，然后再和标签值[1,0,0]去比，一是可比了，二是知道要抑制或鼓励的幅度。

从继承关系的角度来说，softmax函数可以视作sigmoid的一个扩展，比如我们来看一个二分类问题，

$$
A_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}}
$$

是不是和sigmoid的函数形式非常像？比起原始的sigmoid函数，softmax的一个优势是可以用在多分类的问题中。另一个好处是在计算概率的时候更符合一般意义上我们认知的概率分布，体现出物体属于各个类别相对的概率大小。

## 正向计算

### 神经网络计算

$$
z_i = wx_i+b  \tag{1}
$$

### 分类计算

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} \tag{2}
$$

### 损失函数计算

$$J(w,b) =- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \log a_{ij} \tag{3}$$

m是样本数，n是类别数。在交叉熵函数一节有详细介绍。

## 反向传播

### Softmax函数自身的求导

由于Softmax涉及到求和，所以有两种情况：

- 求输出项$a_1$对输入项$z_1$的导数，此时：$j=1, i=1, i=j$，可以扩展到i, j为任意相等值
- 求输出项$a_2或a_3$对输入项$z_1$的导数，此时：$j=2或3, i=1, i \neq j$，可以扩展到i, j为任意不等值

Softmax函数的分子：因为是计算$a_j$，所以分子是$e^{z_j}$。

Softmax函数的分母：
$$
\sum\limits_{i=1}^m e^{z_i} = e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m} = \sum \tag{为了方便简写成$\sum$}
$$

- $i=j$时（比如输出分类值a1对z1的求导），求$a_j$对$z_i$的导数，此时分子上的$e^{z_j}$要参与求导。参考基本数学导数公式33：

$$\frac{\partial{a_j}}{\partial{z_i}} = \frac{\partial{}}{\partial{z_i}}(e^{z_j}/\sum)$$
$$= \frac{\partial{}}{\partial{z_j}}(e^{z_j}/\sum) \tag{因为$z_i=z_j$}$$
$$=\frac{e^{z_j}\sum-e^{z_j}e^{z_j}}{(\sum)^2}$$
$$=\frac{e^{z_j}}{\sum} - \frac{(e^{z_j})^2}{(\sum)^2}$$
$$= a_j-a^2_j=a_j(1-a_j) \tag{4}$$

- $i \neq j$时（比如输出分类值a1对z2的求导，j=1, i=2），$a_j$对$z_i$的导数，分子上的$z_j与i$没有关系，求导为0，分母的求和项中$e^{z_i}$要参与求导。同样是公式33，因为分子$e^{z_j}$对$e^{z_i}$求导的结果是0：

$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}
$$
求和公式对$e^{z_i}$的导数$(\sum)'$，除了$e^{z_i}$项外，其它都是0：
$$
(\sum)' = (e^{z_1} + \dots + e^{z_i} + \dots +e^{z_m})'=e^{z_i}
$$
所以：
$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}=-\frac{e^{z_j}e^{z_i}}{{(\sum)^2}}=-\frac{e^{z_j}}{{\sum}}\frac{e^{z_j}}{{\sum}}=-a_{i}a_{j} \tag{5}
$$

### 用具体数值举例

- $i = j = 1$时
  
  $${\partial a_1 \over \partial z_1}=a_1(1-a_1)$$

这和Sigmoid的导数一样，这也是可以预期的，可以看作是单路前传计算，即$a_1$只依赖于$z_1$。

- $i = 1，j = 2$时

$${\partial a_1 \over \partial z_1}=-a_1 a_2$$


### 结合损失函数的整体反向传播公式

<img src=".\Images\7\Loss-A-Z.jpg">

看上图，我们要求Loss值对Z1的偏导数。和以前的Sigmoid函数不同，那个函数是一个z对应一个a，所以反向关系也是一对一。而在这里，a1的计算是有z1,z2,z3参与的，a2的计算也是有z1,z2,z3参与的，即所有a的计算都与前一层的z有关，所以考虑反向时也会比较复杂。

先从Loss的公式看，$J=-(y_1lna_1+y_2lna_2+y_3lna_3)$，a1肯定与z1有关，那么a2,a3是否与z1有关呢？

再从Softmax函数的形式来看：
$$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}
$$
$$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}
$$
$$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}
$$
无论是a1，a2，a3，都是与z1相关的，而不是一对一的关系，所以，想求Loss对Z1的偏导，必须把Loss->A1->Z1， Loss->A2->Z1，Loss->A3->Z1，这三条路的结果加起来。于是有了如下公式：

$$
\frac{\partial{J}}{\partial{z_i}}= \frac{\partial{J}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_i}} + \frac{\partial{J}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_i}} + \frac{\partial{J}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_i}}$$
$$=\sum_j \frac{\partial{J}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}$$

你可以假设上式中$i=1，j=3$，就完全符合我们的假设了，而且不失普遍性。

前面说过了，因为Softmax涉及到各项求和，A的分类结果和Y的标签值分类是否一致，所以需要分情况讨论：

$$
\frac{\partial{a_j}}{\partial{z_i}} = \begin{cases} a_j(1-a_j), & i = j \\ -a_ia_j, & i \neq j \end{cases}\\
$$

因此，$\frac{\partial{J}}{\partial{z_i}}$应该是$i=j和i \neq j$两种情况的和：
- $i = j时，J通过A1对Z1求导（或者是通过A2对Z2求导）$：

$$
\frac{\partial{J}}{\partial{z_i}} = \frac{\partial{J}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=-\frac{y_j}{a_j}a_j(1-a_j)=y_j(a_j-1)=y_i(a_i-1) \tag{6}
$$

- $i \neq j，J通过A2+A3对Z1求导$：

$$
\frac{\partial{J}}{\partial{z_i}} = \frac{\partial{J}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=\sum_j^m(-\frac{y_j}{a_j})(-a_ja_i)=\sum_j^m(y_ja_i)=a_i\sum{y_j} \tag{7}
$$

如果不好理解的话，就拆开上面的公式，假设求J通过a2/a3对z1的导数：

$$
\frac{\partial{J}}{\partial{z_1}}=\frac{\partial{J}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_1}}+\frac{\partial{J}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_1}}
$$
因为：
$$
\frac{\partial{J}}{\partial{a_2}}=-\frac{y_2}{a_2}
$$
$$
\frac{\partial{a_2}}{\partial{z_1}}=-a_2a_1
$$
$$
\frac{\partial{J}}{\partial{a_3}}=-\frac{y_3}{a_3}
$$
$$
\frac{\partial{a_3}}{\partial{z_1}}=-a_3a_1
$$
所以：
$$
\frac{\partial{J}}{\partial{z_1}} = (-\frac{y_2}{a_2})(-a_2a_1)+(-\frac{y_3}{a_3})(-a_3a_1)=y_2a_1+y_3a_1=a_1(y_2+y_3)=a_i\sum_{j\neq i}y_j
$$

把两种情况加起来：

$$\frac{\partial{J}}{\partial{z_i}} = y_i(a_i-1)+a_i\sum_{j \neq i}y_j$$
$$=-y_i+a_iy_i+a_i\sum_{j \neq i}y_j$$
$$=-y_i+a_i(y_i+\sum_{j \neq i}y_j)$$
$$=-y_i + a_i\sum_jy_j$$
$$=-y_i + a_i*1$$
$$=a_i-y_i \tag{8}$$

因为$y_j$是取值[1,0,0]或者[0,1,0]或者[0,0,1]的，这三者用$\sum$加起来，就是[1,1,1]，在矩阵乘法运算里乘以[1,1,1]相当于什么都不做，就等于原值。

我们惊奇地发现，最后的反向计算过程就是：$a_i-y_i$，假设当前样本的$a_i=[0.3, 0.6, 0.1]$，而$y_i=[1, 0, 0]$，则：
$$a_i - y_j = [0.3,0.6,0.1]-[1,0,0]=[-0.7,0.6,0.1]$$

其含义是，样本预测第二类，但实际是第一类，所以给第一类-0.7的惩罚值，反向传播给神经网络。

后面对$z=wx+b$的求导，与二分类一样，不再赘述。

## Softmax函数的Python实现

第一种，直截了当按照公式写：
```Python
def Softmax1(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```
这个可能会发生的问题是，当x很大时，np.exp(x)很容易溢出，因为是指数运算。所以，有了下面这种改进的代码：

```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
测试一下：
```Python
Z = np.array([3,0,-3])
print(Softmax1(Z))
print(Softmax2(Z))
```
两个实现方式的结果一致：
```
[0.95033021 0.04731416 0.00235563]
[0.95033021 0.04731416 0.00235563]
```

为什么一样呢？从代码上看差好多啊！我们来证明一下：

假设有3个值a，b，c，并且a在三个数中最大，则b所占的Softmax比重应该这样写：

$$P(b)=\frac{e^b}{e^a+e^b+e^c}$$

如果减去最大值变成了a-a，b-a，c-a，则b'所占的Softmax比重应该这样写：

$$P(b') = \frac{e^{b-a}}{e^{a-a}+e^{b-a}+e^{c-a}}$$
$$=\frac{e^b/e^a}{e^a/e^a+e^b/e^a+e^c/e^a}$$
$$= \frac{e^b}{e^a+e^b+e^c}$$
$$
所以：P(b) == P(b')
$$

Softmax2的写法对一个一维的向量或者数组是没问题的，如果遇到Z是个MxN维(M,N>1)的矩阵的话，就有问题了，因为np.sum(exp_Z)这个函数，会把MxN矩阵里的所有元素加在一起，得到一个标量值，而不是相关列元素加在一起。

所以应该这么写：

```Python
def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    #s = np.sum(exp_z)
    s = np.sum(exp_z, axis=0)
    A = exp_z / s
    return A
```

axis=0这个参数非常非常重要，因为如果输入Z是单样本的预测值话，如果是分三类，则应该是个3x1的数组，如果：

> $Z = [3, 1, -3]$
> 
>$e^z = [1, 0.135, 0.002]$

>$s = 1.138$

> $A = [0.879, 0.119, 0.002]$

但是，如果是批量训练，假设每次用两个样本，则：

> $Z = [[3, 1, -3][1, -3, 3]]$
>
> $e^z = [[1, 0.135, 0.002][0.135, 0.002, 1]]$
> 
> $s = [[1.138][1.138]]$ 
>
> $A = [[0.879, 0.119, 0.002][0.119, 0.002, 0.879]]$

其中，A是包含两个样本的softmax结果，每个数组里面的三个数字相加为1。

如果s = np.sum(exp_z)，没有axis=0参数，则：

> $Z = [[3, 1, -3][1, -3, 3]]$

> $e^z = [[1, 0.135, 0.002][0.135, 0.002, 1]]$

> $s = 2.276$

> $A = [[0.439, 0.059, 0.001][0.059, 0.001, 0.439]]$

A虽然仍然包含两个样本，但是变成了两个样本所有的数字相加为1，这不是softmax的本意，softmax只计算一个样本中的数据。


