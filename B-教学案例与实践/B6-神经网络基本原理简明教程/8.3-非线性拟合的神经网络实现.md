Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 非线性拟合的神经网络实现

先来观察一下样本：

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|x|0.606|0.129|0.643|...|0.199|
|y|-0.113|-0.269|-0.217|...|-0.281|

首先观察一下样本数据的范围，x是在[0,1]，y是[-1,1]，这样我们就不用做数据归一化了。这条线看起来像一条处于攻击状态的眼镜蛇！

<img src=".\Images\8\Sample.png">

## 定义神经网络结构

我们定义一个两层的神经网络，输入层不算，一个隐藏层，含128个神经元，一个输出层。

<img src=".\Images\8\setup.jpg">

## 输入层

输入层就是一个标量x值。

## 权重矩阵W1/B1

它是连接两层之间的纽带，有的人理解它应该属于输入层，有的人理解应该属于隐藏层，各有各的道理，我个人倾向于把它归到隐藏层，理由是$Z1=W1 \cdot X+B1$，在X固定的前提下，W1决定了Z1的值。另外一个理由是B1的存在位置，在本例中B1是一个128x1的矩阵，它是隐藏层128个神经元的偏移，所以它应该属于隐藏层。

$$
W1=
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3} \\
\dots \\
w_{128} \\
\end{pmatrix}
$$

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-128的权重线B1-1...B1-128应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-128的矩阵/向量。
$$
B1=
\begin{pmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
\dots \\
b_{128} \\
\end{pmatrix}
$$


## 隐藏层

我们用一个128个神经元的网络来模拟函数，这个大家可以自己试验一下，把代码中的神经元数量修改一下，然后在保持迭代次数和其它（超）参数不变的情况，看看最终的精确度有何区别，训练时间的差异，以及内存占用有何差异。

每个神经元的输入$Z1 = W1 \cdot X + B1$，我们在这里使用双曲sigmoid函数，所以输出是$A1 = Sigmoid(Z1)$。当然也可以使用其它激活函数如果tanh, Relu等等。

## 权重矩阵W2/B2

与W1/B1类似，我个人认为它属于输出层。W2的尺寸是1x128，B2的尺寸是1x1。
$$
W2=
\begin{pmatrix}w_{1} & w_{2} & w_{3} \dots w_{128} \end{pmatrix}
$$

$$
B2=
\begin{pmatrix}
b
\end{pmatrix}
$$

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元，$Z2=W2 \cdot A1+B2$。


## 前向计算图

<img src=".\Images\8\FWC.jpg">

至此，我们得到了以下一串公式：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$

```Python
# 激活函数实现
def Sigmoid(z):
    a = 1 / (1 + np.exp(-z))
    return a

def ForwardCalculationBatch(batch_x, dict_weights):
    W1 = dict_weights["W1"]
    B1 = dict_weights["B1"]
    W2 = dict_weights["W2"]
    B2 = dict_weights["B2"]

    Z1 = np.dot(W1, batch_x) + B1
    A1 = Sigmoid(Z1)

    Z2 = np.dot(W2, A1) + B2
    A2 = Z2

    dict_cache ={"A1": A1, "A2": A2}
    return dict_cache
```
由于输入输出参数较多，所以我们用一个dictionary(dict_weights)来保存W,B这些参数，如果是更多层的神经网络，就会有更多的参数，我们这里使用的还是一些最基本的参数。返回一个dict_cache是因为反向传播时要用到前向计算时的输出值。

## 损失函数

我们仍然使用传统的均方差函数。其中，Z是每一次迭代的预测输出，Y是样本标签数据

$$J = \frac{1}{2m}\sum^m_{i=1}(z_i-y_i)^2$$

```Python
def CheckLoss(X, Y, dict_weights):
    m = X.shape[1]
    dict_cache = ForwardCalculationBatch(X, dict_weights)
    A2 = dict_cache["A2"]
    p1 = A2 - Y
    LOSS = np.multiply(p1, p1)
    loss = LOSS.sum()/m/2
    return loss
```

## 反向传播

```Python
def BackPropagationBatch(batch_x, batch_y, dict_cache, dict_weights):
    # 批量下降，需要除以样本数量，否则会造成梯度爆炸
    m = batch_x.shape[1]
    # 取出缓存值
    A1 = dict_cache["A1"]
    A2 = dict_cache["A2"]
    W2 = dict_weights["W2"]
    # 第二层的梯度
    dZ2 = A2 - batch_y  # 公式1
    # 第二层的权重和偏移
    dW2 = np.dot(dZ2, A1.T)/m # 公式2
    dB2 = np.sum(dZ2, axis=1, keepdims=True)/m  # 公式3
    # 第一层的梯度
    dA1 = np.dot(W2.T, dZ2)
    dA1_Z1 = np.multiply(A1, 1 - A1)
    dZ1 = np.multiply(dA1, dA1_Z1)  # 公式4
    # 第一层的权重和偏移
    dW1 = np.dot(dZ1, batch_x.T)/m  # 公式5
    dB1 = np.sum(dZ1, axis=1, keepdims=True)/m  # 公式6
    # 保存到词典中返回
    dict_grads = {"dW1":dW1, "dB1":dB1, "dW2":dW2, "dB2":dB2}
    return dict_grads
```

## 梯度更新

```Python
def UpdateWeights(dict_weights, dict_grads, learningRate):
    W1 = dict_weights["W1"]
    B1 = dict_weights["B1"]
    W2 = dict_weights["W2"]
    B2 = dict_weights["B2"]

    dW1 = dict_grads["dW1"]
    dB1 = dict_grads["dB1"]
    dW2 = dict_grads["dW2"]
    dB2 = dict_grads["dB2"]

    W1 = W1 - learningRate * dW1
    W2 = W2 - learningRate * dW2
    B1 = B1 - learningRate * dB1
    B2 = B2 - learningRate * dB2

    dict_weights = {"W1": W1,"B1": B1,"W2": W2,"B2": B2}

    return dict_weights
```

## 训练函数

```Python
def train(method, X, Y, num_input, num_hidden, num_output, loss_history):
    num_example = X.shape[1]
    num_feature = X.shape[0]
    num_category = Y.shape[0]
    # hyper parameters
    eta, max_epoch,batch_size = InitializeHyperParameters(method,num_example)
    # W(num_category, num_feature), B(num_category, 1)
    dict_weights = InitialParameters(num_input, num_hidden, num_output, 2)

    # calculate loss to decide the stop condition
    loss = 0        # initialize loss (larger than 0)
    error = 0.001    # stop condition

    # if num_example=200, batch_size=10, then iteration=200/10=20
    max_iteration = (int)(num_example / batch_size)
    for epoch in range(max_epoch):
        for iteration in range(max_iteration):
            # get x and y value for one sample
            batch_x, batch_y = GetBatchSamples(X,Y,batch_size,iteration)
            # get z from x,y
            dict_cache = ForwardCalculationBatch(batch_x, dict_weights)
            # calculate gradient of w and b
            dict_grads = BackPropagationBatch(batch_x, batch_y, dict_cache, dict_weights)
            # update w,b
            dict_weights = UpdateWeights(dict_weights, dict_grads, eta)
        # end for            
        # calculate loss for this batch
        loss = CheckLoss(X, Y, dict_weights)
        print("epoch=%d, loss=%f" %(epoch,loss))
        loss_history.AddLossHistory(loss, dict_weights, epoch, iteration)            
        if math.isnan(loss):
            break
        # end if
        if loss < error:
            break
        # end if
    # end for
```

# 主程序
```Python
TrainData = ReadData()
num_samples = TrainData.shape[1]
X = TrainData[0,:].reshape(1, num_samples)
Y = TrainData[1,:].reshape(1, num_samples)

n_input, n_hidden, n_output = 1, 128, 1
learning_rate = 0.1
eps = 1e-10
dictWeights = InitialParameters(n_input, n_hidden, n_output, 2)
max_iteration = 100000
min_loss = 0.002
loss, prev_loss, diff_loss = 0, 0, 10
loop = num_samples
for iteration in range(max_iteration):
    for i in range(loop):
        x = X[0,i]
        y = Y[0,i]
        A2, dictCache = ForwardCalculation(x, dictWeights)
        dictGrads = BackPropagation(x, y, dictCache, dictWeights)
        dictWeights = UpdateWeights(dictWeights, dictGrads, learning_rate)
   
    loss, diff_loss = LossCalculation(X, Y, dictWeights, prev_loss, num_samples)
    print(iteration,loss,diff_loss)
    if diff_loss < eps:
        break
    if loss < min_loss:
        break
    prev_loss = loss

print(loss, diff_loss)
ShowResult(iteration+1, n_hidden, min_loss, loop)
```

# 运行结果

按照上述代码的“标准”设置，我们可以得到以下结果：

<img src=".\Images\8\r128-1000.png"> 

