Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 梯度下降优化算法

## 随机梯度下降

先回忆一下随机梯度下降的基本算法，便于和后面的各种算法比较。

## 输入和参数

- $\eta$ - 全局学习率

算法：

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

> 更新参数：$\theta_t = \theta_{t-1}  - \eta \cdot g_t$


## 动量 Momentum

SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：

Momentum算法会观察历史梯度，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度。若当前梯度与历史梯方向不一致，则梯度会衰减。

一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。

### 输入和参数

- $\eta$ - 全局学习率
- $\alpha$ - 动量参数，一般取值为0.5, 0.9, 0.99
- $v_t$ - 当前时刻的动量，初值为0
  
### 算法

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

> 计算速度更新：$v_t = \alpha \cdot v_{t-1} + \eta \cdot g_t$
> 
> 更新参数：$\theta_t = \theta_{t-1}  - v_t$

但是在花书上的公式是这样的：

$$v_t = \alpha \cdot v_{t-1} - \eta \cdot \nabla \theta \tag{3}$$
$$\theta_{t} = \theta_{t-1} + v_t \tag{4}$$

这两个差别好大啊！一个加减号错会导致算法不工作！为了搞清楚，咱们手推一下迭代。

根据公式(1)(2)，以W参数为例，有：

$$v_0 = 0$$
$$
dW_0 = \nabla J(w)\\
v_1 = \alpha v_0 + \eta \cdot dW_0 = \eta \cdot dW_0 \\
W_1 = W_0 - v_1=W_0 - \eta \cdot dW_0
$$
$$
dW_1 = \nabla J(w)\\
v_2 = \alpha v_1 + \eta dW_1 \\
W_2 = W_1 - v_2 = W_1 - (\alpha v_1 +\eta dW_1) = W_1 - \alpha \cdot \eta \cdot dW_0 - \eta \cdot dW_1
$$
$$
dW_2 = \nabla J(w)\\
v_3=\alpha v_2 + \eta dW_2 \\
W_3 = W_2 - v_3=W_2-(\alpha v_2 + \eta dW_2) = W_2 - \alpha^2 \eta dW_0 - \alpha \eta dW_1 - \eta dW_2 \tag{5}
$$

根据公式(3)(4)有：

$$v_0 = 0$$
$$
dW_0 = \nabla J(w)\\
v_1 = \alpha v_0 - \eta \cdot dW_0 = -\eta \cdot dW_0 \\
W_1 = W_0 + v_1=W_0 - \eta \cdot dW_0
$$
$$
dW_1 = \nabla J(w)\\
v_2 = \alpha v_1 - \eta dW_1 \\
W_2 = W_1 + v_2 = W_1 + (\alpha v_1 - \eta dW_1) = W_1 - \alpha \cdot \eta \cdot dW_0 - \eta \cdot dW_1
$$
$$
dW_2 = \nabla J(w)\\
v_3=\alpha v_2 - \eta dW_2 \\
W_3 = W_2 + v_3=W_2 + (\alpha v_2 - \eta dW_2) = W_2 - \alpha^2 \eta dW_0 - \alpha \eta dW_1-\eta dW_2 \tag{6}
$$

通过手工推导迭代，我们得到两个结论：

1. 可以看到公式(5)(6)是相同的，即公式(1)(2)等同于(3)(4)
2. 与普通SGD的算法$W_3 = W_2 - \eta dW_2$相比，动量法不但每次要减去当前梯度，还要减去历史梯度再乘以一个不断减弱的因子$\alpha$，因为$\alpha$小于1，所以$\alpha^2$比$\alpha$小，$\alpha^3$比$\alpha^2$小。



|SGD|Momentum|
|---|---|
|<img src=".\Images\8\SGD.png">|<img src=".\Images\8\Momentum.png">|

从上图的比较可以看到，使用同等的条件：
- 终止条件：loss = 0.001
- batch_size = 10
- eta = 0.1
- 隐层神经元数量ne=4

左侧的普通梯度下降法，epoch=25280次停止，而右侧的动量法经过2243次迭代结束。原因是在左侧的图中，中间有一大段比较平坦的区域，梯度值很小，或者是随机梯度下降算法找不到合适的方向前进，只能慢慢搜索。而右侧的动量法，利用惯性，判断当前梯度与上次梯度的关系，如果方向相同，则会加速前进；如果不同，则会减速，并趋向平衡。

## NAG - Nesterov Accelerated Gradient

在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好。

### 输入和参数

- $\eta$ - 全局学习率
- $\alpha$ - 动量参数，确实取值0.9
- v - 动量，初始值为0
  
### 算法

> 临时更新：$\hat \theta = \theta_{t-1} + \alpha \cdot v_{t-1}$

> 前向计算：$f(\hat \theta)$

> 计算梯度：$g_t = \nabla_{\hat\theta} J(\hat \theta)$

> 计算速度更新：$v_t = \alpha \cdot v_{t-1} + \eta \cdot g_t$

> 更新参数：$\theta_t = \theta_{t-1}  - v_t$

其核心思想是：注意到 momentum 方法，如果只看 γ * v 项，那么当前的θ经过momentum的作用会变成 θ-γ * v。因此可以把 θ-γ * v这个位置看做是当前优化的一个”展望”位置。所以，可以在 θ-γ * v求导, 而不是原始的θ。

同带动量的SGD相比，梯度不是根据当前位置θ计算出来的，而是在移动之后的位置$\theta - \gamma v_{t-1}$计算梯度。理由是，既然已经确定会移动$\theta - \gamma v_{t-1}$，那不如之前去看移动后的梯度。

这个改进的目的就是为了提前看到前方的梯度。如果前方的梯度和当前梯度目标一致，那我直接大步迈过去； 如果前方梯度同当前梯度不一致，那我就小心点更新。

|Momentum|NAG|
|---|---|
|<img src=".\Images\8\Momentum.png">|<img src=".\Images\8\NAG.png">|

从上图的比较可以看出，NAG比动量法还要快一些，只迭代了885个epoch次就到达loss=0.001的停止条件。

下图是二者的前进方向比较：

<img src=".\Images\8\NesterovMomentum.jpg">

蓝色线表示正常的带动量SGD的两次移动； 棕色线是计划移动的量$\gamma v_{t-1}$; 红色线是在移动后位置计算的移动量； 棕色线和红色线的合并效果就是绿色线NAG。

<img src=".\Images\8\sgd_m_nag.png">


有了Optimizer之后，我们的系统结构有了变化，多了一个组件：

<img src=".\Images\8\NNModule2.png">

这个优化器专门为训练过程提供额外的帮助，来加速网络收敛速度。

代码位置：ch08, Level5, Level6
