Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 参数调整

## 可调与不可调参数

我们使用如下参数做第一次的训练：

|参数|缺省值|是否可调|
|---|---|---|
|输入层神经元数|1|No|
|隐层神经元数|4|Yes|
|输出层神经元数|1|No|
|学习率|0.1|Yes|
|批样本量|10|Yes|
|最大epoch|50000|Yes|
|损失门限值|0.001|No|
|损失函数|MSE|No|
|参数初始化方法|Xavier|Yes|

木头：买嘎哒！怎么这么多参数！

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

## 前提

在后面的几组比较中，都是用Xavier方法初始化的。另外，两次使用Xavier初始化，也会得到不同的结果，为了避免这个随机性，我们在代码Level1_TwoLayer.py中，使用了一个小技巧，调用下面这个函数：

```Python
    def GenerateWeightsArrayFileName(self):
        self.w1_name = str.format("w1_{0}_{1}_{2}.npy", self.num_hidden, self.num_input, self.init_method.name)
        self.w2_name = str.format("w2_{0}_{1}_{2}.npy", self.num_output, self.num_hidden, self.init_method.name)

    # load exist initial weights which has same parameters
    # if not found, create new, then save
    def LoadSameInitialParameters(self):
        self.GenerateWeightsArrayFileName()
        w1_file = Path(self.w1_name)
        w2_file = Path(self.w2_name)
        if w1_file.exists() and w2_file.exists():
            W1 = np.load(w1_file)
            W2 = np.load(w2_file)
            B1 = np.zeros((self.num_hidden, 1))
            B2 = np.zeros((self.num_output, 1))
        else:
            W1, B1 = CParameters.InitialParameters(self.num_input, self.num_hidden, self.init_method)
            W2, B2 = CParameters.InitialParameters(self.num_hidden, self.num_output, self.init_method)
            np.save(w1_file, W1)
            np.save(w2_file, W2)
        # end if
        return W1, B1, W2, B2
```

第一次调用时，会得到一个随机初始化矩阵，以后再次调用时，只要隐层神经元数量不变并且初始化方法不变，就会用第一次的初始化结果，否则后面的各种参数调整的结果就没有可比性了。

## 学习率的调整

|学习率的影响||
|---|---|
|<img src=".\Images\10\eta01_loss.png">|<img src=".\Images\10\eta05_loss.png">|
|eta=0.1, epoch=12166|eta=0.5, epoch=5940|
|<img src=".\Images\10\eta07_loss.png">|<img src=".\Images\10\eta09_loss.png">|
|eta=0.7, epoch=2204|eta=0.9, epoch=2599|

对于这个特定问题，较大的学习率可以带来很快的收敛速度，但并不是对所有问题都这样。


## 批大小的调整

|批大小的影响||
|------|---|
|<img src=".\Images\10\bz1_loss.png">|<img src=".\Images\10\bz5_loss.png">|
|batch_size=1, epoch=1973|batch_size=5, epoch=3278|
|<img src=".\Images\10\bz10_loss.png">|<img src=".\Images\10\bz20_loss.png">|
|batch_size=10, epoch=5940|batch_size=20, epoch=11894|

批样本量越小，epoch次数越少，收敛越快。但是收敛速度快，不代表运行速度快，在笔者的机器上，图一比图二需要更长时间运行完毕，因为图二是5个样本一起计算的，底层库做了优化。

但是！但是！这个结论的前提是我们用了0.5的学习率，如果用0.1的话，将会得到不同结论。

## 隐层神经元数量的调整

|隐层神经元数量的影响||
|---|---|
|<img src=".\Images\10\ne4_loss.png">|<img src=".\Images\10\ne6_loss.png">|
|neuron number = 4, epoch = 5940|neuron number = 6, epoch = 2007|
|<img src=".\Images\10\ne8_loss.png">|<img src=".\Images\10\ne10_loss.png">|
|neuron number = 8, epoch = 3271|neuron number = 10, epoch = 3575|

对于这个特定问题，隐层神经元个数为6时，收敛速度最快。

代码位置：ch08, Level3


**课后作业**

使用下列参数设置，找到批大小和学习率的关系：

- 隐层神经元：4
- 初始化：Xavier
- 批大小选择：1，5，10，15，20，25，30
- 学习率选择：0.1，0.3，0.5，0.7

得到下表：

|批大小|1|5|10|15|20|25|30|
|---|---|---|---|---|---|---|---|
|0.1|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.3|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.5|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.7|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|

从而得到批大小与学习率的最佳组合。
