Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  
## 5.2 神经网络解法

### 5.2.1 定义神经网络结构

我们定义一个一层的神经网络，输入层为2或者更多，反正大于2了就没区别。这个一层的神经网络的特点是：
1. 没有中间层，只有输入项和输出层（输入项不算做一层），
2. 输出层只有一个神经元，
3. 神经元有一个线性输出，不经过激活函数处理，即在下图中，经过$\Sigma$求和得到Z值之后，直接把Z值输出。

与上一章的神经元相比，这次仅仅是多了一个输入，但却是质的变化，即，一个神经元可以同时接收多个输入，这是神经网络能够处理复杂逻辑的根本。

<img src=".\Images\5\setup.png">

- 输入层

单独看第一个样本是这样的：

$$
x_1 =
\begin{pmatrix}
x_{11} & x_{12}
\end{pmatrix} = 
\begin{pmatrix}
10.06 & 60
\end{pmatrix} 
$$

$$
y_1 = \begin{pmatrix} 302.86 \end{pmatrix}
$$


一共有1000个样本，每个样本2个特征值，X就是一个$1000 \times 2$的矩阵：

$$
X = \\
\begin{pmatrix} 
x_1 \\ x_2 \\ \dots \\ x_{1000}
\end{pmatrix} =
\begin{pmatrix} 
x_{1,1} & x_{1,2} \\
x_{2,1} & x_{2,2} \\
\dots & \dots \\
x_{1000,1} & x_{1000,2}
\end{pmatrix}
$$

$$
Y =
\begin{pmatrix}
y_1 \\ y_2 \\ \dots \\ y_{1000}
\end{pmatrix}=
\begin{pmatrix}
302.86 \\ 393.04 \\ \dots \\ 450.59
\end{pmatrix}
$$


$x_1$表示第一个样本，$x_{1,1}$表示第一个样本的一个特征值，$y_1$是第一个样本的标签值。

- 权重W和B

由于输入层是两个特征，输出层是一个变量，所以w的形状是2x1，而b的形状是1x1。

$$
W=
\begin{pmatrix}
w_1 \\ w_2
\end{pmatrix}
$$

$$B=(b)$$

B是个单值，因为输出层只有一个神经元，所以只有一个bias，每个神经元对应一个bias，如果有多个神经元，它们都会有各自的b值。

- 输出层

由于我们只想完成一个回归（拟合）任务，所以输出层只有一个神经元。由于是线性的，所以没有用激活函数。
  
$$Z = X^{(1,2)} \cdot W^{(2,1)} + B^{(1,1)}$$

上述公式中括号中的数字表示该矩阵的 (行，列) 数，如W(2,1)表示W是一个2行1列的矩阵。

对于拟合，可以想象成用一支笔在一堆点中画一条直线或者曲线，而那一个神经元就是这支笔。如果有多个神经元，可以画出多条线来，就不是拟合了，而是分类。

### 5.2.2 代码实现

我们依然采用第四章中已经写好的HelperClass目录中的那些类，来表示我们的神经网络。虽然此次神经元多了一个输入，但是不用改代码就可以适应这种变化，因为在前向计算代码中，使用的是矩阵乘的方式，可以自动适应x的多个列的输入，只要对应的w的矩阵形状是正确的即可。

但是在初始化时，我们必须手动指定x和w的形状，如下面的代码所示：

```Python
from HelperClass.SimpleDataReader import *

if __name__ == '__main__':
    # data
    reader = SimpleDataReader()
    reader.ReadData()
    # net
    params = HyperParameters(eta=0.1, max_epoch=100, batch_size=1, eps = 1e-5)
    net = NeuralNet(params, 2, 1)
    net.train(reader)
    # inference
    x1 = 15
    x2 = 93
    x = np.array([x1,x2]).reshape(1,2)
    print(net.inference(x))
```
在参数中，指定了学习率0.1，最大循环次数100轮，批大小1个样本，以及停止条件损失函数值1e-5。

在神经网络初始化时，指定了input_size=2，且output_size=1，即一个神经元可以接收两个输入，最后是一个输出。

最后的inference部分，是把两个条件（15公里，93平方米）代入，查看输出结果。

在下面的神经网络的初始化代码中，W的初始化是根据input_size和output_size的值进行的。

```Python
class NeuralNet(object):
    def __init__(self, params, input_size, output_size):
        self.params = params
        self.W = np.zeros((input_size, output_size))
        self.B = np.zeros((1, output_size))
```

### 5.2.3 运行结果

在Visual Studio 2017中，可以使用Ctrl+F5运行Level2的代码，但是，会遇到一个令人沮丧的打印输出：

```
epoch=0
NeuralNet.py:32: RuntimeWarning: invalid value encountered in subtract
  self.W = self.W - self.params.eta * dW
0 500 nan
epoch=1
1 500 nan
epoch=2
2 500 nan
epoch=3
3 500 nan
......
```

减法怎么会出问题？什么是nan？

nan的意思是数值异常，导致计算溢出了，出现了没有意义的数值。现在是每500个迭代监控一次，我们把监控频率调小一些，再试试看：

```
epoch=0
0 10 6.838664338516814e+66
0 20 2.665505502247752e+123
0 30 1.4244204612680962e+179
0 40 1.393993758296751e+237
0 50 2.997958629609441e+290
NeuralNet.py:76: RuntimeWarning: overflow encountered in square
  LOSS = (Z - Y)**2
0 60 inf
0 70 inf
0 80 inf
0 90 inf
0 100 inf
0 110 inf
NeuralNet.py:32: RuntimeWarning: invalid value encountered in subtract
  self.W = self.W - self.params.eta * dW
0 120 nan
0 130 nan
```

前10次迭代，损失函数值已经达到了6.83e+66，而且越往后运行值越大，最后终于溢出了。下面的损失函数历史记录也表明了这一过程。

<img src=".\Images\5\wrong_loss.png">

### 5.2.4 寻找失败的原因

我们可以在NeuralNet.py文件中，在下述代码行上设置断点，跟踪一下训练过程，以便找到问题所在：

<img src=".\Images\5\debug.png">

在VS2017中用F5运行debug模式，看第50行的结果：
```
batch_x
array([[ 4.96071728, 41.        ]])
batch_y
array([[244.07856544]])
```
返回的样本数据是正常的。再看下一行：
```
batch_z
array([[0.]])
```
第一次运行前向计算，由于W和B初始值都是0，所以z也是0，这是正常的。再看下一行：
```
dW
array([[ -1210.80475712],
       [-10007.22118309]])
dB
array([[-244.07856544]])
```
dW和dB的值都非常大，这是因为下面这行代码：

<img src=".\Images\5\backward.png">

batch_z是0，batch_y是244.078，二者相减，是-244.078，因此dB就是-244.078，dW因为矩阵乘了batch_x，值就更大了。

再看W和B的更新值，一样很大：
```
self.W
array([[ 121.08047571],
       [1000.72211831]])
self.B
array([[24.40785654]])
```
如果W和B的值很大，那么再下一轮进行前向计算时，会得到更糟糕的结果：
```
batch_z
array([[82459.53752331]])
```
果不其然，这次的z值飙升到了8万多，如此下去，几轮以后数值溢出是显而易见的事情了。

那么我们到底遇到了什么情况？

### 代码位置

ch05, Level2