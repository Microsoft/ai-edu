# 学习要点

- 多元线性回归
- 样本数据归一化
- 输入层的矩阵运算设计

# 定义神经网络结构

我们定义一个一层的神经网络，输入层为3或者更多，反正大于2了就没区别。然后这个一层的神经网络只有一个神经元，且神经元有一个线性输出，不经过激活函数，也不经过额外的输出层。

<img src=".\Images\6\Setup.jpg" width="600">

这样的一个神经元（网络）能做什么事情呢？答案就是多元线性回归。函数模型如下：

$$y=a_0+a_1x_1+a_2x_2+\dots+a_kx_k$$

为了方便大家理解，咱们具体化一下上面的公式，按照本系列文章的符号约定就是：

$$ 
Z = w_1x_1+w_2x_2+w_3x_3+b = WX + B
$$

## 输入层

假设一共有m个样本，X就是一个$n \times m$的矩阵，模样是这样紫的（n=3）：

|样本序号|1|2|3|4|...|m|
|---|---|----|---|--|--|--|
|样本x1特征值|3|2|6|3|...|3|
|样本x2特征值|1|4|2|4|...|2|
|样本x3特征值|96|100|54|72|...|69|
|样本标签值y|434|500|321|482|...|410|
变成矩阵：

$$
X =
\begin{pmatrix}
x_1^1 & x_1^2 & \dots & x^m_1 \\
\\
x_2^1 & x_2^2 & \dots & x^m_2 \\
\\
x_3^1 & x_3^2 & \dots & x^m_3
\end{pmatrix} = 
\begin{pmatrix}
3 & 2 & \dots & 3 \\
\\
1 & 4 & \dots & 2 \\
\\
96 & 100 & \dots & 54
\end{pmatrix} 
$$

$$
Y =
\begin{pmatrix}
y^1 & y^2 & \dots & y^m \\
\end{pmatrix}=
\begin{pmatrix}
434 & 500 & \dots & 410 \\
\end{pmatrix}
$$

单独看一个样本是这样的：

$$
x^1 =
\begin{pmatrix}
x_1 \\
\\
x_2 \\
\\
x_3
\end{pmatrix} = 
\begin{pmatrix}
3 \\
\\
1 \\
\\
96
\end{pmatrix} 
$$

$$
y^1 = \begin{pmatrix} 434 \end{pmatrix}
$$



有人问了，为何不把这个表格转一下，变成横向是样本特征值，纵向是样本数量？

## 权重矩阵W1/B1

如果假设每个样本X有n个特征向量，上式中的W就是一个$1 \times n$（n=3）向量或者矩阵，这个样子：
$$
\begin{pmatrix}w_1 & w_2 & w_3\end{pmatrix}
$$

B1是个单值，因为只有一个神经元，所以只有一个bias。

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元。它们的左侧是$Z2=W2*A1+B2$，右侧是$A2=Z2$。

为什么在最后一步没有用激活函数，而是直接令A2=Z2呢？我们后面再说。

# 创造训练数据

让我们先自力更生创造一些模拟数据：

```Python
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def TargetFunction(x):
    p1 = 0.4 * (x**2)
    p2 = 0.3 * x * np.sin(15 * x)
    p3 = 0.01 * np.cos(50 * x)
    y = p1 + p2 + p3 - 0.3
    return y

def CreateSampleDataXY(m):
    S = np.random.random((m,2))
    S[:,1] = TargetFunction(S[:,0])
    return S

def CreateTestData(n):
    TX = np.linspace(0,1,100)
    TY = TargetFunction(TX)
    TZ = np.zeros(n)
    return TX, TY, TZ
```

其函数图像在[0,1]之间的样子是：

<img src=".\Images\6-FunctionCurve.png" width="600">

生成的数据格式如下：

$$
\begin{pmatrix}
x_1, y_1\\
x_2, y_2\\
\dots\\
x_m, y_m\\
\end{pmatrix}
$$

其中，x就是上图中蓝色点的横坐标值，y是纵坐标值。在[0,1]之外的函数曲线没这么复杂，似乎拟合起来没什么难度，所以我们特点选择了[0,1]之间这一段来做试验。

# 定义前向计算过程

<img src=".\Images\6-FWC.jpg" width="600">

至此，我们得到了以下一串公式：

$$Z1=W1*X+B1$$

$$A1=sigmoid(Z1)$$

$$Z2=W2*A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$

```Python
def ForwardCalculation(x, dictWeights):
    W1 = dictWeights["W1"]
    B1 = dictWeights["B1"]
    W2 = dictWeights["W2"]
    B2 = dictWeights["B2"]

    Z1 = np.dot(W1,x) + B1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2,A1) + B2
    A2 = Z2  # 这一步可以省略

    dictCache ={"A1": A1, "A2": A2}
    return A2, dictCache
```
由于参数较多，所以我们用一个dictionary(dictWeights)来保存W,B这些参数，如果是更多层的神经网络，就会有更多的参数，我们这里使用的还是一些最基本的参数。

# 定义代价函数

我们用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。我们使用所有样本参与训练，因此损失函数实际为：

$$Loss = \frac{1}{2}(Z - Y) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

# 定义针对w和b的梯度函数

看一下计算图，然后用链式求导法则反推：

<img src=".\Images\6-Back.jpg" width="800">

蓝色的箭头线表示正向计算过程，黄色的箭头线表示反向的传播过程，其中dLoss/dZ2就是下面公式中的$\partial{Loss}\over{\partial{Z2}}$。

## 求W2的梯度
因为：

$$Z2 = W2*A1+B2$$

$$Loss = \frac{1}{2}(Z2-Y2)^2$$

所以我们用Loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{Loss}}{\partial{W2}} = \frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{W2}}
$$

其中：

$$
\frac{\partial{Loss}}{\partial{Z2}} = \frac{\partial{}}{\partial{Z2}}[\frac{(Z2-Y)^2}{2}] = Z2-Y
$$

而：

$$
\frac{\partial{Z2}}{\partial{W2}} = \frac{\partial{}}{\partial{W2}}(W2*A1+B2) = A1^T
$$

所以：

$$
\frac{\partial{Loss}}{\partial{W2}} = \frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{W2}} = (Z2-Y)*A1^T
$$

矩阵求导的理论部分较为复杂，请大家参考我们的《基本数学导数公式》章节。

## 求B2的梯度

$$
\frac{\partial{Loss}}{\partial{B2}} = \frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{B2}}
$$

其中第一项前面算w的时候已经有了，而：

$$
\frac{\partial{Z2}}{\partial{B2}} = \frac{\partial{(W2*A1+B2)}}{\partial{B2}} = 1
$$

所以：

$$
\frac{\partial{Loss}}{\partial{B2}} = \frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{B2}} = Z2-Y
$$

## 求W1的梯度
因为：

$$A1 = sigmoid(Z1)$$

$$Z1 = W1*X+B1$$

对Z1求导：

$$
\frac{\partial{Loss}}{\partial{Z1}} = \frac{\partial{Loss}}{\partial{Z2}}*\frac{\partial{Z2}}{\partial{A1}}*\frac{\partial{A1}}{\partial{Z1}}
$$

其中前面推导过：

$$
\frac{\partial{Loss}}{\partial{Z2}} = Z2-Y = dZ2 \tag{令结果为dZ2，以便后面使用}
$$

而：

$$
\frac{\partial{Z2}}{\partial{A1}} = \frac{\partial{}}{\partial{A1}}(W2*A1+B2) = W2^T
$$

$$
\frac{\partial{A1}}{\partial{Z1}} = \frac{\partial{}}{\partial{Z1}}(sigmoid(Z1)) = A1*(1-A1)
$$
对sigmoid函数的求导请看我们的《基本数学导数公式》或者《激活函数》章节。

所以：

$$
\frac{\partial{Loss}}{\partial{Z1}} = W2^T * dZ2 * A1 * (1-A1) = dZ1\tag{令结果为dZ1，以便后面使用}
$$
而W1,B1的求导结果和W2,B2类似：

$$
\frac{\partial{Loss}}{\partial{W1}} = \frac{\partial{Loss}}{\partial{Z1}}*\frac{\partial{Z1}}{\partial{W1}}=dZ1*\frac{\partial{(W1*X+B1)}}{\partial{W1}}=dZ1*X^T
$$

$$
\frac{\partial{Loss}}{\partial{B1}} = \frac{\partial{Loss}}{\partial{Z1}}*\frac{\partial{Z1}}{\partial{B1}}=dZ1*\frac{\partial{(W1*X+B1)}}{\partial{B1}}=dZ1
$$

变成代码：
```Python
def BackPropagation(x, y, dictCache, dictWeights):
    A1 = dictCache["A1"]
    A2 = dictCache["A2"]
    W2 = dictWeights["W2"]

    dLoss_Z2 = A2 - y
    dZ2 = dLoss_Z2
    dW2 = dZ2 * A1.T
    dB2 = dZ2

    dZ2_A1 = W2.T * dZ2
    dA1_Z1 = A1 * (1 - A1)
    # dZ1 is dLoss_Z1
    dZ1 = dZ2_A1 * dA1_Z1
    dW1 = dZ1 * x
    dB1 = dZ1

    dictGrads = {"dW1":dW1, "dB1":dB1, "dW2":dW2, "dB2":dB2}
    return dictGrads
```

# 每次迭代后更新w,b的值

```Python
def UpdateWeights(dictWeights, dictGrads, learningRate):
    W1 = dictWeights["W1"]
    B1 = dictWeights["B1"]
    W2 = dictWeights["W2"]
    B2 = dictWeights["B2"]

    dW1 = dictGrads["dW1"]
    dB1 = dictGrads["dB1"]
    dW2 = dictGrads["dW2"]
    dB2 = dictGrads["dB2"]

    W1 = W1 - learningRate * dW1
    W2 = W2 - learningRate * dW2
    B1 = B1 - learningRate * dB1
    B2 = B2 - learningRate * dB2

    dictWeights = {"W1": W1,"B1": B1,"W2": W2,"B2": B2}

    return dictWeights
```
# 

# 帮助函数
第一个show_result函数用于最后输出结果。第二个print_progress函数用于训练过程中的输出。

```Python
def sigmoid(x):
    s=1/(1+np.exp(-x))
    return s

def initialize_with_zeros(n_x,n_h,n_y):
    np.random.seed(2)
    # W1=np.random.randn(n_h,n_x)*0.00000001    # W1=np.random.randn(n_h,n_x)
    W1=np.random.uniform(-np.sqrt(6)/np.sqrt(n_x+n_h),np.sqrt(6)/np.sqrt(n_h+n_x),size=(n_h,n_x))
    # W1=np.reshape(32,784)
    B1=np.zeros((n_h,1))
    # W2=np.random.randn(n_y,n_h)*0.00000001  # W2=np.random.randn(n_y,n_h)
    W2=np.random.uniform(-np.sqrt(6)/np.sqrt(n_y+n_h),np.sqrt(6)/np.sqrt(n_y+n_h),size=(n_y,n_h))
    B2=np.zeros((n_y,1))

    assert (W1.shape == (n_h, n_x))
    assert (B1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (B2.shape == (n_y, 1))

    dictWeights = {"W1": W1,"B1": B1,"W2": W2,"B2": B2}

    return dictWeights
```



# 主程序初始化
```Python
m = 1000
S = CreateSampleDataXY(m)
#plt.scatter(S[:,0], S[:,1], 1)
#plt.show()
n_input, n_hidden, n_output = 1, 128, 1
learning_rate = 0.1
eps = 1e-10
dictWeights = initialize_with_zeros(n_input, n_hidden, n_output)
max_iteration = 1000
loss, prev_loss, diff_loss = 0, 0, 0
```

# 程序主循环

```Python
for iteration in range(max_iteration):
    for i in range(m):
        x = S[i,0]
        y = S[i,1]
        A2, dictCache = ForwardCalculation(x, dictWeights)
        dictGrads = BackPropagation(x,y,dictCache,dictWeights)
        dictWeights = UpdateWeights(dictWeights, dictGrads, learning_rate)
    print("iteration", iteration)
```

# 测试并输出拟合结果


```Python
tm = 100
TX, TY, TZ = CreateTestData(tm)
correctCount = 0
for i in range(tm):
    x = TX[i]
    y = TY[i]
    a2, dict = ForwardCalculation(x, dictWeights)
    TZ[i] = a2

plt.scatter(TX, TY)
plt.plot(TX, TZ, 'r')
str = str.format("cell:{0} sample:{1} iteration:{2} rate:{3}", n_hidden, m, max_iteration, learning_rate)
plt.title(str)
plt.show()
```
- 上面的TX是[0,1]之间的连续数，共100个，间隔相同。
- TY是针对被模拟的函数计算出来的精确值，就是下图中的蓝色点，表示函数的实际位置。
- TZ是我们训练的模型的预测值。

我们的目的就是要比较TY和TZ之间的差距。下图就是拟合结果，还比较令人满意。

<img src=".\Images\6-128-1000-1000-010.png" width="600">

# 参数调整

经常听人说起“调参”，这次咱们亲身经历一下调参的痛（快）苦（乐）！下面一切的比较都是以这组参数为基准：

1. 隐藏层神经元数=128
2. 输入训练数据量=1000
3. 迭代次数=1000
4. 权重调整步进值=0.1

以上这些标准值如何得到呢？试了很多组合后得到的，这就是所谓“试错”的过程了。

## 神经元数量的变化（标准值128）

神经元数量=64

<img src=".\Images\6-64-1000-2000-010.png" width="600">

神经元数量=96

<img src=".\Images\6-96-1000-1000-010.png" width="600">

神经元数量=256，迭代次数=500

<img src=".\Images\6-256-1000-500-010.png" width="600">

基准神经元数为128，在96时，拟合效果很差，在64时，尽管我们增加了迭代次数为2000，仍然很差。
第三张图，尽管神经元数量翻了一倍，成为256个，但是迭代次数为500，少了一倍，也会造成奇怪的结果。

## 样本量的变化（标准值1000）

样本数据量=500

<img src=".\Images\6-128-500-1000-010.png" width="600">

样本数据量=1500

<img src=".\Images\6-128-1500-1000-010.png" width="600">

样本数据量不够时，拟合效果不好。但是当样本数据量超过一定值后，就没多大作用了。

## 迭代次数的变化（标准值1000）

迭代次数=500

<img src=".\Images\6-128-1000-500-010.png" width="600">

输入数据量=1500

<img src=".\Images\6-128-1000-1500-010.png" width="600">

迭代次数少，拟合效果不好。迭代次数超过一定值后，容易造成过拟合，效果不大。

## 步长值的变化（标准值0.1）

步长=0.5

<img src=".\Images\6-128-1000-1000-050.png" width="600">

步长=0.01

<img src=".\Images\6-128-1000-1000-005.png" width="600">

步长值太大或者太小，都会造成不好的效果。


总结如下（效果5分为最好）：

||神经元数量|样本量|迭代次数|步长值|效果|
|---|---|----|---|--|--|
|0|128|1000|1000|0.1|5|
|1|64|1000|2000|0.1|2|
|2|96|1000|1000|0.1|3|
|3|256|1000|500|0.1|0|
|4|128|500|1000|0.1|2|
|5|128|1500|1000|0.1|5|
|6|128|1000|500|0.1|3|
|7|128|1000|1500|0.1|5|
|8|128|1000|1000|0.5|1|
|9|128|1000|1000|0.05|2|

