## 神经网络模型

为什么说神经网络是个黑盒子呢？一方面，由于神经网络中有数量及其庞大的参数，形成的函数方程是难以拟合或者分析这个函数有什么性质呢，或者说，这是一个难以解释的函数。以mnist为例，比如输入一个28\*28大小的手写字符图片，后接一个有16个神经元的全连接层，那么仅仅这一层的参数数量，就有28\*28\*16=12544个。然而一个神经网络不可能只有这样一层呀！多层堆叠起来的神经网络，参数数量是更加庞大的，以主流的几个神经网络为例，AlexNet参数数量有6000万个，Vgg网络的参数数量多达1.38亿个。

从另外一个浅显的角度来说，我们使用了各种各样的代码和深度学习框架，辛辛苦苦训练了很多轮之后得到了一个模型文件，比如tensorflow训练得到的ckpt，pytorch训练模型出来的pth文件等等。这样的模型文件里究竟保存了什么呢？是简简单单的一个保存了我们代码的文件吗？

当然，如果不想深入了解而只求使用深度学习框架的话，这样保存的文件内部是什么是并不重要的，反正都是保存读入都是函数一句话的事情嘛！但是深入了解学习一下模型文件内容还是很有必要的。

为了阐述的方便，我们以一个tensorflow生成的模型作为例子。

首先，我们使用代码生成一个非常简单的深度学习模型，由一个卷积层，一个relu激活函数组成，

```python
import tensorflow as tf
SEED = 46
def main():
    # first generate weights and bias used in conv layers
    conv1_weights = tf.Variable(
      tf.truncated_normal([5, 5, 3, 8],  # 5x5 filter, depth 8.
                          stddev=0.1,
                          seed=SEED))
    conv1_biases = tf.Variable(tf.zeros([8]))

    # data and out is placeholder used for input and output data in practice
    data = tf.placeholder(dtype=tf.float32, name="data", shape=[8, 32, 32, 3])
    out = tf.placeholder(dtype=tf.float32, name="out", shape=[8, 32, 32, 8])

    # as the structure of the simple model
    def model():
        conv = tf.nn.conv2d(data,
                        conv1_weights,
                        strides=[1, 1, 1, 1],
                        padding='SAME', name="conv")
        out = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases), name="relu")

    # saver is used for saving model
    saver = tf.train.Saver()
    with tf.Session() as sess:
        model()
        # initialize all variables
        tf.global_variables_initializer().run()
        # save the model in the file path
        saver.save(sess, './model')
```

在上面这份代码中，我们生成了一个输入大小是8\*32\*32\*3，输出尺寸是8\*32\*32\*8，卷积核大小是5\*5，输入通道数是3，输出通道数是8的只有一个卷积层的神经网络。我们甚至没有给这个神经网络进行训练，就直接进行了保存。这样一个神经网络模型文件里是一个什么样的结构呢？是像一般的程序一样，编译完了程度就被转化成了一堆逻辑指令呢？还是继续维持着这样一个模型文件的结构呢？

在运行完上面的代码后，在当前路径下应该多了四个文件，checkpoint，model.index， model.meta， model.data-00000-of-00001，如果没有的话请重新运行文件生成这样的文件。这样四个文件里面保存了什么呢？按照官方文档的解释，meta文件保存了序列化后的计算图，index文件则保存了数据文件的索引，包括变量名等等，data文件是保存了类似于变量值这样的数据的文件，checkpoint记录了最新的模型文件的名称或者序号。什么是计算图呢？直接用图形来做说明吧，

<img src=".\TrainedModel\graph.png" width="500">

上面的计算图就描述了一个将两个数相乘之后输出的计算过程。模型文件中的计算图描述的也是一个计算过程，不过要比这种简单的过程要复杂很多。

怎么查看一个模型文件中的内容呢？这里要使用[Tools for AI](https://visualstudio.microsoft.com/zh-hans/downloads/ai-tools-vs/?rr=https%3A%2F%2Fwww.msra.cn%2Fzh-cn%2Fnews%2Ffeatures%2Ftools-for-ai)工具里查看模型的功能了，

<img src=".\TrainedModel\AItools.PNG" width="500">

点击view Model按钮，会弹出来这样一个窗口

<img src=".\TrainedModel\netron.PNG" width="500">

选择Open Model，选择我们之前生成的那个简单的模型文件，稍等片刻，会看到这样一张截图：

<img src=".\TrainedModel\model.PNG" width="500">

哇，我不是只写了一个卷积层吗？为什么会多出来这么多东西？

下面呢，让我们来仔细分析下这张图的成分

首先看右半部分，

<img src=".\TrainedModel\graphpart.PNG" width="500">

每个方框是不是很眼熟？conv2D就是我们执行卷积操作的部分，按照箭头标示的数据流走向，经过卷积层处理的数据会进入BiasAdd部分，加上偏置，之后进入Relu部分进行relu操作，经过relu的数据会进行输出。这也就是我们在tensorflow代码中显示构建的部分。

下面我们从各个结点单独入手进行分析。

以卷积层的卷积核这样一个参数结点为例，

<img src=".\TrainedModel\variableNode.PNG" width="500">

从右侧窗口看到这个变量的尺寸是[5, 5, 3, 8]，正是我们初始化所预期的一个大小。那这个结点有4个和其他结点的联系，我们来逐个看一看。

首先是和左上侧结点之间的联系，

<img src=".\TrainedModel\trunc.PNG" width="400">

这一条支路上的联系对应的是代码中

```python
conv1_weights = tf.Variable(
      tf.truncated_normal([5, 5, 3, 8],  # 5x5 filter, depth 8.
                          stddev=0.1,
                          seed=SEED))
```

这样的生成一个截断的正态分布的初始化过程。也许有人会好奇这个初始化不是就一步吗？为什么这里会有四个结点进行这样一个过程呢？

分别来看看各个节点的作用吧。这里将依次展示四个结点的性质描述

<img src=".\TrainedModel\trunc1.PNG" width="400">

这是第一个结点的性质描述，该结点的功能也就是生成一个标准的截断的正态分布，也就是标准差是1，平均值是0的标准正态分布。

<img src=".\TrainedModel\trunc2.PNG" width="400">

这是乘法结点的性质描述，看这个结点的两个输入，一个是根据上一个结点输出的正态分布结果，另一个输入就是我们在代码中规定的标准差了，这个结点的作用就是将标准化的正态分布的标准差改变成我们在代码中期望的标准差。

<img src=".\TrainedModel\trunc3.PNG" width="400">

这个结点是一个执行加法操作的结点。在我们对卷积核使用截断的正态分布进行初始化的过程中，我们可以给出期望的两个参数，分别是用于初始化的正太分布的标准差和均值。在上一个乘法结点中，我们对所生成的标准分布的标准差进行了修改，也就是说，输入这个加法结点的是一个标准差是所期望的标准差，但是均值是零的一个正态分布。在加法结点中，我们采取对数据加上所期望的均值的方法将用于初始化的正态分布由标准正态分布转化成一个标准差是输入的stddev参数规定的标准差，均值是输入的mean参数规定的均值的一个正态分布。

<img src=".\TrainedModel\trunc4.PNG" width="400">

在前三步结点的操作中，我们只是生成了这样一个用于初始化的标准正态分布的有关数据，还没有把这个正态分布的值和我们要初始化的卷积核联系起来。这个结点的作用就是建立了这样的一个联系，将初始化操作得到的值赋给卷积核结点。

再看最右边的连接路径

<img src=".\TrainedModel\convkernel.PNG" width="400">

在这部分的计算图中，data结点就是我们在代码中声明的输入结点，Identity结点的作用是将我们的输入tensor进行一侧复制，将复制的结果传入conv2D结点进行卷积计算操作。

也就是说，这一部分就是我们预期的在前向传播中进行的操作的步骤。

那么另外两条路径的作用是什么呢？

再看我们之前的代码，建立模型的部分已经在计算图中体现了，对参数进行初始化的步骤也在最左边的连接中进行过了，还有哪个部分没有讲到呢？

还剩下的部分也就是我们在计算图中剩余的部分，对模型的存储操作了。不过，存储的目的是为了之后能够将模型从新导入代码文件中进行直接的使用，也就是说，虽然我们在代码中只体现了存储的部分，但是tensorlfow这个框架会自动的帮我们补全模型重载需要的操作，这一部分也会体现在计算图中。

而这一个保存，一个重载，也就是剩下的两条连接所对应的内容了。

<img src=".\TrainedModel\saveRestore.PNG" width="500">

我们之前说，在index文件中保存的是数据文件的索引，是一张表，这样的表里存放了什么呢？从这张计算图中看，在saveV2这样一个结点中，保存了变量的名称，由这个模型的名字，也就是在这份代码中的函数名称"model"加上变量自己的名称构成一个全局的变量名，保证不会出现重复的tensor名称。之后会存储的是这个tensor的大小，也就是shape_and_slices这个域中的内容，再之后保存的就会是这个tensor本身的值。

restore的过程是一个逆过程。程序会根据tensor的名称和对应的尺寸，从存下的文件中读取对应的tensor的值，经过一个赋值的assign结点，将保存下来的具体数值再重新放入对应的变量中，在目前这个例子中，也就是卷积核这个变量中。

另外一个variable结点对应的连接与我们上面所述的结点完全相似，希望大家自己尝试把这个结点对应的连接和代码中所描述的部分联系起来。